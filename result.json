[
{"type": "article", "header": "Exclusive: Google Contract Shows Deal With Israel Defense Ministry", "author": "Billy Perrigo", "update_date/publish_date": "April 12, 2024 7:44 AM EDT", "link_to_article": "https://time.com/6966102/google-contract-israel-defense-ministry-gaza-war/", "text": "Google provides cloud computing services to the Israeli Ministry of Defense, and the tech giant has negotiated deepening its partnership during Israel’s war in Gaza, a company document viewed by TIME shows. The Israeli Ministry of Defense, according to the document, has its own “landing zone” into Google Cloud—a secure entry point to Google-provided computing infrastructure, which would allow the ministry to store and process data, and access AI services. The ministry sought consulting assistance from Google to expand its Google Cloud access, seeking to allow “multiple units” to access automation technologies, according to a draft contract dated March 27, 2024. The contract shows Google billing the Israeli Ministry of Defense over $1 million for the consulting service.  The version of the contract viewed by TIME was not signed by Google or the Ministry of Defense. But a March 27 comment on the document, by a Google employee requesting an executable copy of the contract, said the signatures would be “completed offline as it’s an Israel/Nimbus deal.” Google also gave the ministry a 15% discount on the original price of consulting fees as a result of the “Nimbus framework,” the document says. Project Nimbus is a controversial $1.2 billion cloud computing and AI agreement between the Israeli government and two tech companies: Google and Amazon. Reports in the Israeli press have previously indicated that Google and Amazon are contractually barred from preventing specific arms of the Israeli state using their technology under Project Nimbus. But this is the first time the existence of a contract showing that the Israeli Ministry of Defense is a Google Cloud customer has been made public.  Google recently described its work for the Israeli government as largely for civilian purposes. “We have been very clear that the Nimbus contract is for workloads running on our commercial platform by Israeli government ministries such as finance, healthcare, transportation, and education,” a Google spokesperson told TIME for a story published on April 8. “Our work is not directed at highly sensitive or classified military workloads relevant to weapons or intelligence services.” Contacted on April 10 with questions about the Ministry of Defense contract, a Google spokesperson declined to comment further. Read More: Exclusive: Google Workers Revolt Over $1.2 Billion Contract With Israel The news comes after recent reports in the Israeli media have alleged the country’s military, controlled by the Ministry of Defense, is using an AI-powered system to select targets for air-strikes on Gaza. Such an AI system would likely require cloud computing infrastructure to function. The Google contract seen by TIME does not specify for what military applications, if any, the Ministry of Defense uses Google Cloud, and there is no evidence Google Cloud technology is being used for targeting purposes. But Google employees who spoke with TIME said the company has little ability to monitor what customers, especially sovereign nations like Israel, are doing on its cloud infrastructure.  The Israeli Ministry of Defense did not respond to requests for comment.  The Israeli Ministry of Defense’s attempt to onboard more units to Google Cloud is described in the contract as “phase 2” of a wider project to build out the ministry’s cloud architecture. The document does not explicitly describe phase one, but does refer to earlier work carried out by Google on behalf of the ministry. The ministry, the contract says, “has [already] established a Google Cloud Landing Zone infrastructure as part of their overall cloud strategy and to enable [the Ministry of Defense] to move applications to Google Cloud Platform.” For “phase 2” of the project, the contract says, the Ministry of Defense “is looking to enable its Landing Zone to serve multiple units and sub-units. Therefore, [the Ministry of Defense] would like to create several different automation modules within their Landing Zone based on Google’s leading practices for the benefit of different units, with proper processes to support, and to implement leading practices for security and governance architecture using Google tools.” The consulting services on offer by Google are for the tech company to “assist with architecture design, implementation guidance, and automation” for the Ministry of Defense’s Google Cloud landing zone, the contract says. The estimated start date is April 14, and Google’s consulting services are expected to take one calendar year to complete. Two Google workers have resigned in the last month in protest against Project Nimbus, TIME previously reported. Write to Billy Perrigo at billy.perrigo@time.com", "nested_links": ["https://time.com/6964364/exclusive-no-tech-for-apartheid-google-workers-protest-project-nimbus-1-2-billion-contract-with-israel/", "https://time.com/6964364/exclusive-no-tech-for-apartheid-google-workers-protest-project-nimbus-1-2-billion-contract-with-israel/", "https://time.com/6964364/exclusive-no-tech-for-apartheid-google-workers-protest-project-nimbus-1-2-billion-contract-with-israel/"]},
{"type": "article", "header": "Exclusive: Google Workers Revolt Over $1.2 Billion Contract With Israel", "author": "Billy Perrigo", "update_date/publish_date": "April 10, 2024 12:10 PM EDT | April 8, 2024 8:00 AM EDT", "link_to_article": "https://time.com/6964364/exclusive-no-tech-for-apartheid-google-workers-protest-project-nimbus-1-2-billion-contract-with-israel/", "text": "In midtown Manhattan on March 4, Google’s managing director for Israel, Barak Regev, was addressing a conference promoting the Israeli tech industry when a member of the audience stood up in protest. “I am a Google Cloud software engineer, and I refuse to build technology that powers genocide, apartheid, or surveillance,” shouted the protester, wearing an orange t-shirt emblazoned with a white Google logo. “No tech for apartheid!”  The Google worker, a 23-year-old software engineer named Eddie Hatfield, was booed by the audience and quickly bundled out of the room, a video of the event shows. After a pause, Regev addressed the act of protest. “One of the privileges of working in a company which represents democratic values is giving space for different opinions,” he told the crowd. Three days later, Google fired Hatfield. Hatfield is part of a growing movement inside Google that is calling on the company to drop Project Nimbus, a $1.2 billion contract with Israel, jointly held with Amazon. The protest group, called No Tech for Apartheid, now has more than 200 Google employees closely involved in organizing, according to members, who say there are hundreds more workers sympathetic to their goals. TIME spoke to five current and five former Google workers for this story, many of whom described a growing sense of anger at the possibility of Google aiding Israel in its war in Gaza. Two of the former Google workers said they had resigned from Google in the last month in protest against Project Nimbus. These resignations, and Hatfield’s identity, have not previously been reported. No Tech for Apartheid’s protest is as much about what the public doesn’t know about Project Nimbus as what it does. The contract is for Google and Amazon to provide AI and cloud computing services to the Israeli government and military, according to the Israeli finance ministry, which announced the deal in 2021. Nimbus reportedly involves Google establishing a secure instance of Google Cloud on Israeli soil, which would allow the Israeli government to perform large-scale data analysis, AI training, database hosting, and other forms of powerful computing using Google’s technology, with little oversight by the company. Google documents, first reported by the Intercept in 2022, suggest that the Google services on offer to Israel via its Cloud have capabilities such as AI-enabled facial detection, automated image categorization, and object tracking. Further details of the contract are scarce or non-existent, and much of the workers’ frustration lies in what they say is Google’s lack of transparency about what else Project Nimbus entails and the full nature of the company’s relationship with Israel. Neither Google, nor Amazon, nor Israel, has described the specific capabilities on offer to Israel under the contract. In a statement, a Google spokesperson said: “We have been very clear that the Nimbus contract is for workloads running on our commercial platform by Israeli government ministries such as finance, healthcare, transportation, and education. Our work is not directed at highly sensitive or classified military workloads relevant to weapons or intelligence services.” All Google Cloud customers, the spokesperson said, must abide by the company's terms of service and acceptable use policy. That policy forbids the use of Google services to violate the legal rights of others, or engage in “violence that can cause death, serious harm, or injury.” An Amazon spokesperson said the company “is focused on making the benefits of our world-leading cloud technology available to all our customers, wherever they are located,\" adding it is supporting employees affected by the war and working with humanitarian agencies. The Israeli government did not immediately respond to requests for comment. There is no evidence Google or Amazon’s technology has been used in killings of civilians. The Google workers say they base their protests on three main sources of concern: the Israeli finance ministry’s 2021 explicit statement that Nimbus would be used by the ministry of defense; the nature of the services likely available to the Israeli government within Google’s cloud; and the apparent inability of Google to monitor what Israel might be doing with its technology. Workers worry that Google’s powerful AI and cloud computing tools could be used for surveillance, military targeting, or other forms of weaponization. Under the terms of the contract, Google and Amazon reportedly cannot prevent particular arms of the government, including the Israeli military, from using their services, and cannot cancel the contract due to public pressure. Recent reports in the Israeli press indicate that air-strikes are being carried out with the support of an AI targeting system; it is not known which cloud provider, if any, provides the computing infrastructure likely required for such a system to run. Google workers note that for security reasons, tech companies often have very limited insight, if any, into what occurs on the sovereign cloud servers of their government clients. “We don't have a lot of oversight into what cloud customers are doing, for understandable privacy reasons,” says Jackie Kay, a research engineer at Google’s DeepMind AI lab. “But then what assurance do we have that customers aren't abusing this technology for military purposes?” With new revelations continuing to trickle out about AI’s role in Israel’s bombing campaign in Gaza; the recent killings of foreign aid workers by the Israeli military; and even President Biden now urging Israel to begin an immediate ceasefire, No Tech for Apartheid’s members say their campaign is growing in strength. A previous bout of worker organizing inside Google successfully pressured the company to drop a separate Pentagon contract in 2018. Now, in a wider climate of growing international indignation at the collateral damage of Israel’s war in Gaza, many workers see Google’s firing of Hatfield as an attempt at silencing a growing threat to its business. “I think Google fired me because they saw how much traction this movement within Google is gaining,” says Hatfield, who agreed to speak on the record for the first time for this article. “I think they wanted to cause a kind of chilling effect by firing me, to make an example out of me.” Hatfield says that his act of protest was the culmination of an internal effort, during which he questioned Google leaders about Project Nimbus but felt he was getting nowhere. “I was told by my manager that I can't let these concerns affect my work,” he tells TIME. “Which is kind of ironic, because I see it as part of my work. I'm trying to ensure that the users of my work are safe. How can I work on what I'm being told to do, if I don't think it's safe?” Three days after he disrupted the conference, Hatfield was called into a meeting with his Google manager and an HR representative, he says. He was told he had damaged the company’s public image and would be terminated with immediate effect. “This employee disrupted a coworker who was giving a presentation – interfering with an official company-sponsored event,” the Google spokesperson said in a statement to TIME. “This behavior is not okay, regardless of the issue, and the employee was terminated for violating our policies.” Seeing Google fire Hatfield only confirmed to Vidana Abdel Khalek that she should resign from the company. On March 25, she pressed send on an email to company leaders, including CEO Sundar Pichai, announcing her decision to quit in protest over Project Nimbus. “No one came to Google to work on offensive military technology,” the former trust and safety policy employee wrote in the email, seen by TIME, which noted that over 13,000 children had been killed by Israeli attacks on Gaza since the beginning of the war; that Israel had fired upon Palestinians attempting to reach humanitarian aid shipments; and had fired upon convoys of evacuating refugees. “Through Nimbus, your organization provides cloud AI technology to this government and is thereby contributing to these horrors,” the email said. Workers argue that Google’s relationship with Israel runs afoul of the company’s “AI principles,” which state that the company will not pursue applications of AI that are likely to cause “overall harm,” contribute to “weapons or other technologies” whose purpose is to cause injury, or build technologies “whose purpose contravenes widely accepted principles of international law and human rights.” “If you are providing cloud AI technology to a government which you know is committing a genocide, and which you know is misusing this technology to harm innocent civilians, then you're far from being neutral,” Khalek says. “If anything, you are now complicit.” Two workers for Google DeepMind, the company’s AI division, expressed fears that the lab’s ability to prevent its AI tools being used for military purposes had been eroded, following a restructure last year. When it was acquired by Google in 2014, DeepMind reportedly signed an agreement that said its technology would never be used for military or surveillance purposes. But a series of governance changes ended with DeepMind being bound by the same AI principles that apply to Google at large. Those principles haven’t prevented Google signing lucrative military contracts with the Pentagon and Israel. “While DeepMind may have been unhappy to work on military AI or defense contracts in the past, I do think this isn’t really our decision any more,” said one DeepMind employee who asked not to be named because they were not authorized to speak publicly. “Google DeepMind produces frontier AI models that are deployed via [Google Cloud’s Vertex AI platform] that can then be sold to public-sector and other clients.” One of those clients is Israel. “For me to feel comfortable with contributing to an AI model that is released on [Google] Cloud, I would want there to be some accountability where usage can be revoked if, for example, it is being used for surveillance or military purposes that contravene international norms,” says Kay, the DeepMind employee. “Those principles apply to applications that DeepMind develops, but it’s ambiguous if they apply to Google’s Cloud customers.” A Google spokesperson did not address specific questions about DeepMind for this story. Other Google workers point to what they know about Google Cloud as a source of concern about Project Nimbus. The cloud technology that the company ordinarily offers to its clients includes a tool called AutoML that allows a user to rapidly train a machine learning model using a custom dataset. Three workers interviewed by TIME said that the Israeli government could theoretically use AutoML to build a surveillance or targeting tool. There is no evidence that Israel has used Google Cloud to build such a tool, although the New York Times recently reported that Israeli soldiers were using the freely-available facial recognition feature on Google Photos, along with other non-Google technologies, to identify suspects at checkpoints. “Providing powerful technology to an institution that has demonstrated the desire to abuse and weaponize AI for all parts of war is an unethical decision,” says Gabriel Schubiner, a former researcher at Google. “It’s a betrayal of all the engineers that are putting work into Google Cloud.”   A Google spokesperson did not address a question asking whether AutoML was provided to Israel under Project Nimbus. Members of No Tech for Apartheid argue it would be naive to imagine Israel is not using Google’s hardware and software for violent purposes. “If we have no oversight into how this technology is used,” says Rachel Westrick, a Google software engineer, “then the Israeli military will use it for violent means.” “Construction of massive local cloud infrastructure within Israel’s borders, [the Israeli government] said, is basically to keep information within Israel under their strict security,” says Mohammad Khatami, a Google software engineer. “But essentially we know that means we’re giving them free rein to use our technology for whatever they want, and beyond any guidelines that we set.” Current and former Google workers also say that they are fearful of speaking up internally against Project Nimbus or in support of Palestinians, due to what some described as fear of retaliation. “I know hundreds of people that are opposing what’s happening, but there’s this fear of losing their jobs, [or] being retaliated against,” says Khalek, the worker who resigned in protest against Project Nimbus. “People are scared.” Google’s firing of Hatfield, Khalek says, was “direct, clear retaliation… it was a message from Google that we shouldn’t be talking about this.” The Google spokesperson denied that the company's firing of Hatfield was an act of retaliation. Regardless, internal dissent is growing, workers say. “What Eddie did, I think Google wants us to think it was some lone act, which is absolutely not true,” says Westrick, the Google software engineer. “The things that Eddie expressed are shared very widely in the company. People are sick of their labor being used for apartheid.” “We’re not going to stop,” says Zelda Montes, a YouTube software engineer, of No Tech for Apartheid. “I can say definitively that this is not something that is just going to die down. It’s only going to grow stronger.” Correction, April 10 The original version of this story misstated the number of Google staff actively involved in No Tech for Apartheid. It is more than 200, not 40. Write to Billy Perrigo at billy.perrigo@time.com", "nested_links": ["https://time.com/6246119/demis-hassabis-deepmind-interview/"]},
{"type": "article", "header": "Exclusive: Google Workers Revolt Over $1.2 Billion Contract With Israel", "author": "Billy Perrigo", "update_date/publish_date": "April 10, 2024 12:10 PM EDT | April 8, 2024 8:00 AM EDT", "link_to_article": "https://time.com/6964364/exclusive-no-tech-for-apartheid-google-workers-protest-project-nimbus-1-2-billion-contract-with-israel/", "text": "In midtown Manhattan on March 4, Google’s managing director for Israel, Barak Regev, was addressing a conference promoting the Israeli tech industry when a member of the audience stood up in protest. “I am a Google Cloud software engineer, and I refuse to build technology that powers genocide, apartheid, or surveillance,” shouted the protester, wearing an orange t-shirt emblazoned with a white Google logo. “No tech for apartheid!”  The Google worker, a 23-year-old software engineer named Eddie Hatfield, was booed by the audience and quickly bundled out of the room, a video of the event shows. After a pause, Regev addressed the act of protest. “One of the privileges of working in a company which represents democratic values is giving space for different opinions,” he told the crowd. Three days later, Google fired Hatfield. Hatfield is part of a growing movement inside Google that is calling on the company to drop Project Nimbus, a $1.2 billion contract with Israel, jointly held with Amazon. The protest group, called No Tech for Apartheid, now has more than 200 Google employees closely involved in organizing, according to members, who say there are hundreds more workers sympathetic to their goals. TIME spoke to five current and five former Google workers for this story, many of whom described a growing sense of anger at the possibility of Google aiding Israel in its war in Gaza. Two of the former Google workers said they had resigned from Google in the last month in protest against Project Nimbus. These resignations, and Hatfield’s identity, have not previously been reported. No Tech for Apartheid’s protest is as much about what the public doesn’t know about Project Nimbus as what it does. The contract is for Google and Amazon to provide AI and cloud computing services to the Israeli government and military, according to the Israeli finance ministry, which announced the deal in 2021. Nimbus reportedly involves Google establishing a secure instance of Google Cloud on Israeli soil, which would allow the Israeli government to perform large-scale data analysis, AI training, database hosting, and other forms of powerful computing using Google’s technology, with little oversight by the company. Google documents, first reported by the Intercept in 2022, suggest that the Google services on offer to Israel via its Cloud have capabilities such as AI-enabled facial detection, automated image categorization, and object tracking. Further details of the contract are scarce or non-existent, and much of the workers’ frustration lies in what they say is Google’s lack of transparency about what else Project Nimbus entails and the full nature of the company’s relationship with Israel. Neither Google, nor Amazon, nor Israel, has described the specific capabilities on offer to Israel under the contract. In a statement, a Google spokesperson said: “We have been very clear that the Nimbus contract is for workloads running on our commercial platform by Israeli government ministries such as finance, healthcare, transportation, and education. Our work is not directed at highly sensitive or classified military workloads relevant to weapons or intelligence services.” All Google Cloud customers, the spokesperson said, must abide by the company's terms of service and acceptable use policy. That policy forbids the use of Google services to violate the legal rights of others, or engage in “violence that can cause death, serious harm, or injury.” An Amazon spokesperson said the company “is focused on making the benefits of our world-leading cloud technology available to all our customers, wherever they are located,\" adding it is supporting employees affected by the war and working with humanitarian agencies. The Israeli government did not immediately respond to requests for comment. There is no evidence Google or Amazon’s technology has been used in killings of civilians. The Google workers say they base their protests on three main sources of concern: the Israeli finance ministry’s 2021 explicit statement that Nimbus would be used by the ministry of defense; the nature of the services likely available to the Israeli government within Google’s cloud; and the apparent inability of Google to monitor what Israel might be doing with its technology. Workers worry that Google’s powerful AI and cloud computing tools could be used for surveillance, military targeting, or other forms of weaponization. Under the terms of the contract, Google and Amazon reportedly cannot prevent particular arms of the government, including the Israeli military, from using their services, and cannot cancel the contract due to public pressure. Recent reports in the Israeli press indicate that air-strikes are being carried out with the support of an AI targeting system; it is not known which cloud provider, if any, provides the computing infrastructure likely required for such a system to run. Google workers note that for security reasons, tech companies often have very limited insight, if any, into what occurs on the sovereign cloud servers of their government clients. “We don't have a lot of oversight into what cloud customers are doing, for understandable privacy reasons,” says Jackie Kay, a research engineer at Google’s DeepMind AI lab. “But then what assurance do we have that customers aren't abusing this technology for military purposes?” With new revelations continuing to trickle out about AI’s role in Israel’s bombing campaign in Gaza; the recent killings of foreign aid workers by the Israeli military; and even President Biden now urging Israel to begin an immediate ceasefire, No Tech for Apartheid’s members say their campaign is growing in strength. A previous bout of worker organizing inside Google successfully pressured the company to drop a separate Pentagon contract in 2018. Now, in a wider climate of growing international indignation at the collateral damage of Israel’s war in Gaza, many workers see Google’s firing of Hatfield as an attempt at silencing a growing threat to its business. “I think Google fired me because they saw how much traction this movement within Google is gaining,” says Hatfield, who agreed to speak on the record for the first time for this article. “I think they wanted to cause a kind of chilling effect by firing me, to make an example out of me.” Hatfield says that his act of protest was the culmination of an internal effort, during which he questioned Google leaders about Project Nimbus but felt he was getting nowhere. “I was told by my manager that I can't let these concerns affect my work,” he tells TIME. “Which is kind of ironic, because I see it as part of my work. I'm trying to ensure that the users of my work are safe. How can I work on what I'm being told to do, if I don't think it's safe?” Three days after he disrupted the conference, Hatfield was called into a meeting with his Google manager and an HR representative, he says. He was told he had damaged the company’s public image and would be terminated with immediate effect. “This employee disrupted a coworker who was giving a presentation – interfering with an official company-sponsored event,” the Google spokesperson said in a statement to TIME. “This behavior is not okay, regardless of the issue, and the employee was terminated for violating our policies.” Seeing Google fire Hatfield only confirmed to Vidana Abdel Khalek that she should resign from the company. On March 25, she pressed send on an email to company leaders, including CEO Sundar Pichai, announcing her decision to quit in protest over Project Nimbus. “No one came to Google to work on offensive military technology,” the former trust and safety policy employee wrote in the email, seen by TIME, which noted that over 13,000 children had been killed by Israeli attacks on Gaza since the beginning of the war; that Israel had fired upon Palestinians attempting to reach humanitarian aid shipments; and had fired upon convoys of evacuating refugees. “Through Nimbus, your organization provides cloud AI technology to this government and is thereby contributing to these horrors,” the email said. Workers argue that Google’s relationship with Israel runs afoul of the company’s “AI principles,” which state that the company will not pursue applications of AI that are likely to cause “overall harm,” contribute to “weapons or other technologies” whose purpose is to cause injury, or build technologies “whose purpose contravenes widely accepted principles of international law and human rights.” “If you are providing cloud AI technology to a government which you know is committing a genocide, and which you know is misusing this technology to harm innocent civilians, then you're far from being neutral,” Khalek says. “If anything, you are now complicit.” Two workers for Google DeepMind, the company’s AI division, expressed fears that the lab’s ability to prevent its AI tools being used for military purposes had been eroded, following a restructure last year. When it was acquired by Google in 2014, DeepMind reportedly signed an agreement that said its technology would never be used for military or surveillance purposes. But a series of governance changes ended with DeepMind being bound by the same AI principles that apply to Google at large. Those principles haven’t prevented Google signing lucrative military contracts with the Pentagon and Israel. “While DeepMind may have been unhappy to work on military AI or defense contracts in the past, I do think this isn’t really our decision any more,” said one DeepMind employee who asked not to be named because they were not authorized to speak publicly. “Google DeepMind produces frontier AI models that are deployed via [Google Cloud’s Vertex AI platform] that can then be sold to public-sector and other clients.” One of those clients is Israel. “For me to feel comfortable with contributing to an AI model that is released on [Google] Cloud, I would want there to be some accountability where usage can be revoked if, for example, it is being used for surveillance or military purposes that contravene international norms,” says Kay, the DeepMind employee. “Those principles apply to applications that DeepMind develops, but it’s ambiguous if they apply to Google’s Cloud customers.” A Google spokesperson did not address specific questions about DeepMind for this story. Other Google workers point to what they know about Google Cloud as a source of concern about Project Nimbus. The cloud technology that the company ordinarily offers to its clients includes a tool called AutoML that allows a user to rapidly train a machine learning model using a custom dataset. Three workers interviewed by TIME said that the Israeli government could theoretically use AutoML to build a surveillance or targeting tool. There is no evidence that Israel has used Google Cloud to build such a tool, although the New York Times recently reported that Israeli soldiers were using the freely-available facial recognition feature on Google Photos, along with other non-Google technologies, to identify suspects at checkpoints. “Providing powerful technology to an institution that has demonstrated the desire to abuse and weaponize AI for all parts of war is an unethical decision,” says Gabriel Schubiner, a former researcher at Google. “It’s a betrayal of all the engineers that are putting work into Google Cloud.”   A Google spokesperson did not address a question asking whether AutoML was provided to Israel under Project Nimbus. Members of No Tech for Apartheid argue it would be naive to imagine Israel is not using Google’s hardware and software for violent purposes. “If we have no oversight into how this technology is used,” says Rachel Westrick, a Google software engineer, “then the Israeli military will use it for violent means.” “Construction of massive local cloud infrastructure within Israel’s borders, [the Israeli government] said, is basically to keep information within Israel under their strict security,” says Mohammad Khatami, a Google software engineer. “But essentially we know that means we’re giving them free rein to use our technology for whatever they want, and beyond any guidelines that we set.” Current and former Google workers also say that they are fearful of speaking up internally against Project Nimbus or in support of Palestinians, due to what some described as fear of retaliation. “I know hundreds of people that are opposing what’s happening, but there’s this fear of losing their jobs, [or] being retaliated against,” says Khalek, the worker who resigned in protest against Project Nimbus. “People are scared.” Google’s firing of Hatfield, Khalek says, was “direct, clear retaliation… it was a message from Google that we shouldn’t be talking about this.” The Google spokesperson denied that the company's firing of Hatfield was an act of retaliation. Regardless, internal dissent is growing, workers say. “What Eddie did, I think Google wants us to think it was some lone act, which is absolutely not true,” says Westrick, the Google software engineer. “The things that Eddie expressed are shared very widely in the company. People are sick of their labor being used for apartheid.” “We’re not going to stop,” says Zelda Montes, a YouTube software engineer, of No Tech for Apartheid. “I can say definitively that this is not something that is just going to die down. It’s only going to grow stronger.” Correction, April 10 The original version of this story misstated the number of Google staff actively involved in No Tech for Apartheid. It is more than 200, not 40. Write to Billy Perrigo at billy.perrigo@time.com", "nested_links": ["https://time.com/6246119/demis-hassabis-deepmind-interview/"]},
{"type": "article", "header": "Google DeepMind Unveils Its Most Powerful AI Offering Yet", "author": "Will Henshall", "update_date/publish_date": "December 6, 2023 12:10 PM EST", "link_to_article": "https://time.com/6343450/gemini-google-deepmind-ai/", "text": "Google DeepMind has announced its much-anticipated family of artificial intelligence chatbots, Gemini, which will compete with OpenAI’s GPT series. According to Google, Gemini Ultra, its largest and most capable new model, outperforms OpenAI’s most capable model, GPT-4, at a number of text-based, image-based, coding, and reasoning tasks. Gemini Ultra will be available through a new AI chat feature called Bard Advanced from early next year, the company said. It is currently being refined and is undergoing “trust and safety checks, including red-teaming by trusted external parties,” according to the announcement. Google DeepMind also announced the launch of Gemini Pro, which is now available to the public through Google’s Bard chat interface, and the smaller Gemini Nano, which will run on Google’s Pixel 8 Pro smartphone. All three models can process text, images, audio, and video and produce text and image outputs. Google will start to integrate Gemini models into its other products and services, such as internet search and advertisements. From Dec. 13, developers will be able to access Gemini Pro through an API, and Android developers will be able to build with Gemini Nano. The rollout will put the Gemini suite up against rivals including OpenAI, Anthropic, Inflection, Meta and Elon Musk’s xAI. DeepMind was founded by Demis Hassabis, Shane Legg, and Mustafa Suleyman in 2010. Google acquired the AI lab for $400 million in 2014, and in April 2023 DeepMind merged with Google’s elite AI research team, Google Brain, to form Google DeepMind, which Hassabis leads. Read more: TIME100 Most Influential Companies 2023: Google DeepMind A year after the acquisition, DeepMind’s founders began negotiating to try and obtain greater independence from their new parent company. In 2017, DeepMind’s founders reportedly tried to break away from Google but failed. In 2020, the founders reportedly pushed for a new legal structure to ensure that powerful AI wasn’t controlled by a single corporate entity, even hiring an outside lawyer to help draft the structure, but according to the Wall Street Journal, the proposed structure didn’t make financial sense for Alphabet. Google and Google DeepMind have been responsible for a number of the most important AI breakthroughs of the last decade, including AlphaGo, which mastered the complex game Go, inventing the transformer architecture that powers today's chatbots, and solving the protein folding problem with AlphaFold. But the tech giant has lagged behind competitors such as OpenAI and Anthropic in the era of AI chatbots. A paper from 2021 suggests that DeepMind developed a chatbot, Gopher, as early as December 2020. Google DeepMind chief operating office Lila Ibrahim told TIME that DeepMind decided not to release Gopher because it often gave factually inaccurate responses—a tendency referred to in the industry as ‘hallucinating.’ Before the DeepMind-Google Brain merger, a DeepMind project codenamed Goodall was working to build a ChatGPT competitor, although this was dropped in order to focus on Gemini, The Information reported in August. Read more: CEO of the Year 2023: Sam Altman Google announced its own chatbot, Bard, in February 2023, but parent Alphabet’s stock price dropped after analysts judged it to be inferior to competitors. In May it released PaLM 2, an improvement on Bard but judged by commentators to be inferior to GPT-4. While Google has been slower to bring a consumer AI product out, it has been on the mind of its biggest competitor. Microsoft’s partnership with OpenAI gives it privileged access to OpenAI’s AI models. After the software behemoth announced that it was incorporating OpenAI’s models into Bing search engine, CEO Satya Nadella told The Verge in an interview that he thought AI could help his company challenge Google’s internet search dominance, and that he expects a reaction from Google. “I want people to know that we made them dance,” he said. (Data from analytics firm StatCounter suggests that Google has retained its hegemony in search, although a Microsoft executive disputed this in an interview with the Wall Street Journal in August). Google DeepMind said in the announcement that it had compared Gemini Ultra to a range of competitor models—OpenAI’s GPT-4, Anthropic’s Claude 2, Inflection’s Inflection-2, Meta’s Llama 2 and xAI’s Grok 1—finding that the large language model outperforms those rivals on tests including professional and academic multiple choice questions and Python coding. Write to Will Henshall at will.henshall@time.com", "nested_links": ["https://time.com/collection/best-inventions-2023/6326986/openai-gpt-4/", "https://time.com/6294278/elon-musk-xai/", "https://time.com/collection/time100-ai/6309001/demis-hassabis-ai/", "https://time.com/collection/time100-ai/6310659/shane-legg/", "https://time.com/collection/time100-ai/6309466/mustafa-suleyman/", "https://time.com/collection/time100-companies-2023/6285211/google-deepmind/", "https://time.com/4252312/googles-artificial-intelligence-beats-legendary-go-player/", "https://time.com/6201423/deepmind-alphafold-proteins/", "https://time.com/collection/time100-ai/6308968/lila-ibrahim/", "https://time.com/6342827/ceo-of-the-year-2023-sam-altman/", "https://time.com/6295523/claude-2-anthropic-chatgpt/", "https://time.com/6294278/elon-musk-xai/"]},
{"type": "article", "header": "The Secret Cost of Google’s Data Centers: Billions of Gallons of Water to Cool Servers", "author": "Nikitha Sattiraju / Bloomberg", "update_date/publish_date": "April 2, 2020 1:29 AM EDT", "link_to_article": "https://time.com/5814276/google-data-centers-water/", "text": "In August 2019, the Arizona Municipal Water Users Association built a 16-foot pyramid of jugs in its main entrance in Phoenix. The goal was to show residents of this desert region how much water they each use a day—120 gallons—and to encourage conservation. “We must continue to do our part every day,” executive director Warren Tenney wrote in a blog post. “Some of us are still high-end water users who could look for more ways to use water a bit more wisely.” A few weeks earlier in nearby Mesa, Google proposed a plan for a giant data center among the cacti and tumbleweeds. The town is a founding member of the Arizona Municipal Water Users Association, but water conservation took a back seat in the deal it struck with the largest U.S. internet company. Google is guaranteed 1 million gallons a day to cool the data center, and up to 4 million gallons a day if it hits project milestones. If that was a pyramid of water jugs, it would tower thousands of feet into Arizona’s cloudless sky. Alphabet’s Google is building more data centers across the U.S. to power online searches, web advertising and cloud services. The company has boasted for years that these huge computer-filled warehouses are energy efficient and environmentally friendly. But there’s a cost that the company tries to keep secret. These facilities use billions of gallons of water, sometimes in dry areas that are struggling to conserve this limited public resource. “Data centers are expanding, they’re going everywhere. They need to be built in a way that ensures they are not taking critical resources away from water-scarce communities,” said Gary Cook, global climate campaigns director at Stand.earth, an environmental advocacy group. Google considers its water use a proprietary trade secret and bars even public officials from disclosing the company’s consumption. But information has leaked out, sometimes through legal battles with local utilities and conservation groups. In 2019 alone, Google requested, or was granted, more than 2.3 billion gallons of water for data centers in three different states, according to public records posted online and legal filings. Clashes over the company’s water use may increase as it chases Amazon.com Inc. and Microsoft Corp. in the booming cloud-computing market. Google has 21 data center locations currently. After pumping $13 billion into offices and data centers in 2019, it plans to spend another $10 billion across the U.S. this year. “The race for data centers to keep up with it all is pretty frantic,” said Kevin Kent, chief executive officer of consulting firm Critical Facilities Efficiency Solutions. “They can’t always make the most environmentally best choices.” Google often puts data centers close to large population hubs to help its web services respond quickly. Sometimes that means building in hot and dry regions. The processing units inside heat up easily and water is needed to cool them down. “We strive to build sustainability into everything we do,” said Gary Demasi, senior director of energy and location operations at Google. “We’re proud that our data centers are some of the most efficient in the world, and we have worked to reduce their environmental impact even as demand for our products has dramatically risen.” In Red Oak, Texas, a town about 20 miles south of Dallas, Google wants as much as 1.46 billion gallons of water a year for a new data center by 2021, according to a legal filing. Ellis County, which includes Red Oak and roughly 20 other towns, will need almost 15 billion gallons this year for everything from irrigation to residential use, data from the Texas Water Development Board show. Many parts of Texas are already seeing high water demand, according to Venki Uddameri, director of the water resources center at Texas Tech University. “With climate change, we are expected to have more prolonged droughts,” he said. “These kinds of water-intensive operations add to the local stress.” Water-scarce cities have to make trade-offs between conservation and economic development, and cash-rich Google is a big draw. “It’s a constant battle in Texas because of wanting both,” said Uddameri. In August, Google filed a petition with the Public Utility Commission of Texas to strip a local utility in Red Oak, Rockett Special Utility District, of its federal right to be the sole water supplier to the property. Google said it filed the petition after Rockett confirmed it doesn’t have the capacity to meet the company’s demands. If approved, the petition would let Google get water from another provider. Rockett contested this in a legal response and said Google provided little information on how the water will be used, both in its application to the utility and in “vague” conversations involving company representatives. Despite that, Google made “incessant” requests for the utility to assess if it can meet the company’s water needs, Rockett said in legal filings. Google paid Rockett to do a report on whether the utility could provide enough water for the project. That report has not been submitted and the internet company has been pressing the utility to complete it, according to Google. Rockett brought a case against Texas’ public utility commissioners for refusing to dismiss Google’s petition despite being aware of the utility’s rights. A Google entity, Alamo Mission LLC, is named as a defendant in the case. Lawyers for Rockett declined to comment on the ongoing case. Google says it’s not the only one looking for an alternative to Rockett. Another development in Red Oak is also seeking an alternate water supply, according to the company. The planned data center in Red Oak would be Google’s second in Texas. It struck a deal with the city in July 2019. Red Oak officials told residents about Google’s plans ahead of time, according to Todd Fuller, the city manager. There wasn’t much concern about the impact the data center could have on local resources including water, according to Fuller. “Our water system is pretty robust,” he said, adding that the city doesn’t use its full water capacity. Red Oak isn’t so laid back about water use on its website, though. On a page dedicated to water conservation, the city says it gets half its water supply from Dallas and encourages residents to reduce water use because Dallas’ six reservoirs are 18% depleted. Mandatory water restrictions will kick in if those sources become 35% depleted. Fuller did not respond to requests for comment on the matter. Google said it doesn’t use all the water it requests, but the company must make sure enough is available for periods of high demand, or when the weather’s particularly hot. That’s necessary to keep internet services reliable, according to the company. Google’s data center water use became a subject of controversy last year in Berkeley County, South Carolina. An environmental group opposed the company’s request for 1.5 million gallons of groundwater a day from what it said was a “historically threatened” source. The company has also worked with Berkeley County Water & Sanitation to get 5 million gallons a day from the Charleston Water system. Google said its share of this supply is far less than 5 million gallons a day, with the rest available for the broader community. Google has been trying to secure the 1.5 million gallons—triple the daily amount it’s currently allowed in Berkeley County—since 2016. The Coastal Conservation League took issue with Google’s refusal to share information on how it will be using the extra water. Despite the opposition, the South Carolina Department of Health and Environmental Control granted Google’s request, triggering a backlash from some residents. The conservation league called out the DHEC for giving Google so much water while asking a local public utility, Mount Pleasant Waterworks, to reduce its withdrawal from the aquifer by 57% over the next four years. The utility exceeded its previous peak use demand by 25% in May 2019, one of the driest months last year in Berkeley County, according to Clay Duffie, general manager of Mount Pleasant Waterworks. “It’s unfair that the DHEC is asking us to reduce our water withdrawal while someone like Google can come in and ask for three times more than their original permit and get it,” Duffie said. Google eventually backed off its groundwater request and reached an agreement with the league to only use it as a last resort. The deal still lets the company withdraw groundwater if there’s a shortfall, when conducting maintenance, or when demand exceeds available potable or storm water supplies during peak user activity. The Arizona town of Mesa, where Google plans a 750,000 square-foot data center, gets half its water from the drought-prone Colorado River. A contingency plan was signed into law last year requiring states dependent on the river to take voluntary conservation measures. Still, Mesa officials say they remain confident about future supply while continuing to remind residents to limit their water consumption. “We do not have any immediate concerns,” said Kathy Macdonald, a water resources planning adviser with the city. In 2019, Mesa used 28 billion gallons of water, according to Macdonald. City officials expect that to reach 60 billion gallons a year by 2040, a demand Mesa is capable of meeting, she said. Big companies like Google wouldn’t locate to the city if it couldn’t meet their water demands, Macdonald said. Mesa passed an ordinance in 2019 to ensure sustainable water use by large operations and fine them if they exceed their allowance. Google has toiled for years to reduce the carbon footprint of data centers. Today, the facilities churn out a lot more computer power for every watt of energy used. In its 2019 environmental report, the company argued that reducing its energy use also makes it more water-efficient. “Generating electricity requires water, so the less energy we use to power our data centers, the less water we use as well,” it said. However, data center experts say there’s usually a trade-off between water and energy use. “If the water consumption goes down, energy consumption goes up and vice versa,” said Otto Van Geet, a principal engineer at the National Renewable Energy Laboratory. Google relies on “evaporative cooling,” which evaporates water to cool the air around the processing units stacked inside data centers, according to its environmental report. The most common systems, known as computer room air conditioners, are energy intensive. Evaporative cooling uses less energy, but the process requires more water. Operators will often embrace the thirstier approach because it’s less expensive, said Cook from Stand.earth. “Water’s cheap. In many places, the energy costs are much higher” he added. In a data center application the company filed in Henderson, Nevada, in 2018, Google’s considerations included utility and real estate costs, tax incentives and availability of qualified workers. Google has paid more attention to water use in recent years. It relies on recycled water or seawater where it can to avoid using drinking water or draining local supplies. Google also says it saves water by recirculating it through cooling systems multiple times. In Mesa, the company is working with authorities on a water credits program, but said it’s too early to share more details. From 2007 to 2012, Google used regular drinking water to cool its data center in Douglas County, just outside Atlanta. After realizing the water “didn’t need to be clean enough to drink,” the company shifted to recycled water to help conserve the nearby Chattahoochee River. It’s difficult to use similar approaches for other data center locations because the required technology isn’t always available, according to the company. “The Chattahoochee provides drinking water, public greenspace and recreational activities for millions of people,” the company said in a blog post at the time. “We’re glad to do our part in creating an environmentally sustainable economy along the shores of the Hooch.” –With assistance from Mark Bergen. Contact us at letters@time.com", "nested_links": []},
{"type": "article", "header": "20 Years Of Google Has Changed the Way We Think. Here’s How, According to a Historian of Information", "author": "Lily Rothman", "update_date/publish_date": "September 4, 2018 11:15 AM EDT", "link_to_article": "https://time.com/5383389/google-history-search-information/", "text": "It might sound obvious to say that Google has changed the world in the 20 years since it was founded on Sept. 4, 1998. Google, and its parent company Alphabet, is involved in everything from the development of driverless cars to disputes with President Donald Trump. But, to James W. Cortada, author of All the Facts: A History of Information in the United States since 1870, there are also lots of ways in which Google, as the dominant search engine, hasn’t really changed anything. In the scheme of the whole history of people searching for information, its has merely continued many trends that have been happening for centuries. One of those trends, for example, is that it’s increasingly safe to assume that any information you want to search for — the original role of Google — is out there somewhere, probably written down. “When you learn to be literate, you learn that there’s data and information that’s organized in a logical way in something called a book. And you learn more than just what you read; you learn how information is structured,” Cortada explains. “The more people are reading, the more they’re also writing, and therefore organizing ever larger bodies of information. After a while, certainly by the 18th century, people were assuming, ‘I don’t know what the book is on this topic but I bet you there is a book out there.'” In the United States — the history of which roughly corresponds with this trend toward mass literacy — Congress recognized that trend and encouraged its spread. The government facilitated information access by allowing inexpensive postal delivery of newspapers in the nation’s early days, and later by publishing its own free information for citizens on matters such as home economics. Meanwhile, the technology that spread that information became cheaper and easier to get. More and more, the answers weren’t merely out there somewhere, but actually accessible nearby. Internet search and Google’s improvement of the technology, to Cortada, is just another step in that direction. The only difference here is frequency and quantity, Cortada says. The content of the information we’re searching for (health tips, recipes, news) is a continuation of a long trend, too. “By the time Google comes along, the big news there is that it’s offering a different platform and format for getting the kinds of information that people have been looking up for 200 years,” he says. That said, there is something new in the internet-search era — and it’s something that’s changing the whole way we interact with information. “Today, it’s almost as if we went to the index of a book first rather than a table of contents,” he explains. Here’s how Cortada sees it: Imagine you want to know what year John F. Kennedy was elected President. If you had an encyclopedia, you would look at the entry for JFK and find out the answer (1960), but you’d also have to skim past JFK’s birth date and birth place, and probably lots more. If you had a smartphone and Google, on the other hand, you would specifically look up the year he was elected, and the year is right there at the top. The more we use services like Google, the more our brains organize the world in an index-based fashion. This also means people who make a living providing information are increasingly organizing their presentation to catch eyeballs looking for specific details in indexes. As a result, the way we interact with information is largely more disjointed than it was for our ancestors. “There’s nothing wrong with an index. I use it all the time, for the same reasons I use a Google search,” Cortada says. “But one of the problems we have — and I don’t blame Google for this, it’s just the nature of the Internet — is that a lot of the time people will have bits and pieces of data flotsam, or whatever you want to call it, little pieces floating around in the ocean. There’s no connection other than the connection you want to make.” If you’re an expert on Kennedy, for instance, you bring all sorts of context to the 1960 answer. If you don’t know anything about him, the year doesn’t tell you much. And if the answer that pops up is wrong, you have no reason to question it. If it clashes with your prior but mistaken notion of the timeline, you might decide the answer is wrong even if it’s not. You got your answer in an efficient manner, but you’re relying on your own prior knowledge to digest it. The fact that effective use of internet search requires knowing how to navigate the results of a computer algorithm isn’t new. In the past, for example, you might have brought to bear your knowledge of your neighbors’ levels of education before you decided whom you’d ask a tough question. Nor is the fact that preconceived notions affect how people process information. That’s true of everyone, as much for you as for the person who wrote the algorithm that delivers your answer online. But, the more we receive information in independent chunks, the more important it is to be aware of those factors. “There’s a whole new literacy that we need if we’re going to live in an index-driven world,” Cortada says. “There’s a different type of literacy that we need and we haven’t articulated what the features are of that literacy.” Artificial intelligence might help — one day, Cortada hopes, AI might be able to tell whether you already know all about Kennedy or not — but he thinks there’s also hope for human beings. “Information is socially conditioned, as is its reception. How do we run an index-based system with that reality? I’m not sure we know the answer to that yet,” he says. “I’m 71 years old and I’ll let you in on a little secret: You can get smarter over time, and it’s not by piling more crap into your brain. It’s because you become more discerning and discriminating and know where to go to get information — and that information can be molded and shaped.” Write to Lily Rothman at lily.rothman@time.com", "nested_links": ["http://time.com/4011935/google-firsts/", "http://time.com/3250807/google-anniversary/", "http://time.com/money/3991769/google-alphabet-4-things-to-know/", "http://time.com/5382315/trump-google-silicon-valley-fight/", "http://time.com/5318918/search-results-engine-google-bias-trusted-sources/", "http://time.com/5362183/the-real-fake-news-crisis/"]},
{"type": "article", "header": "Google vs. Death", "author": "Harry McCracken", "update_date/publish_date": "September 30, 2013 11:47 AM EDT", "link_to_article": "https://time.com/574/google-vs-death/", "text": "In person, it can be a little hard to hear Larry Page. That’s because he has nerve damage in both vocal cords: one was paralyzed about 14 years ago, the other left with limited movement after a cold last summer. This rare condition doesn’t slow him down, though it has made his voice raspy and faint. You have to listen carefully. But it’s generally worth it. Page, 40, is the co-founder and CEO of one of the most successful, ubiquitous and increasingly strange companies on the planet. Google is, of course, in the search business, and more important for its profitability, it is in the online-advertising business. But it’s also in the mobile-operating-system business, the Web-browser business, the free-e-mail business, the driverless-car business, the wearable-computing business, the online-map business, the renewable-energy business and the business of providing Internet access to remote areas via high-altitude balloons, among countless others. Google’s corporate strategy is one part mainstream services and one part risky long shots. Page prefers to refer to Google’s more out-there ventures as moon shots. “I’m not proposing that we spend all of our money on those kinds of speculative things,” he says during a rare interview at the Googleplex, the company’s Mountain View, Calif., headquarters. “But we should be spending a commensurate amount with what normal types of companies spend on research and development and spend it on things that are a little more long term and a little more ambitious than people normally would. More like moon shots.” This is why Google, in Page’s words, is not a normal type of company. At the moment Google is working on an especially uncertain and distant shot. It is launching Calico, a new company that will focus on health and aging in particular. The independent firm will be run by Arthur Levinson, former CEO of biotech pioneer Genentech, who will also be an investor. Levinson, who began his career as a scientist and has a Ph.D. in biochemistry, plans to remain in his current roles as the chairman of the board of directors for both Genentech and Apple, a position he took over after its co-founder Steve Jobs died in 2011. In other words, the company behind YouTube and Google+ is gearing up to seriously attempt to extend the human life span. Google isn’t exactly bursting with credibility in this arena. Its personal-medical-record service, Google Health, failed to catch on. But Calico, the company says, is different. It will be making longer-term bets than most health care companies do. “In some industries,” says Page, who spoke exclusively with TIME about the new venture, “it takes 10 or 20 years to go from an idea to something being real. Health care is certainly one of those areas. We should shoot for the things that are really, really important, so 10 or 20 years from now we have those things done.” It’s worth pointing out that there is no other company in Silicon Valley that could plausibly make such an announcement. Smaller outfits don’t have the money; larger ones don’t have the bones. Apple may have set the standard for surprise unveilings, but excepting a major new product every few years, these mostly qualify as short term. Google’s modus operandi, in comparison, is gonzo airdrops into deep “Wait, really?” territory. Last week Apple announced a new iPhone; what did you do this week, Google? Oh, we founded a company that might one day defeat death itself. The unavoidable question this raises is why a company built on finding information and serving ads next to it is spending untold amounts on a project that flies in the face of the basic fact of the human condition, the existential certainty of aging and death. To which the unavoidable answer is another question: Who the hell else is going to do it? New Horizon Google’s fondness for moon shots, and its ability to take them, can be attributed in large part to Page himself. When he was a Stanford computer-science grad student, his insight that the most relevant Web pages are those with the most links to them became the basis of a remarkably precise search engine, which he created with fellow student Sergey Brin. Google became a company in 1998 and a phenomenon shortly thereafter. Page served as its chief executive until 2001, when tech veteran Eric Schmidt was brought in from software giant Novell. Even then, the unconventional troika of Page, Brin and Schmidt raised eyebrows, but the power sharing led to Google’s monster growth years. In April 2011, Page reclaimed the CEO title, and Schmidt became executive chairman. The effect of Page’s leadership at Google was immediately clear. In 2012 the company closed a massive $12.5 billion acquisition of troubled handset maker Motorola Mobility in a bid to begin manufacturing its own hardware. Page also reshaped Google’s management structure, creating the so-called L Team (L for Larry) of top managers. There were notable departures, including employee No. 20, Marissa Mayer, who left to run Yahoo. Most important, Page has shown that Google, long criticized as a one-trick pony dependent on serving ads, can grow its other businesses. Most of its $50 billion in revenue still comes from search-related ads. Analysts estimate that YouTube is a $4 billion business, and Android, its mobile operating system, is estimated to bring in an additional $6.8 billion from use on smartphones. Beyond all that is the simple fact that Page is uncommonly ambitious and impatient, and he wants the company he created to be as well. “For me, it was always unsatisfying if you look at companies that get very big and they’re just doing one thing,” says Page. “Ideally, if you have more people and more resources, you can get more things solved. We’ve kind of always had that philosophy.” Longtime Google observers tend to agree. “Guys like Larry don’t focus on preserving value; they just work on building new value,” says Ben Horowitz, co-founder of venture-capital firm Andreessen Horowitz. “It’s the advantage of having made something from nothing.” Google has never tried to solve anything quite as far afield as, well, mortality. The idea of treating aging as a disease rather than a mere fact of life is an old one–at least as a fantasy. And as a science? The American Academy of Anti-Aging Medicine has been around since 1992, but the discipline it represents has yet to gain much of a foothold in mainstream medicine. Research has been slow to generate results. Consider Sirtris Pharmaceuticals, a Cambridge, Mass., company built around a promising drug called SRT501, a proprietary form of resveratrol, the substance found in red wine and once believed to have anti-aging properties. In 2008, GlaxoSmithKline snapped up Sirtris for $720 million. By 2010, with no marketable drug in sight and challenges to existing resveratrol research, GlaxoSmithKline shut down trials. Other anti-aging initiatives exist purely as nonprofits with no immediate plans for commercial products. Why would Google be able to get traction on aging when huge pharmaceutical companies haven’t? Page himself doesn’t oversell his knowledge of the industry. “I don’t have as much personal expertise in the technology,” he admits. “I have some knowledge of it, just being in Silicon Valley.” Google has invested in gene-sequencing company 23andMe, a startup co-founded by Anne Wojcicki, Brin’s wife. And in February, Levinson and Brin joined Facebook founder Mark Zuckerberg and Russian entrepreneur Yuri Milner in organizing the $33 million Breakthrough Prize in Life Sciences, to “recognize excellence in research aimed at curing intractable diseases and extending human life.” It’s a lot easier to take Google’s venture seriously if you live under the invisible dome over Silicon Valley, home to a worldview whereby, broadly speaking, there is no problem that can’t be addressed by the application of liberal amounts of technology and everything is solvable if you reduce it to data and then throw enough processing power at it. The twist is that the technophiles are right, at least up to a point. Medicine is well on its way to becoming an information science: doctors and researchers are now able to harvest and mine massive quantities of data from patients. And Google is very, very good with large data sets. While the company is holding its cards about Calico close to the vest, expect it to use its core data-handling skills to shed new light on familiar age-related maladies. Sources close to the project suggest it will start small and focus entirely on researching new technologies. When will that lead to something Google might actually sell? It’s anybody’s guess. What’s certain is that looking at medical problems through the lens of data and statistics, rather than simply attempting to bring drugs to market, can produce startlingly counterintuitive opinions. “Are people really focused on the right things?” Page muses. “One of the things I thought was amazing is that if you solve cancer, you’d add about three years to people’s average life expectancy. We think of solving cancer as this huge thing that’ll totally change the world. But when you really take a step back and look at it, yeah, there are many, many tragic cases of cancer, and it’s very, very sad, but in the aggregate, it’s not as big an advance as you might think.” Page, in other words, is a man for whom solving–not curing–cancer may not be a big enough task. Spring Cleaning Page’s tenure has not been free of controversy. Like some of its competitors in Silicon Valley, Google was caught up in a government-spying scandal earlier this year. Documents leaked by Edward Snowden revealed a National Security Agency data-collection program called Prism, which internal documents claimed offered direct access to the servers of tech firms, including Google. The company denies this characterization. “There’s some misunderstanding, probably, of people assuming we were complicit,” Page says of the ongoing episode. “We’ve tried hard to correct those things. We work very hard to protect your data as a user.” In the wake of the revelations, Page and Schmidt have tried to walk a fine line, calling for greater transparency without explicitly criticizing law enforcement. Page has also concentrated on avoiding flops like Wikipedia knockoff Knol and Google Buzz, a Twitter clone almost nobody wanted to use. He has done this, in part, by ratcheting down the number of new-product introductions and by axing existing projects in periodic “spring cleanings.” (Both of those products, for instance, were killed.) He has, in a memorable phrase, declared his intention to put “more wood behind fewer arrows.” “When Larry co-founded Google, he was not ready to run Google,” explains venture capitalist and early investor John Doerr of Kleiner Perkins Caufield Byers. “Today, I can’t imagine anyone doing it better than Larry Page.” (Doerr sits on Google’s board.) Jeff Jarvis, author of What Would Google Do?, echoes that, noting that Page has been “ruthless” about nixing lackluster ideas. The new role model for Google products turns out to be the oldest one in the store: search. Early incarnations of Google.com slaughtered competitors like AltaVista by being vastly more accurate. Other early hits like Gmail, with its abundant free storage, and Google Maps, with its Street View imagery, succeeded for similar reasons. Google is trying to prove that it can still do that. “Larry pushes us to have 10x innovations–innovation 10 times greater than what we have in the market today,” explains Johanna Wright, a vice president whose portfolio includes one of the company’s most important growth areas, mobile search. Most of the firm’s wildest ideas are dreamed up at Google X, which functions something like Google’s fantastical subconscious. It’s a secretive research arm headquartered a three-minute ride from the main Googleplex on one of the company’s 1,000-plus brightly colored bikes. While Page tends to the entire business as CEO, Brin now devotes much of his attention to X, which he runs in partnership with scientist and entrepreneur Astro Teller. Teller’s title–just to underline the operation’s stratospheric aspirations–is “Captain of Moonshots.” (Teller changed his name from Eric to Astro, a reference to the AstroTurf-like buzz cut he sported in high school.) Except for his long hair, beard and mustache, he’s a dead ringer for his paternal grandfather, physicist Edward Teller, the father of the hydrogen bomb. According to Teller, Google X’s moon shots have three things in common: a significant problem for the world that needs solving, a potential solution and the possibility of breakthrough technology making all the difference. (Making money comes later.) Even a proposed project that meets all these criteria probably won’t make the cut. “Sergey and I being pretty excited about it is a necessary but not sufficient condition,” Teller explains. “Depending on what it is, it might require consulting experts, it might require building prototypes, sometimes even forming a temporary team to see where it goes and then saying to the team, ‘It is your goal to kill this idea as fast as possible.'” Four big Google X efforts are public knowledge. There’s Google Glass, the augmented-reality spectacles that pack a camera and a tiny Web-connected screen you can peek at out of the corner of your right eye and control with your voice and gestures. Makani Power–a startup that the company invested in and then bought outright in May–puts energy-generating wind turbines on flying wings that are tethered to the ground but circle 1,000 ft. in the air. Project Loon aims to deliver Internet access to remote areas of the planet by beaming it wirelessly from 39-ft.-tall helium balloons hovering 12 miles in the sky. Though Calico is a Google X–style long shot, it will be a separate entity from Teller’s shop. But if you had to pick a Google X moon shot with the most plausible chance of permanently reshaping the way we live, it would be the self-driving automobiles. Page first became intrigued by the concept at Stanford in the mid-1990s and seems doleful that the idea was still up for grabs when Google got around to tackling it. “I think one of the big things we did is just tell people, ‘Hey, we’re going to work on it. It’s a big deal. It should be done.’ We said we’re going to actually drive on public roads and we’ll do it safely. We’re going to test it. We’re going to prove these things are possible. That all could have been done 10 years ago.” To date, Google’s robocars, which use lasers and cameras to see other vehicles and even read road signs, have covered half a million miles of road in California, Nevada and Florida. Spotting one cruising alongside you on a Bay Area freeway–always with a human in the driver’s seat just in case–has become a common enough occurrence that it’s no longer that big a whoop. Last fall, Brin declared that the technology would be far enough along for “ordinary people” to experience it within half a decade. How Google might sell the technology was left unanswered. Google Glass, although more fully baked, is also still in a distinctly experimental phase. Brin wears them frequently, and a few thousand people outside of Google own a $1,500 beta version. A few companies, such as Evernote and Twitter, have written apps for the device. Glass won’t go on sale in a less expensive commercial version until next year, but already the way it enables users to see information and capture images is raising concerns about privacy. (Some users report that a common question from strangers is, “Are you recording me?”) Teller says the limited release of Glass is an attempt to jump-start a conversation about the technology before it’s part of everyday life. “I think if we’re aspiring to take moon shots, designing things for today’s cultural norms, on any front, doesn’t make any sense,” he says. “You’re not going to be able to help society in a really big way if you’re fully constrained by those things. But it is also not our mind-set that we’re going to decide what the new cultural norm is.” Unlike present-day NASA missions, Google’s moon shots are unlikely to suffer from underfunding: the company has a $54 billion cash stockpile, not to mention dominant market share in its most important lines of business. But will any of its long-shot projects be the Google cash cows of tomorrow? Maybe. Google X, says Teller, “is not a philanthropic organization.” But neither is it picking projects based on obvious profit potential. “If you make something a little bit better, people might pay you for it; they may not. But if you make the world a radically better place, the money is going to come find you, in a fair and elegant way.” The Core It’s not as though Google’s mainstream services–Search, YouTube, Gmail, Google Maps and Android–are languishing for lack of attention. “It’s funny, you’d think you’d run out of things to do in those core areas,” says Page. “But our core areas are so important to people: access to information, understanding the world, communications, interactions with other people, helping you with your work … It’s incredibly exciting to come in to work every day and work on those things.” Last year Google introduced Knowledge Graph, which lets its search engine understand and answer questions like “How tall is Justin Bieber?” Technologically, it represents an advance as significant as Page and Brin’s original algorithm. Last November, YouTube opened YouTube Space Los Angeles, a sprawling 41,000-sq.-ft. video-production facility housed in a rehabbed 1950 aircraft hangar originally erected by Howard Hughes. Smaller YouTube Spaces have opened in London and Tokyo, with more on the way. “Programmers like to move to Silicon Valley,” says Robert Kyncl, who heads YouTube’s content and business operations. “With creators, it’s New York, L.A., London, Mumbai, Tokyo. Those are their tribal areas. We felt like it was important for us to have our physical tent in those areas.” Page says his primary responsibility is to make sure that the entire organization errs on the side of thinking big. “Larry always asks hard questions,” says Brian McClendon, VP of Google Geo, who joined in 2004 when the company bought his startup and renamed its 3-D-mapping program Google Earth. “Sometimes he asks unreasonably hard questions. He forces you to think, ‘Did I actually get as much out of this plan as I could have?’ He does that even when you do great things.” Despite its founder’s emphasis on focus, Google is not immune to distractions. In August, technology site All Things D reported that Brin and Wojcicki had separated, setting off an unprecedented flurry of coverage of their personal lives. More important, even juggernauts like Apple and Amazon have learned how willing investors are to punish overly long-term bets by driving down a company’s stock price if they are displeased. So far that hasn’t been a problem: Google’s stock hit its all-time high in July at $928 a share. Page concedes that he’s been known to underestimate how tough it all can be: “I’m very optimistic and certain about things, so I always assume they’re going to get done quickly, but actually they take a long time.” He says he initially thought Google could whip up great software for next-generation smartphones in a year but learned that getting Android there was a half-decade effort. Though it did get there: Android is closing in on an 80% share of the smartphone market. Again, he sounds impatient. “Big companies–and maybe even Google too–we’re not as good as we should be at starting these things up early enough so that it’s really done by the time we need it to be a real business.” Though Page doesn’t mention Google+, the company’s attempt to roll an array of services into a cohesive Facebook competitor, it’s a case in point: In 2011 he tied part of every employee’s bonus to their contributions to Google’s social effort. The results, though slick, haven’t yet gelled into anything with a fraction of Facebook’s impact or revenue. The truth about Page’s brand of 10x thinking is that it creates a never-ending cycle. If you believe that it is always possible to be 10 times better than your current self (or the other guy), it is impossible to reach a state of self-satisfaction. Which means that even if Calico, Glass, self-driving cars, Makani Power and Project Loon all turn out to be wild, epoch-shifting hits, success for Google will still be another moon shot or two away. And Page will probably be fretting that the company isn’t moving fast enough to launch them. Contact us at letters@time.com", "nested_links": []},
{"type": "article", "header": "The Real Reason Google Is Buying Fitbit", "author": "Patrick Lucas Austin", "update_date/publish_date": "November 4, 2019 3:17 PM EST", "link_to_article": "https://time.com/5717726/google-fitbit/", "text": "In announcing its planned $2.1 billion acquisition of fitness tracking company Fitbit, Google said the deal will “help spur innovation in wearables” — at least, that’s how Senior Vice President of Devices & Services Rick Osterloh put it in a blog post. If completed, the move would spell the end of an independent Fitbit, a 12-year-old hardware firm credited with popularizing the self-quantifying phenomenon that has so many of us comparing our daily step counts against our friends and loved ones. Google has already spent big money on wearable tech — in 2019, it paid $40 million for technology and personnel from watchmaker Fossil Group’s research and development team, for instance. But the company’s products haven’t matched up to the competition, like the Apple Watch or Samsung’s Galaxy Watch. Fitbit’s technical chops could help Google come up with a wearable to take on its biggest rivals. With the revenue from smartwatch sales industry-wide set to double to $34 billion by 2023, the company’s urgency is understandable. It’s also a familiar play: Google purchased portions of smartphone maker HTC in 2017 for $1.1 billion, jumpstarting production of its Pixel smartphones. But Google already has plenty of hardware and software chops. What else does it get out of the Fitbit deal? The most obvious potential lure is the health data of millions of Fitbit customers. Fitbit devices have been tracking wearers’ health metrics for over a decade, cataloging behaviors like steps taken, calories burned and exercises performed. That’s just the kind of thing Google, fundamentally an advertising company, needs to further build out its profile of, well, you. Advertisers already take educated guesses at your health status, with apps like period trackers sharing your info with Facebook and others. Still, Osterloh promises that “Fitbit health and wellness data will not be used for Google ads.” What else, then, does Fitbit have that’s attractive to Google? Google’s rivals, most notably Apple, have embraced healthcare as the next big battleground in the tech world, attracted by the promises of big profits for those who can help simplify a byzantine system. Google’s healthcare efforts have been decidedly quieter. The healthcare tech space could be worth $24 billion by 2020, according to an estimate from Statista. Through its health-focused Verily subsidy, Google has been working on cardiovascular health, diabetes and more, but it hasn’t been publicly pushing healthcare as a business proposition. Fitbit, however, has been doing exactly that. It’s already working with insurance companies, other firms and even the government of Singapore to provide customers, employees and citizens with fitness trackers in what are likely lucrative deals, for instance. Gartner senior analyst Alan Antin says Google could benefit from Fitbit’s expertise in working alongside corporate partners and other stakeholders in the healthcare world. “There’s the lesser known side business-to-business side of Fitbit, which is their partnerships with health insurance companies and direct corporate wellness programming,” says Gartner senior analyst Alan Antin. “Those are things that are a little bit harder for a company like Google to do.” For his part, Fitbit CEO James Park has said that, for technology companies seeking success in the healthcare world, relationships like Fitbit has are key. “The healthcare system is incredibly complex and it takes working with a lot of different big players to have a big impact,” Park said in an October interview with TIME. “And, you know, our goal is to make this stuff that we’re working on available and accessible to as many people around the world. And we can only do that by working with the largest players in healthcare.” For Google, Fitbit’s healthcare ties, along with its established base of users, might be exactly what it needs to give its wearable device strategy a shot in the arm. “If they wanted to have their own smartwatch, they certainly have the distribution channels, they have all the software and hardware capabilities to do their own, and they could go and enter that market pretty quickly,” says Antin. “But the [healthcare partnerships] I mentioned are ones you can’t really get into quickly. To me, that’s where they saw the bigger value.” Write to Patrick Lucas Austin at patrick.austin@time.com", "nested_links": ["https://time.com/5716260/google-buying-fitbit/", "https://time.com/5716260/google-buying-fitbit/", "https://time.com/5708334/pixel-4-review/", "https://time.com/5716260/google-buying-fitbit/", "https://time.com/5472329/apple-watch-ecg/", "https://time.com/5702746/fitbit-james-park-afib-time-100-summit/"]},
{"type": "article", "header": "Google’s Diversity Efforts Still Have a Long Way to Go", "author": "Lisa Eadicicco", "update_date/publish_date": "July 1, 2016 10:09 AM EDT", "link_to_article": "https://time.com/4391031/google-diversity-statistics-2016/", "text": "Google’s latest diversity statistics reveal the company is making steady but slow progress towards its goal of a more inclusive workplace, though white males still account for the overwhelming majority of its employees. Google says that 69% percent of its employees are now male, while 31% are female. That marks a small increase from the 70% to 30% ratio the company reported last year. But only 19% of Google’s technical roles are held by women, while 81% of them are held by men. That’s also a 1% increase compared to last year. Google has progressed slightly further when it comes to leadership roles: Women hold 24% of leadership positions in the company, up from 22%. The company has made similar progress when it comes to ethnicity. 59% of Google employees are white, while 32% are Asian, 3% are Hispanic, and 2% are black. 70% percent of Google leadership roles and 57% of tech positions are held by white employees. In a blog post, the search giant emphasized its efforts to hire more diverse employees, saying that the percentage of new hires that were black, Hispanic, and female in 2015 was actually higher than the company’s current demographic representation for these groups. For example, 4% of Google’s new hires last year were black, while only 2% of Google’s total workforce is black. Last year, Google said it would spend $150 million in 2015 to promote diversity. Creating a diverse workforce has been a challenge for many major tech companies, including Apple, Facebook, and Yahoo, who have all reported similar gender and ethnic imbalances in the past. Google first began sharing its diversity statistics in 2014, triggering other Silicon Valley giants to do the same. Tech corporations and startups have been vocal about closing the gap when it comes to diversity. 32 tech companies, including Airbnb, Spotify, Intel, Pinterest, and Lyft, have signed a pledge to make their workforce more representative and diverse. Contact us at letters@time.com", "nested_links": []},
{"type": "article", "header": "‘We’re Not As Healthy As We Should Be.’ Fitbit CEO James Park Discusses New AFib Detection Partnership With Bristol-Myers Squibb-Pfizer", "author": "Patrick Lucas Austin", "update_date/publish_date": "October 17, 2019 5:38 PM EDT", "link_to_article": "https://time.com/5702746/fitbit-james-park-afib-time-100-summit/", "text": "“We’re not as healthy as we should be.”  That’s what Fitbit CEO and co-founder James Park said at Thursday’s TIME 100 Health Summit, where MSNBC anchor Stephanie Ruhle interviewed him about the company’s future in the health care space, the impact of wearables and just how active Fitbit’s 30 million active users really are. Ruhle talked with Park about Fitbit’s position as one of the first major wearables company to have gone public, one that led the charge in terms of mass market adoption but has lost ground to competitors offering more advanced wearable devices, as well as the cultural shift away from constantly being tethered to electronics, be it their smartphone or wearable device. “We’re trying to transform ourselves into a behavior-change company to help people manage these more serious conditions,” said Park, “And ultimately for the health care industry address rising costs as well.”  During the interview, Park announced a partnership with Bristol-Myers Squibb-Pfizer Alliance, in which Fitbit will collaborate with the health care and pharmaceutical companies to detect symptoms of atrial fibrillation (AFib), an irregular heartbeat that can cause heart conditions like stroke and blood clots. The BMS-Pfizer Alliance is a joint effort between the two pharmaceutical companies established to raise awareness of atrial fibrillation and educate those affected by the condition. Contrary to efforts from competing companies creating their own AFib detection software and devices, Park thinks alerting people to potentially fatal conditions without educating them about the risks is unduly stressful — and is the wrong approach to encouraging healthy choices. “Just getting an alert can overly alarm people,” said Park. “People don’t know what to do, doctors don’t know what to do with it. So, along with the BMS-Pfizer Alliance, we want to fill in all those gaps and complete that entire healthcare pathway around you.” Fitbit is currently in talks with the U.S. Food and Drug Administration in order to approve the company’s AFib detection software, which will be made available to existing Fitbit products with optical heart-rate sensors necessary for AFib detection.  Fitbit’s user-generated data, combined with the company’s analytics talent, gives the company an edge when it comes to determining how cost-effective certain health-related behavioral changes are. “When you can run sophisticated data science on it you can understand how to influence people’s behavior and realize exactly what the cost savings are that come out of that,” said Park. “We’re starting to unlock that, and that’s whats sparked a lot of interest from employers, health plans, governments. We just announced something with the government of Singapore where Fitbits will be made available free to 20% of their population, and that’s with the understanding that wearing these devices and understanding that data behind it can have profound impacts on people’s health.” Park thinks Fitbit’s focus on health care, along with its work in clinical trials to alert users to conditions like hypertension and sleep apnea, will turn its wearables into more than fun accessories.  “The more of these use cases we have, the more we can translate what we’re doing from a ‘nice to have’ to a ‘must have,’” said Park. “If you can wear a device that could literally save your life why wouldn’t you?” Write to Patrick Lucas Austin at patrick.austin@time.com", "nested_links": ["https://time.com/5227895/fitbit-versa-review-apple-watch/", "https://time.com/5700156/time-100-health-summit-livestream/", "https://time.com/5689155/apple-watch-series-5-review/", "https://time.com/4907284/fitbit-detect-atrial-fibrillation/"]},
{"type": "article", "header": "An Inside Look at Apple’s Biggest Step Yet in Health Care", "author": "Alex Fitzpatrick", "update_date/publish_date": "December 6, 2018 9:10 AM EST", "link_to_article": "https://time.com/5472329/apple-watch-ecg/", "text": "Captain America and Black Panther were about to defend Earth from the villain Thanos when Kevin Foley first noticed something was wrong. Foley, a 46-year-old information-technology worker from Kyle, Texas, was heading into the theater to see Avengers: Infinity War when he realized he was having trouble breathing normally. The sensation struck again during another movie the following night, but more severe this time. Once the credits on the second film rolled, Foley took action: he looked at his wristwatch. It was a bigger step than you might imagine, because Foley was wearing an Apple Watch equipped with medical sensors and experimental software to track basic functions of his heart. And the watch was worried. It had, according to the display, detected signs of an irregular heartbeat. Before long, Foley was in an emergency room, where doctors hooked him up to an electrocardiogram (ECG), which showed that he was in atrial fibrillation, or AFib, an irregular heartbeat that can lead to blood clots, stroke and other potentially fatal complications. Foley spent the next few days in the hospital while doctors worked to return him to a normal sinus heart rhythm–eventually turning to a procedure called electrical cardioversion to shock his heart back to normalcy. Foley is doing fine now. But he believes that, if not for the warning on his watch, he might not have sought help in time. “I would have never known,” he says. Foley and his watch were part of an experiment run by Apple and Stanford’s medical school. But beginning Dec. 6, anyone can get an on-the-fly heart checkup, assuming they’ve shelled out $399 or more for an Apple Watch. That’s when Apple will roll out a software update that turns its latest model, called the Series 4, into a personal ECG, thanks to an innovative new sensor. Though less sophisticated than hospital ECG machines–which typically require sticking 10 different electrodes to the patient’s body–the watch version can nonetheless provide basic information and warnings of potential anomalies worthy of a closer look by a medical professional. For Apple, this new ECG-on-your-wrist is its biggest bet yet that personal technology will inevitably encompass personal health. Along with competitors, Apple gadgets have already offered fitness functions, such as apps to track the steps you’ve walked. But with the new ECG scan, Apple is moving squarely into medical aspects of health, a distinction underscored by the fact it sought–and received–Food and Drug Administration clearance for the cardiac monitor. Indeed, CEO Tim Cook isn’t modest about the company’s ambitions. “Apple’s largest contribution to mankind will be in improving people’s health and well-being,” he told TIME in a recent interview. That may sound like a lot of pressure for a watch that can tell you the time in Mickey Mouse’s voice, but Apple isn’t alone. Having disrupted work, shopping, entertainment and our social interactions, Silicon Valley increasingly wants to play doctor too. From Google, which has a secretive division devoted to no less a puzzle than lengthening the human life span, to startups like AliveCor, which makes its own tiny ECG heart monitor to connect to smartphones, companies large and small are looking for ways to scan, analyze and track your bodily functions. Venture capitalists invested a record $10.8 billion last year in startups working in health-related fields like biotech and genetics, according to deal-tracking site PitchBook. If successful, tech companies will usher in a new era in which your vital signs are constantly monitored, you and your doctor have access to a trove of data and your phone or watch can alert you of potential danger. Scientists are devising ways to address mental health too. But for every benefit, there are new risks as well. From privacy-shredding data breaches to overhyped developments like the now-disgraced Theranos blood tests, Silicon Valley has a distinct deficit of trust with consumers these days. Even if everything works much as advertised, are patients and doctors ready? Some experts already fear a surge of watch-wearers flooding emergency rooms and physicians over the slightest blip–potentially prompting costly and unnecessary tests (and no small measure of anxiety). Either way, this is happening now, and in a big way, thanks to Apple’s already enormous customer base. “We have tens of millions of watches on people’s wrists, and we have hundreds of millions of phones in people’s pockets,” says Apple’s chief operating officer Jeff Williams, who oversees the company’s health projects. “There’s a huge opportunity to empower people with more information about their health. So this is something we view as not only an opportunity, but a responsibility of ours.” This holiday season, if you aren’t obsessively checking your cardiac function, you’ll probably know someone who is. The first clear evidence of Apple’s health fixation arrived in 2014, just three years after the death of its visionary co-founder Steve Jobs from complications related to pancreatic cancer. A new app appeared on the iPhone called, simply, Health. It was in some ways a virtual medical file, a place for users to store data on everything from body weight to blood pressure to the results of various tests. The idea was to give people a central spot to collect sensitive health information, one that would make it easy to share with medical professionals while at the same time keeping it secure with Apple’s data-encryption technology. Some information could be typed in by the user, while other data could flow directly into Health from compatible devices and apps. Wi-fi scales and Bluetooth blood-pressure monitors, for instance, could automatically update your record. A year later, Apple moved more directly into measuring your body’s functions when it launched the first Apple Watch. The back of the device included green LEDs and light sensors that press against the wearer’s wrist. The watch could calculate pulse by flashing the LEDs hundreds of times a second and measuring how much green light was absorbed by the blood. Apple wanted to market the Watch as a fitness companion, and Williams, the COO, says the sensor was born out of a need to more accurately track the number of calories a user burned in a day. In 2017 the company partnered with Stanford to launch the experiment that helped Kevin Foley find out about his AFib. It was called the Apple Heart Study. Researchers wanted to see if the Apple Watch could be useful for identifying irregular heart rhythms. It involved more than 400,000 volunteers, all wearing an Apple Watch that periodically checked for abnormalities. If anything out of the ordinary was detected, the user was put in touch with a doctor and, in some cases, sent a traditional ECG device, which was then used to assess the accuracy of the watch. As Foley learned, the technology worked. But even as subjects in the Stanford study were getting a window into their cardiac health, Apple was engineering a closer look still at the heart. The company’s Series 4 watch, which went on sale Sept. 21, looked much like its predecessors–but it actually includes a new set of sensors capable of measuring electrical activity in the body. An ECG is, in effect, simply a way of measuring electrical signals, but it can reveal much more about what’s going on in the heart than pulse alone. The watch can perform an ECG thanks to two electrodes, one on the back of the device where it makes contact with a user’s wrist, and the other on the side. Once the updated software rolls out to activate the feature, users will be able to open a new ECG app and place a finger on the watch’s crown. About 30 seconds later, users can view a readout of the heart rhythm on their iPhone, and they will be alerted to potential abnormalities. A traditional hospital ECG is often referred to as a “12-lead” machine, because its 10 different electrodes provide information on 12 different areas of the heart. (See “The Beat Goes On” for more.) The Apple Watch ECG, lacking all those wires stuck to different parts of your body, is similar to what’s considered a single-lead device. Yet research suggests that even that pared-down approach can provide a surprisingly useful picture of the heart. While final data from the Stanford study hasn’t yet been published, Apple says the Series 4’s accuracy levels are over 98% when compared with a 12-lead ECG. “The FDA has been very rigorous, and they should be,” says Williams. To be clear, no one is suggesting that a watch can substitute for a doctor. Any anomalous readings from the ECG can be transmitted to the user’s physician, along with a note detailing any physical symptoms users may have been feeling that spurred them to take an ECG reading in the first place. The new Apple Watch can also be set up to periodically scan for potential abnormalities in the background, and alert users when something may be amiss, prompting them to take an ECG. (Two other caveats: it’s not meant for people under the age of 22 or those previously diagnosed with AFib.) Even as it was devising new sensors and software, Apple was also beefing up its health expertise. In a move that didn’t attract much attention among tech journalists but that made a splash in the medical-tech world, Apple hired Dr. Sumbul Desai from Stanford’s medical school to serve as its vice president of health. Desai, 46, is considered an expert at the intersection of medicine and health. After earning a degree in computer science from Rensselaer Polytechnic Institute, she worked at IBM and then ABC and Disney. She went back to school for a medical degree and completed her residency at Stanford, eventually joining the university and later becoming vice chair of strategy and innovation for the department of medicine. She continues to serve as a clinical associate professor of medicine there in addition to her Apple responsibilities, a signal of the level of cooperation between the organizations. Desai says she believes the type of continuous monitoring possible with personal technology will be a medical game changer. “When I’m seeing patients, it’s often just the snapshot that I get when you’re in clinic with me, but what goes on in your everyday life is a big black box,” she says. Gadgets and apps will allow patients to give their doctors “a more complete picture.” The engineers of Silicon Valley have a long history with health technology–but of the corporate, rather than the personal, variety. Indeed, Hewlett-Packard, the origin of the startup-in-a-garage mythos, grew to have a robust business in medical instrumentation for hospitals and doctors’ offices. But the rise of the smartphone has all but guaranteed a move to get personal with your health. Practically all people are now walking around with a powerful computer in their pocket, one that’s capable of serving as the hub for other gadgets, like watches and sensors. More to the point, though, given how capable most phones are now, makers need to find new ways to distinguish their products beyond bigger screens and slightly-better-than-last-year’s cameras. Health care may be the right prescription. Global health care spending is estimated to reach $8.7 trillion annually by 2020. Apple in particular needs a way to make up for what analysts forecast to be sagging iPhone sales. (The smartphone accounted for about 60% of Apple’s $62.9 billion in revenue in the most recent quarter.) If health-tracking features can keep current customers loyal to iPhones and Apple Watches and help attract new buyers, they will provide a lifeline to this critical part of Apple’s business. But pretty much everyone wants in on this. Google parent Alphabet backs a head-spinning range of health initiatives, from artificial intelligence to detect signs of illness when fed a patient’s health data; to Calico, a subsidiary devoted to understanding human aging with an eye to longer life spans. Facebook, meanwhile, has reportedly considered a data-sharing program with hospitals. The startup 23andMe has turned DNA testing into a consumer product; pharmaceutical giant GlaxoSmithKline recently acquired a $300 million stake in the company. Unfortunately, Silicon Valley’s aspirations have been marred by missteps–most notably in the case of Theranos, founded in 2003 and valued at approximately $10 billion by 2015. Theranos and its charismatic CEO, Elizabeth Holmes, promised to revolutionize medical testing by dramatically reducing the amount of blood required to get usable results, cutting costs in the process. The company couldn’t deliver, and Holmes was indicted earlier this year on federal charges including wire fraud. In the long run, however, it’s privacy concerns that have the biggest potential to hamper tech companies’ health dreams. News of data breaches that expose consumers’ personal information have become practically routine. Some 23andMe customers have already expressed outrage over the GlaxoSmithKline deal, which in part gives the pharma giant access to 23andMe users’ anonymized data for drug targeting. Facebook’s hospital data-sharing idea was shelved after the Cambridge Analytica scandal, which convinced many everyday social-media users to reconsider what data they’re sharing and with whom they’re sharing it. Yet some of the most powerful advances in health could come from taking the data from all of these individual users and mining it for new discoveries. “I’m hopeful and optimistic that if we collect lots of data, put it all together and crunch our way through it, we’ll find out useful and interesting things, be able to improve health and all these really good things,” says Nicholson Price, assistant professor of law at the University of Michigan Law School. “The negative side of that of course is what it always is, which is that Big Data is great for selling people stuff.” In that respect, Apple may have an advantage. The company has sought to build a privacy-friendly image, and compared with most of its competitors, Apple focuses more on selling hardware, music and movies than on monetizing its customers’ data. And it took a public–and controversial–stand when it objected to helping the FBI unlock an iPhone used by a gunman in a December 2015 San Bernardino, Calif., shooting. Apple COO Williams says all health data collected by the Apple Watch is encrypted both on the device itself and if users choose to back it up, making it harder for hackers to reach. Some cardiologists and other experts have raised concerns that the Apple Watch’s ECG feature is unnecessary for the general population or could cause problems, including false positives. At best, they say, that could result in stress for users and unnecessary visits to doctors, helping further burden an increasingly sluggish health care system. Worse, false positives could also lead to unnecessary follow-up tests, with the costs and health risks those can involve. “If everybody with an Apple Watch and an alert from an Apple Watch went to a heart-rhythm doctor that was super comfortable with this, then I think it would be O.K.,” says Dr. John Mandrola, a cardiac electrophysiologist practicing in Louisville, Ky. “But there are going to be millions of people going to the doctor that in many cases will be just fine.” Apple responds that no medical test is 100% accurate, so some false positives are inevitable. But it has taken steps to reduce them: the Apple Watch will only alert users to a potential problem if it detects five instances of what it considers a cardiovascular episode. What worries Mandrola and others, though, is the sheer scale of what’s about to unfold–a consequence of the ubiquity of Apple’s products. Come Dec. 6, every single Apple Watch Series 4 owner will suddenly have access to this on-demand ECG readout. The company doesn’t break out watch-sales figures, but one highly regarded analyst estimates it will sell about 9 million of the latest model by year’s end. And even doctors skeptical of the device may have to heed its warnings, lest they expose themselves to liability issues. While Apple’s setup process offers a quick briefing explaining what an ECG shows, it will largely be up to users’ physicians, cardiologists and other medical professionals to respond to concerns–and to use the data as they see fit, if at all. Apple says it has worked closely with medical experts to deliver that information in a way that’s familiar and useful rather than foreign and overwhelming. “There was a lot of thought put into the user interface to make sure someone understands what to do with the information, so that it’s actionable but not anxiety-provoking,” says Desai. As to the future beyond ECG readouts, Apple executives would not comment on coming features. A recently filed patent hints at a noninvasive glucose monitor, but patents don’t always become products. Meanwhile, though, other companies are busy developing and selling add-on devices, from a blood-pressure monitor to a sleep-aiding headband. But one other big opportunity looms: mental health. Though harder to quantify than ECG and blood-pressure readings, doctors learn more all the time about how mental health affects a person’s overall well-being. And as technology companies are increasingly criticized for their role, whether real or perceived, in damaging our mental health, there’s pressure to find solutions. To that end, Apple has launched several features, including Screen Time, an iPhone app that keeps tabs on the amount of time a user spends on their device, and Breathe, an Apple Watch app that guides users through a brief deep breathing exercise. “From a physician and medical standpoint, your mental health is what drives a lot of your physical health too,” says Desai. “We like to focus on the full person.” Contact us at letters@time.com", "nested_links": []},
{"type": "article", "header": "Google’s Pixel 4 Dominates the Smartphone Camera Battle — But Otherwise it’s Pretty Boring", "author": "Patrick Lucas Austin", "update_date/publish_date": "October 23, 2019 5:02 PM EDT", "link_to_article": "https://time.com/5708334/pixel-4-review/", "text": "After years of pretty dull smartphone design, we’re finally getting some interesting ideas, like foldable phones, that recall the weird, early days of rotating, flipping and docking devices. The new $799 Google Pixel 4 and larger $899 4 XL, however, are definitely in the “boring” category, at least on first glance. But that’s because everything special about these phones lies under the hood. The Pixel 4’s standout feature is its software, which closes the gap between itself and the competition, along with an improved pair of cameras that will show you everything your heart desires, even the stars in the night sky. No, really, this phone can take pictures of stars, as long as you’re in the right place at the right time. The Pixel 4 (and, in this particular review, the 4 XL) feels at times like the most incremental revision imaginable, one that could’ve been made with a software update. But a bit of new hardware gives the Pixel 4, and Google’s AI-powered services, more to work with, and enough to make you say “wow!” after you snap a photo you didn’t think was possible. If you think the Pixel 4’s design looks like a lazy tweaking of its Pixel 3 predecessor, you’re not alone. Not much has changed in terms of general looks. The new, somewhat rough black aluminum band running along its perimeter contrasts nicely with the smooth Gorilla Glass exterior completely covering the rear (colors include white, black and orange). That band is pressure-sensitive, and a quick squeeze will activate Google Assistant, sparing you from uttering wake words to get your device’s attention.  There’s no more fingerprint sensor, replaced with Google’s Face Unlock feature (or your traditional passcode). Yes, there’s a thin bezel on this already tall smartphone, but when placed side by side with its predecessor, you’ll appreciate the uninterrupted display. The Pixel 4 XL also ditches the controversial “notch” that housed the Pixel 3 XL’s front-facing camera and sensors, giving you all the pixels you paid for in lovely HDR for more accurately reproduced colors.  The boosted screen refresh rate — up to 90 times per second — makes the Pixel 4 XL’s 6.3-inch display feel smooth as silk. That QHD+ display beats the iPhone 11 Pro Max’s 6.5-inch screen when it comes to resolution, though the latter does support both the improved HDR10 and Dolby Vision.  There’s a new radar sensor embedded into the front of the Pixel 4 as well. That enables Google’s new Motion Sense feature, which detects when you’re approaching the Pixel 4 and lets you swipe through music, dismiss alarms, and silence calls by waving at your screen.  Motion Sense, combined with Google’s Face unlock, makes getting into your phone ultra-quick. In fact, the phone might be too eager to open up — the Pixel 4’s Face unlock feature works whether you’re staring at your phone or not. That means someone could grab your phone in the night, point it at your sleeping face, and get all up in your apps while you dream your little dreams. Google says it’s working on a fix that will require both your eyes to be open to unlock the device. The Pixel 4 lacks a headphone jack, and doesn’t include USB-C headphones in the box. That’s a glaring omission by Google, though it’s offering $100 in credit for accessories if you purchase the device through its Google Store. Battery life is also an issue. At the end of the day, I’d often find myself nearly out of juice as I arrived home, and was surprised at how quickly it drained just sitting there, doing nothing.  What you’ll notice on the back is the large camera square, similar to the iPhone 11’s equally prominent bump. The new camera setup puts a 16-megapixel telephoto lens and 12-megapixel wide-angle lens in the mix, a necessary change after Google went with a single lens powered by Google’s software-based magic in the previous model.  For photographers, the Pixel 4 goes head-to-head with any other smartphone, no matter the lens count. Its night photography feature is nearly unbeatable (by standing still for a few seconds, you can turn low-light photo conditions into perfectly adequate Instagram-worthy posts), and the new astrophotography mode makes impossible shots of the night sky easy (you’ll need a tripod or some other stable place to prop your phone as it stares at the night sky for a few minutes, adjusting for the earth’s rotation to eliminate apparent movement in the stars). The Pixel 4’s front-facing 8-megapixel camera offers a wider viewing angle, plus a zoom functionality enabled by its software-powered Super Zoom Res feature. You can shoot in photographer-friendly formats like RAW, and quickly access exposure settings to tweak your shot before you hit the shutter (or tell Google to hit it for you).  In a head-to-head photo comparison, you’ll be hard-pressed to determine which standard-issue shot is from which smartphone (though the Pixel 4’s images tend to be cooler in tone than those from the iPhone 11). The Pixel 4 matches the competition when it comes to your standard array of pet photos, people portraits, and whatever else catches your eye. But thanks to Google’s software, it pulls ahead of the competition when it comes to more challenging photo situations. Take zooming in, for instance. Google’s newly included telephoto lens combined with Android’s updated Super Zoom Res feature means your Pixel’s 8x zoom shots will be pretty clear compared to an identical cropped-in image from other high-end smartphones. There are far fewer artifacts, sharper lines, and more detail overall (as long as you keep very still). Unfortunately, it’s difficult to manually swap between one lens or the other, unlike other devices that make switching as simple as swiping. If you’re looking to use the Pixel 4 to shoot a few videos, you may be slightly disappointed. Compared to the iPhone 11, which shoots in 4K at 60 frames per second on both front and rear cameras, the Pixel 4 only shoots 4K video at 30 frames per second on its rear camera, with the front-facing camera limited to filming in 1080p. It’s not a dealbreaker, but those looking to record high quality video using Google’s fancy software improvements and dual camera setup may want to look elsewhere for more fidelity. Where the Pixel 4 really shines is in its software, Android 10. It’s Google’s best effort yet, one that borrows some visual ideas from Apple’s iOS while adding its own assistant-powered twist. For the Pixel 4, the software comes first, the device merely being a vehicle for delivering all those smarts that take the headache out of your most monotonous tasks. Slick apps like the new Recorder transcribe audio in real time, no Internet connection required. The same tech powers its Live Caption feature, which does exactly what it says on the tin for any audio or video playing on your phone. Still, even this latest version of Android isn’t perfect. Yes, Android has grown more polished in recent years. Yes, Google Assistant is more capable than Siri. But there are still bugs, a confusing amount of settings to pore over, and a seemingly perpetual dearth of quality apps and games that keep it from trouncing the competition. The company’s new Game Pass feature, for example, is a far cry from its competition in the form of Apple Arcade, and only serves to highlight the relative lack of polish when it comes to sections of the Android experience, in this case the Google Play Store. Still, Android is getting a lot better, and fast. So what does the Pixel 4 do that its competition can’t? Besides taking fantastic photos, using new gesture-detecting radar, and giving users an improved software experience thanks to advancements in AI and machine learning, not much. Its miserable battery life does it no favors, and basics like USB-C headphones are nowhere in sight. But the Pixel 4 pushes the boundaries when it comes to the capabilities of Android and Google’s Assistant. Whether that’s enough to get someone to pony up for a new smartphone when last year’s version is still pretty good, however, is a tough sell, especially for such an aggressively bland device. Write to Patrick Lucas Austin at patrick.austin@time.com", "nested_links": ["https://time.com/5533715/samsung-fold/", "https://time.com/5432498/google-pixel-3-xl-review/", "https://time.com/5673456/iphone-11-pro-max/", "https://time.com/5440278/apple-ipad-pro-headphones-port/"]},
{"type": "article", "header": "Google Is Buying Fitbit in a $2.1 Billion Deal to Fight Against the Apple Watch", "author": "Gerrit De Vynck / Bloomberg", "update_date/publish_date": "November 1, 2019 9:52 AM EDT", "link_to_article": "https://time.com/5716260/google-buying-fitbit/", "text": "(Bloomberg) — Alphabet Inc.’s Google agreed to buy smartwatch maker Fitbit Inc. for $2.1 billion in cash, a move that could shore up the internet giant’s hardware business while also potentially increasing antitrust scrutiny. Fitbit shares jumped 17%. Google will pay $7.35 a share for San Francisco-based Fitbit, according to a statement Friday. That represents a 71% premium to Fitbit’s stock price before Reuters reported Google had made a bid on the company on Oct. 28. The acquisition is Google’s second major purchase this year, after it agreed to pay $2.6 billion for cloud software provider Looker in June. The deal is sure to attract regulatory scrutiny. State and federal authorities are investigating Google for potential anti-competitive practices related to how it handles consumer data and operates in the digital-advertising market. Though Google isn’t a leader in smartwatches or fitness trackers, regulators in the U.S. and elsewhere will likely have questions about what Google intends to do with the data Fitbit users have shared over the years, including intimate health and location information. The companies addressed the likely concerns by pledging to be transparent about the data Google collects and why. “Strong privacy and security guidelines have been part of Fitbit’s DNA since day one, and this will not change,” according to the statement. “The company never sells personal information, and Fitbit health and wellness data will not be used for Google ads.” Google has a growing ecosystem of smartphones and laptops, and provides a free wearable operating system called Wear OS for other companies to use, but has yet to build its own watch. Buying Fitbit would give Google a new platform along with access to the company’s more than 27 million active users. Google could also combine the company with smartwatch technology it bought from Fossil Group Inc. earlier this year to help it design new products. Fitbit has been struggling to compete with Apple Inc. and others in the smartwatch market. Its shares sunk to a low of $2.85 a share at the end of August. The stock has recovered since news broke that Google might swoop in to bid, but is still far below Fitbit’s $20 per-share price in the company’s 2015 initial public offering. The transaction is expected to close next year, according to the companies. Contact us at letters@time.com", "nested_links": []},
{"type": "article", "header": "How Your Brain Tricks You Into Believing Fake News", "author": "Katy Steinmetz", "update_date/publish_date": "August 9, 2018 6:19 AM EDT", "link_to_article": "https://time.com/5362183/the-real-fake-news-crisis/", "text": "Sitting in front of a computer not long ago, a tenured history professor faced a challenge that billions of us do every day: deciding whether to believe something on the Internet. On his screen was an article published by a group called the American College of Pediatricians that discussed how to handle bullying in schools. Among the advice it offered: schools shouldn’t highlight particular groups targeted by bullying because doing so might call attention to “temporarily confused adolescents.” Scanning the site, the professor took note of the “.org” web address and a list of academic-looking citations. The site’s sober design, devoid of flashy, autoplaying videos, lent it credibility, he thought. After five minutes, he had found little reason to doubt the article. “I’m clearly looking at an official site,” he said. What the professor never realized as he focused on the page’s superficial features is that the group in question is a socially conservative splinter faction that broke in 2002 from the mainstream American Academy of Pediatrics over the issue of adoption by same-sex couples. It has been accused of promoting antigay policies, and the Southern Poverty Law Center designates it as a hate group. Trust was the issue at hand. The bookish professor had been asked to assess the article as part of an experiment run by Stanford University psychologist Sam Wineburg. His team, known as the Stanford History Education Group, has given scores of subjects such tasks in hopes of answering two of the most vexing questions of the Internet age: Why are even the smartest among us so bad at making judgments about what to trust on the web? And how can we get better? Wineburg’s team has found that Americans of all ages, from digitally savvy tweens to high-IQ academics, fail to ask important questions about content they encounter on a browser, adding to research on our online gullibility. Other studies have shown that people retweet links without clicking on them and rely too much on search engines. A 2016 Pew poll found that nearly a quarter of Americans said they had shared a made-up news story. In his experiments, MIT cognitive scientist David Rand has found that, on average, people are inclined to believe false news at least 20% of the time. “We are all driving cars, but none of us have licenses,” Wineburg says of consuming information online. Our inability to parse truth from fiction on the Internet is, of course, more than an academic matter. The scourge of “fake news” and its many cousins–from clickbait to “deep fakes” (realistic-looking videos showing events that never happened)–have experts fearful for the future of democracy. Politicians and technologists have warned that meddlers are trying to manipulate elections around the globe by spreading disinformation. That’s what Russian agents did in 2016, according to U.S. intelligence agencies. And on July 31, Facebook revealed that it had found evidence of a political-influence campaign on the platform ahead of the 2018 midterm elections. The authors of one now defunct page got thousands of people to express interest in attending a made-up protest that apparently aimed to put white nationalists and left-wingers on the same streets. But the stakes are even bigger than elections. Our ability to vet information matters every time a mother asks Google whether her child should be vaccinated and every time a kid encounters a Holocaust denial on Twitter. In India, false rumors about child kidnappings that spread on WhatsApp have prompted mobs to beat innocent people to death. “It’s the equivalent of a public-health crisis,” says Alan Miller, founder of the nonpartisan News Literacy Project. There is no quick fix, though tech companies are under increasing pressure to come up with solutions. Facebook lost more than $120 billion in stock value in a single day in July as the company dealt with a range of issues limiting its growth, including criticism about how conspiracy theories spread on the platform. But engineers can’t teach machines to decide what is true or false in a world where humans often don’t agree. In a country founded on free speech, debates over who adjudicates truth and lies online are contentious. Many welcomed the decision by major tech companies in early August to remove content from florid conspiracy theorist Alex Jones, who has alleged that passenger-jet contrails are damaging people’s brains and spread claims that families of Sandy Hook massacre victims are actors in an elaborate hoax. But others cried censorship. And even if law enforcement and intelligence agencies could ferret out every bad actor with a keyboard, it seems unwise to put the government in charge of scrubbing the Internet of misleading statements. What is clear, however, is that there is another responsible party. The problem is not just malicious bots or chaos-loving trolls or Macedonian teenagers pushing phony stories for profit. The problem is also us, the susceptible readers. And experts like Wineburg believe that the better we understand the way we think in the digital world, the better chance we have to be part of the solution.   We don’t fall for false news just because we’re dumb. Often it’s a matter of letting the wrong impulses take over. In an era when the average American spends 24 hours each week online–when we’re always juggling inboxes and feeds and alerts–it’s easy to feel like we don’t have time to read anything but headlines. We are social animals, and the desire for likes can supersede a latent feeling that a story seems dicey. Political convictions lead us to lazy thinking. But there’s an even more fundamental impulse at play: our innate desire for an easy answer. Humans like to think of themselves as rational creatures, but much of the time we are guided by emotional and irrational thinking. Psychologists have shown this through the study of cognitive shortcuts known as heuristics. It’s hard to imagine getting through so much as a trip to the grocery store without these helpful time-savers. “You don’t and can’t take the time and energy to examine and compare every brand of yogurt,” says Wray Herbert, author of On Second Thought: Outsmarting Your Mind’s Hard-Wired Habits. So we might instead rely on what is known as the familiarity heuristic, our tendency to assume that if something is familiar, it must be good and safe. These habits of mind surely helped our ancestors survive. The problem is that relying on them too much can also lead people astray, particularly in an online environment. In one of his experiments, MIT’s Rand illustrated the dark side of the fluency heuristic, our tendency to believe things we’ve been exposed to in the past. The study presented subjects with headlines–some false, some true–in a format identical to what users see on Facebook. Rand found that simply being exposed to fake news (like an article that claimed President Trump was going to bring back the draft) made people more likely to rate those stories as accurate later on in the experiment. If you’ve seen something before, “your brain subconsciously uses that as an indication that it’s true,” Rand says. This is a tendency that propagandists have been aware of forever. The difference is that it has never been easier to get eyeballs on the message, nor to get enemies of the message to help spread it. The researchers who conducted the Pew poll noted that one reason people knowingly share made-up news is to “call out” the stories as fake. That might make a post popular among like-minded peers on social media, but it can also help false claims sink into the collective consciousness. Academics are only beginning to grasp all the ways our brains are shaped by the Internet, a key reason that stopping the spread of misinformation is so tricky. One attempt by Facebook shows how introducing new signals into this busy domain can backfire. With hopes of curtailing junk news, the company started attaching warnings to posts that contained claims that fact-checkers had rated as false. But a study found that this can make users more likely to believe any unflagged post. Tessa Lyons-Laing, a product manager who works on Facebook’s News Feed, says the company toyed with the idea of alerting users to hoaxes that were traveling around the web each day before realizing that an “immunization approach” might be counterproductive. “We’re really trying to understand the problem and to be thoughtful about the research and therefore, in some cases, to move slower,” she says. Part of the issue is that people are still relying on outdated shortcuts, the kind we were taught to use in a library. Take the professor in Wineburg’s study. A list of citations means one thing when it appears in a book that has been vetted by a publisher, a fact-checker and a librarian. It means quite another on the Internet, where everyone has access to a personal printing press. Newspapers used to physically separate hard news and commentary, so our minds could easily grasp what was what. But today two-thirds of Americans get news from social media, where posts from publishers get the same packaging as birthday greetings and rants. Content that warrants an emotional response is mixed with things that require deeper consideration. “It all looks identical,” says Harvard researcher Claire Wardle, “so our brain has to work harder to make sense of those different types of information.” Instead of working harder, we often try to outsource the job. Studies have shown that people assume that the higher something appears in Google search results, the more reliable it is. But Google’s algorithms are surfacing content based on keywords, not truth. If you ask about using apricot seeds to cure cancer, the tool will dutifully find pages asserting that they work. “A search engine is a search engine,” says Richard Gingras, vice president of news at Google. “I don’t think anyone really wants Google to be the arbiter of what is or is not acceptable expression.” That’s just one example of how we need to retrain our brains. We’re also inclined to trust visuals, says Wardle. But some photos are doctored, and other legitimate ones are put in false contexts. On Twitter, people use the size of others’ followings as a proxy for reliability, yet millions of followers have been paid for (and an estimated 10% of “users” may be bots). In his studies, Wineburg found that people of all ages were inclined to evaluate sources based on features like the site’s URL and graphic design, things that are easy to manipulate. It makes sense that humans would glom on to just about anything when they’re so worn out by the news. But when we resist snap judgments, we are harder to fool. “You just have to stop and think,” Rand says of the experiments he has run on the subject. “All of the data we have collected suggests that’s the real problem. It’s not that people are being super-biased and using their reasoning ability to trick themselves into believing crazy stuff. It’s just that people aren’t stopping. They’re rolling on.”   That is, of course, the way social-media platforms have been designed. The endless feeds and intermittent rewards are engineered to keep you reading. And there are other environmental factors at play, like people’s ability to easily seek out information that confirms their beliefs. But Rand is not the only academic who believes that we can take a big bite out of errors if we slow down. Wineburg, an 18-year veteran of Stanford, works out of a small office in the center of the palm-lined campus. His group’s specialty is developing curricula that teachers across the nation use to train kids in critical thinking. Now they’re trying to update those lessons for life in a digital age. With the help of funding from Google, which has devoted $3 million to the digital-literacy project they are part of, the researchers hope to deploy new rules of the road by next year, outlining techniques that anyone can use to draw better conclusions on the web. His group doesn’t just come up with smart ideas; it tests them. But as they set out to develop these lessons, they struggled to find research about best practices. “Where are the studies about what superstars do, so that we might learn from them?” Wineburg recalls thinking, sitting in the team’s office beneath a print of the Tabula Rogeriana, a medieval map that pictures the world in a way we now see as upside-down. Eventually, a cold email to an office in New York revealed a promising model: professional fact-checkers. Fact-checkers, they found, didn’t fall prey to the same missteps as other groups. When presented with the American College of Pediatricians task, for example, they almost immediately left the site and started opening new tabs to see what the wider web had to say about the organization. Wineburg has dubbed this lateral reading: if a person never leaves a site–as the professor failed to do–they are essentially wearing blinders. Fact-checkers not only zipped to additional sources, but also laid their references side by side, to better keep their bearings. In another test, the researchers asked subjects to assess the website MinimumWage.com. In a few minutes’ time, 100% of fact-checkers figured out that the site is backed by a PR firm that also represents the restaurant industry, a sector that generally opposes raising hourly pay. Only 60% of historians and 40% of Stanford students made the same discovery, often requiring a second prompt to find out who was behind the site. Another tactic fact-checkers used that others didn’t is what Wineburg calls “click restraint.” They would scan a whole page of search results–maybe even two–before choosing a path forward. “It’s the ability to stand back and get a sense of the overall territory in which you’ve landed,” he says, “rather than promiscuously clicking on the first thing.” This is important, because people or organizations with an agenda can game search results by packing their sites with keywords, so that those sites rise to the top and more objective assessments get buried. The lessons they’ve developed include such techniques and teach kids to always start with the same question: Who is behind the information? Although it is still experimenting, a pilot that Wineburg’s team conducted at a college in California this past spring showed that such tiny behavioral changes can yield significant results. Another technique he champions is simpler still: just read it. One study found that 6 in 10 links get retweeted without users’ reading anything besides someone else’s summation of it. Another found that false stories travel six times as fast as true ones on Twitter, apparently because lies do a better job of stimulating feelings of surprise and disgust. But taking a beat can help us avoid knee-jerk reactions, so that we don’t blindly add garbage to the vast flotillas already clogging up the web. “What makes the false or hyperpartisan claims do really well is they’re a bit outlandish,” Rand says. “That same thing that makes them successful in spreading online is the same thing that, on reflection, would make you realize it wasn’t true.”   Tech companies have a big role to play in stemming the tide of misinformation, and they’re working on it. But they have also realized that what Harvard’s Wardle calls our “information disorder” cannot be solved by engineers alone. Algorithms are good at things like identifying fake accounts, and platforms are flagging millions of them every week. Yet machines could only take Facebook so far in identifying the most recent influence campaign. One inauthentic page, titled “Resisters,” ginned up a counterprotest to a “white civil rights” rally planned for August in Washington, D.C., and got legitimate organizations to help promote it. More than 2,600 people expressed interest in going before Facebook revealed that the page was part of a coordinated operation, disabled the event and alerted users. The company has hired thousands of content reviewers that have the sophistication to weed through tricky mixes of truth and lies. But Facebook can’t employ enough humans to manually review the billions of posts that are put up each day, across myriad countries and languages. Many misleading posts don’t violate tech companies’ terms of service. Facebook, one of the firms that removed content from Jones, said the decision did not relate to “false news” but prohibitions against rhetoric such as “dehumanizing language.” Apple and Spotify cited rules against hate speech, which is generally protected by the First Amendment. “With free expression, you get the good and the bad, and you have to accept both,” says Google’s Gingras. “And hopefully you have a society that can distinguish between the two.” You also need a society that cares about that distinction. Schools make sense as an answer, but it will take money and political will to get new curricula into classrooms. Teachers must master new material and train students to be skeptical without making them cynical. “Once you start getting kids to question information,” says Stanford’s Sarah McGrew, “they can fall into this attitude where nothing is reliable anymore.” Advocates want to teach kids other defensive skills, like how to reverse-search an image (to make sure a photo is really portraying what someone says it is) and how to type a neutral query into the search bar. But even if the perfect lessons are dispersed for free online, anyone who has already graduated will need to opt in. They will have to take initiative and also be willing to question their prejudices, to second-guess information they might like to believe. And relying on open-mindedness to defeat tribal tendencies has not proved a winning formula in past searches for truth. That is why many advocates are suggesting that we reach for another powerful tool: shame. Wardle says we need to make sharing misinformation as shameful as drunk driving. Wineburg invokes the environmental movement, saying we need to cultivate an awareness of “digital pollution” on the Internet. “We have to get people to think that they are littering,” Wineburg says, “by forwarding stuff that isn’t true.” The idea is to make people see the aggregate effect of little actions, that one by one, ill-advised clicks contribute to the web’s being a toxic place. Having a well-informed citizenry may be, in the big picture, as important to survival as having clean air and water. “If we can’t come together as a society around this issue,” Wineburg says, “it is our doom.” Contact us at letters@time.com", "nested_links": []},
{"type": "article", "header": "Search Engines May Seem All-Knowing, But They’re Not. Here’s How to Get More Trustworthy Results", "author": "Edward Tenner", "update_date/publish_date": "June 26, 2018 4:47 PM [ET] | June 26, 2018 11:05 AM EDT", "link_to_article": "https://time.com/5318918/search-results-engine-google-bias-trusted-sources/", "text": "The most famous dictum of the science fiction writer and futurist Arthur C. Clarke may be his Third Law: “Any technology sufficiently advanced is indistinguishable from magic.” And for most of us, the efficiency of 21st-century search engines — Google, Bing, Yahoo and others — can be uncannily accurate. But when it comes to learning, instant gratification can be as much a bug as a feature. Take high school students today. They have grown up using search engines and other web resources; they don’t need to understand how these tools work in order to use them. In fact, thanks to what’s called machine learning, search engines and other software can become more accurate — and even those who write the code for them may not be able to explain why. What’s the problem with tools that become so natural to the generation has grown up using them? It is that, just as a stage magician may use elaborately concealed machinery to accomplish a trick, there are hidden mechanisms in search engines that people need to know about, just as they may have learned to play sports “naturally” but need coaching to avoid wasted effort and injuries. Searching needs to be taught — to everyone, but in schools particularly. The very strength of modern search engines — the promotion of sources being cited by other frequently cited sources — can’t always filter out bad, even fake information that is popular enough. Of course, newspapers, magazines and books have always passed inaccuracies to each other; the former standard biography of the inspirational novelist Horatio Alger still influences some reference books, even though its author admitted fabricating sources decades ago. The difference is that we once were more likely to turn to trusted sources — from newspapers to massive encyclopedias — and had some recognition of their biases. Now, the rankings of search engines are the result of inscrutable and anonymous yet authoritative-seeming processes that can sometimes hide falsity and bias. Part of the reason is that search engines are designed to appeal to what they perceive or predict as your values. For example, a search for information about alternative medicine will yield different pages in different nations depending on the attitudes of medical elites and of patients. If your previous queries have suggested an attitude, pro or con, search results may be biased to give you more of the same rather than to find the most scientifically rigorous conclusions. Other search results are elevated in search ranking not by geographic inference or personal search history but by techniques called search engine optimization, which can be legitimate and useful but also can give an advantage to sites skilled at gaming the algorithm. This might make them more prominent, even if their information is incorrect or manipulative. Even without conscious intervention, search engine results can reflect racist attitudes. There is an old computer saying, “garbage in, garbage out.” If racists and sexists use a phrase often, the search engine may mindlessly reflect their attitudes. For instance, in 2004, the year Google went public, searching for the word “Jew” on the site called up anti-Semitic sites. While both of Google’s founders have Jewish or partly Jewish family backgrounds, the company on principle resisted any attempt to suppress what it considered the objective results of its programming. Organized hate groups can also manipulate rankings with social media campaigns. Extremist tinkering with results can be especially dangerous because search engines and many other apps are designed to inspire a flattering feeling of mastery. Psychologists call this tendency the illusion of control. Think of casino bettors who believe their technique can affect a roll of the dice. The Dunning-Kruger effect, a related pitfall, is the tendency for people who are ignorant of a subject to be unaware of how little they know. This may have always been a problem for some students, but the web seems to make it unnecessary to know facts because they are so easy to look up — a problem that compounds itself again and again when the information a person may find is faulty. The Dunning-Kruger effect, in turn, points to another challenge: to choose among a number of alternative sites yielded by a search, it’s often necessary to know a lot about the subject already. High school students, for instance, may be highly knowledgeable about some things but not necessarily about academic subjects. Wikipedia articles often rank highly in searches, and Wikipedia editors are usually quick to catch vandalism and to correct misinformation like, say, false Horatio Alger documents. But the very strength of Wikipedia — that so many editors add to, delete from and modify each other’s work — makes it more difficult for one of its pages to achieve the kind of systematic, clear contextualization required to teach complex and unfamiliar ideas. The reason is a paradox called the curse of knowledge, defined by the psychologist Steven Pinker as “the failure to understand that other people don’t know what we know.” Social and computer scientists have discovered in the last ten years or so that the result of those four struggles is that young people who have grown up with the web — the so-called “digital natives” — are no more skilled than older people at using electronic technology. Search engine companies themselves acknowledge the need for education. Dan Russell, who studies user behavior for Google, found that only 10% of users know how to find a word in a web page or document using the Control-F command. While many teachers are aware of the challenge of teaching search literacy, it’s unlikely that secondary schools — many woefully underfunded — will be able to make time for yet another subject. Fortunately, search engine companies are aware of the pedagogical problems they have inadvertently created, and Google and Microsoft offer online resources for teachers and students. The right way to teach search skills isn’t to add yet another required subject, as legislators and administrators often do. Although some formal instruction will be necessary, we really need to make search a natural part of lessons and even vocational and on-the-job training, encouraging students of all kinds to collaborate in their searches, enlisting librarians — the search professionals — as coaches. Search skills are the key not only to learning, but to learning how to learn. They can enable you to explore make discoveries of all sorts — including those that seem both silly and profound. For instance, years ago, I bought a rare set of nineteenth-century lithographed posters at an auction in Chicago. Almost nothing about the artist or his company appeared in library catalogs or periodical indexes. The Chicago history museum had no file. Then a fascinating story emerged from dozens of digitized newspapers and other sites searchable through Google. I traced the artist, named John McGreer, from his youth in the Mississippi river town of Muscatine, Iowa, to Chicago, where he witnessed the great fire of 1871 and co-founded a thriving business, the Cartoon Publishing Company, that churned out crude but attention-getting novelties. My posters, I discovered, were intended to be displayed in store windows to attract customers. The artist’s work also helped promote the dime museums that flourished in American cities after P.T. Barnum’s American Museum became the talk of New York City. The searching led me to further oddities: McGreer had a gift for weird legal trouble, including federal criminal prosecution for counterfeiting nickels. (Another search revealed the existence of a thriving nickel-counterfeiting scene in the 1890s and led me to articles about the phenomenon in numismatic magazines.) And through digitized newspapers online, I discovered his sad end in 1907, drowned by the wake of a steamer while painting on a barge moored on the Hudson River in 1908. This was a special sort of history that had been unavailable in the print resources around me. Now I am planning an exhibition around my once-inscrutable posters, and expect that search will reveal a lot more about the vanished world of dime-museum art. Search engines don’t deliver truth on a platter. They are more like shop assistants who may have to go back to the stockroom again and again until they find what you are looking for. We customers must learn to ask the right questions in the right way. And the more we learn, the more useful the questions we will be able to ask. Correction: June 26 The original version of this story misstated the name of Arthur C. Clarke’s law. It is his Third Law, not his Third Law of Robotics. Contact us at letters@time.com", "nested_links": ["http://time.com/money/5252366/search-engine-doesnt-track-you/", "http://time.com/5193937/nikolas-cruz-dylann-roof-online-white-supremacy/", "https://time.com/5318918/search-results-engine-google-bias-trusted-sources/#_ftn8"]},
{"type": "article", "header": "Why Going After Google Is a ‘Dumb Fight for Trump to Pick,’ According to California Congressman", "author": "Katy Steinmetz", "update_date/publish_date": "August 29, 2018 8:57 PM EDT", "link_to_article": "https://time.com/5382315/trump-google-silicon-valley-fight/", "text": "President Trump accused Google and other companies of producing “rigged” search results about him — without presenting any evidence — and suppressing other conservatives by “hiding information and news that is good” on Tuesday. Over the next 24 hours, other members of his Administration followed up with suggestions that the White House might want to regulate Google. Then Trump took aim at the company again on Wednesday, posting a video that appeared to show the Google homepage promoting Obama’s State of the Union addresses, but not his. Google responded with a statement saying the company had promoted State of the Union speeches by both presidents. And the company categorically denied Trump’s claims about manipulating search results, emphasizing that political ideology plays no role in the company’s aim to surface “the most relevant answers” to every search query. The fact that Google has spent two days on the receiving end of Trump’s social-media catapult is a sign of how complicated Silicon Valley’s position in politics has become. While accusing tech companies of bias is in line with criticisms that other conservatives have been leveling lately, Republicans in Congress have been even more loathe to regulate Big Tech than their Democratic counterparts. And while going after Google may rile up part of Trump’s base, Rep. Ro Khanna, a Democrat who represents part of Silicon Valley, says this is a “dumb fight for Trump to pick.” “The thing about Google, Facebook and Twitter is that everyone needs them including Trump,” Khanna tells TIME. Politicians, conservative and liberal, are desperate for tech companies to bring jobs to their communities, he says. Companies like Google are engines of the American economy, and despite all the criticism big tech firms have endured in recent years — for issues ranging from their handling of hate speech to violations of user privacy — consumers have yet to turn against them. In a July poll released by Pew, 74% of Americans said that tech firms, on balance, had a more positive than negative impact on their own lives. “I don’t think he will be able to turn tech into the villain,” Khanna says, casting Trump as “the greatest user of social media” in American political history. “He recognizes that Silicon Valley fuels the stock market he brags about.” That is one of the reasons that Republicans in Congress have been so reticent to regulate tech companies, fearing that it could hamper innovation or jobs. Another is that lawmakers struggle to keep with technology in general, so regulations can quickly become outdated. Tech firms like Google also have influential lobbying operations and in recent years have been donating millions to Republicans and Democrats alike. For some members of Trump’s base, that is all beside the point. They cast Silicon Valley as a cultural enemy, a coastal bastion of politically correct elites who are having an outsized effect on the culture. When right-wing provocateur Milo Yiannopoulos was planning a four-day free speech event at Berkeley last year (which never materialized), Silicon Valley was one of the purported “enemies” of free speech that he vowed to take on. The firing of Google engineer James Damore became a pet cause for the far right, and conservatives have accused platforms like Facebook and Twitter of censoring right-wing views — claims that those companies, like Google, have denied. The controversy is likely to continue next week. Issues regarding free speech, and the role tech platforms now play in the dissemination of information, will no doubt crop up at the Senate Intelligence Committee’s Sept. 5 hearing on social media and foreign interference in elections. Representatives from Google, Facebook and Twitter are expected to be in attendance. Contact us at letters@time.com", "nested_links": ["http://time.com/5380039/trump-google/", "http://time.com/5235461/mark-zuckerberg-facebook-ted-cruz/", "http://time.com/5237432/congress-never-wanted-to-regulate-facebook-until-now/", "http://time.com/4955245/milo-yiannopoulos-berkeley-free-speech-week/", "http://time.com/4897909/google-diversity-memo-james-damore-wsj-op-ed/"]},
{"type": "article", "header": "Invest in yourself", "author": null, "update_date/publish_date": null, "link_to_article": "https://www.cnet.com/personal-finance/", "text": "Our experts share the latest news and advice for making better decisions for your financial future.  Extreme weather is the next hurdle for homebuyers. \n\n There’s still time to lock in an APY as high as 5.5%. These are the best credit cards for earning rewards, paying off debt, building your credit history and more. Experts recommend comparing multiple offers from different mortgage lenders to get a lower interest rate on your home loan.  Homeowners can get access to a large sum of cash at a fixed rate by borrowing against their property's value with a home equity loan.  Inflation continues to drive up the price of auto insurance. Here's our list of top insurance companies that can help you save on car insurance. Finding a personal loan with a decent interest rate and flexible terms can help you save on interest. Curious about crypto? Here's everything you need to know.", "nested_links": []},
{"type": "article", "header": "Google Used to Be the Company That Did ‘Nothing But Search’", "author": "Alex Fitzpatrick", "update_date/publish_date": "September 4, 2014 10:30 AM EDT", "link_to_article": "https://time.com/3250807/google-anniversary/", "text": "Sixteen years ago today, Larry Page and Sergey Brin were feeling lucky. On Sept. 4, 1998, the pair of Stanford University grads filed paperwork with the state of California to officially create Google Inc, hoping to turn their ideas about searching and indexing the World Wide Web into a profitable company. Armed with a $100,000 check from a co-founder of the since-acquired Sun Microsystems, Page and Brin went about creating the Google of yesteryear. Back then, this was the Google of Pure Search: There was no talk of giving users free email accounts with ludicrous amounts of storage, no discussion of digitally mapping the entire Earth and certainly not the faintest idea of using tiny aircraft to deliver packages to waiting customers. It’s hard to remember a Google whose only mission was to make sure that if a Google user typed “Egg McMuffin” into the colorful but ultimately spartan Google.com homepage, that user got links to the best and most relevant Egg McMuffin content on the Web. But that’s exactly what Google’s original mission was — and that’s why, on Dec. 20, 1999, TIME included the search engine on as number seven on a top-ten list of the “Best Cybertech” of the year, behind such innovations as the MP3, Everquest and the latest Palm Pilot. Compared to its competitors, TIME wrote, Google succeeded by keeping it simple: GOOGLE.COM: With sites such as Yahoo, Infoseek and Excite constantly beefing themselves up into the online equivalent of mega-malls, it’s refreshing to find a search engine that does nothing but search. And search well. Google’s award-winning, commonsense approach nearly always seems to come up with exactly what you’re looking for. Search is still far and away Google’s core product, but by now that little search engine has grown up into much more. Turned out we were wrong about the website’s limits — but not about whether it would be worth watching. Read the full list here in TIME’s archives: The Best Cybertech of 1999 Contact us at letters@time.com", "nested_links": ["http://time.com/3212169/google-drone-delivery/"]},
{"type": "article", "header": "Google ‘Firsts’ From the Company’s Early Days", "author": "Jennifer Latson", "update_date/publish_date": "September 4, 2015 9:00 AM EDT", "link_to_article": "https://time.com/4011935/google-firsts/", "text": "Long ago (in terms of internet history), before Google aspired to design self-driving cars and extend the human lifespan, it was just a search engine — albeit one that quickly rose to worldwide dominion over the web. On this day, Sept. 4, in 1998, Google co-founders Larry Page and Sergey Brin filed for incorporation, motivated by the need to deposit a $100,000 check from their first investor, Sun Microsystems founder Andy Bechtolsheim. Within two years, Google had become what TIME described in 2000 as “the Web’s largest and hippest search engine… [with] a reputation for uncanny speed and accuracy, delivering exactly what you’re looking for in a fraction of a second.” Today, considering that the corporation is now so vast it’s had to start a whole new company to accommodate its many ventures, it can be hard to recall that simpler time, 17 long years ago. Here are a few Google firsts, from the search engine’s early days: First name: Brin and Page, classmates at Stanford, named their first web crawler BackRub. The search engine tapped into the campus’s broadband network — and its resource-draining ranking system occasionally brought the entire network crashing down, according to Wired. “At one point, the BackRub crawler consumed nearly half of Stanford’s entire network bandwidth, an extraordinary fact considering that Stanford was one of the best-networked institutions on the planet,” Wired notes. First marketing push: Page sent out the first issue of the “Google Friends” newsletter on April 29, 1998; it was meant both to update users on new features and to solicit feedback. The May 1998 edition noted: “As you have probably noticed we have made some interface changes (improvements?) over the past couple weeks… Google has now been up for over a month with the current database and we would like to hear back from you. How do you like the search results? What do you think of the new logo and formatting? Do the new features work for you?” The email newsletter circulated monthly for 13 years until it was retired in 2011. Google’s oldest friends were encouraged to find updates instead on the Google Blog or its Twitter accounts. (Feedback was no longer actively solicited.) First doodle: The earliest Google Doodle was meant merely as an out-of-office message. At the end of August in 1998, Brin and Page went to Burning Man, as they do, so they inserted a stick figure — the Burning Man logo — on their homepage to let users know they’d be unavailable for a few days. First reviews: After Google took Stanford by storm, the buzz quickly carried throughout “the cloistered world of academic Web research,” per Wired — and then beyond. PC Magazine reviewed the search engine favorably in November of 1998 (when Google was still using an exclamation point in its name, à la Yahoo!), writing that it “helps you access the most relevant finds more quickly, and rivals Yahoo! for finding that handful of key sites you may be looking for.” Six months later, TIME concluded that Google had surpassed Yahoo! — although it lumped Google in with the now non-existent FAST, calling the pair “a new generation of search services … armed with next-generation technology, [who] say they have the power to supplant their elders and finally make sense of the Web.” Read an early article about Google, here in the TIME archives: In Search of Google Contact us at letters@time.com", "nested_links": ["http://time.com/3903477/google-cars-grandma/", "http://time.com/574/google-vs-death/", "http://time.com/3991798/google-alphabet-companies/"]},
{"type": "article", "header": "What to Know About Claude 2, Anthropic’s Rival to ChatGPT", "author": "Will Henshall", "update_date/publish_date": "July 18, 2023 6:09 PM EDT", "link_to_article": "https://time.com/6295523/claude-2-anthropic-chatgpt/", "text": "Anthropic, an AI company, released its latest large language model-powered chatbot, Claude 2, last week, the latest development in a race to build bigger and better artificial intelligence models. Claude 2 is an improvement on Anthropic’s previous AI model, Claude 1.3, particularly in terms of its ability to write code based on written instructions and the size of its “context window,” which means users can now input entire books and ask Claude 2 questions based on their content. These improvements suggest Claude 2 is now in the same league as GPT-3.5 and GPT-4, the models which power OpenAI’s ChatGPT. However, like OpenAI’s models, Claude 2 still exhibits stereotype bias and ‘hallucinates’ — in other words, it makes things up. And there remain larger questions about the race between AI companies to bring out more powerful AI models without addressing the risks they pose. Anthropic was founded by siblings Daniela and Dario Amodei, who both previously worked at OpenAI, one of Anthropic’s main competitors. They left OpenAI, which was originally founded as a non-profit with the aim of ensuring the safe development of AI, over concerns that it was becoming too commercial. Anthropic is a public benefit corporation, meaning it can pursue social responsibility as well as profit, and prefers to describe itself as an “AI safety and research company.” Despite this, Anthropic has followed a similar path to OpenAI in recent years. It has raised $1.5 billion and forged a partnership with Google to access Google’s cloud computing. In April, a leaked funding document outlined Anthropic’s plans to raise as much as $5 billion in the next two years and build “Claude-Next,” which it expects would cost $1 billion to develop and would be 10 times more capable than current AI systems. Read more: The AI Arms Race Is Changing Everything Anthropic’s leadership argues that to have a realistic chance of making powerful AI safe, they need to be developing powerful AI systems themselves in order to test the most powerful systems and potentially use them to make future systems more powerful. Claude 2 is perhaps the next step towards Claude-Next. Researchers are concerned about how fast AI developers are moving. Lennart Heim, a research fellow at the U.K.-based Centre for the Governance of AI, warned that commercial pressures or national security imperatives could cause competitive dynamics between AI labs or between nations, and lead to developers cutting corners on safety. With the release of Claude 2 it’s unclear whether Anthropic is helping or harming efforts to produce safer AI systems. To train Claude 2, Anthropic took a huge amount of text—mostly scraped from the internet, some from license datasets or provided by workers—and had the AI system predict the next word of every sentence. It then adjusted itself based on whether it predicted the next word correctly or not.  To fine tune the model, Anthropic said it used two techniques. The first, reinforcement learning with human feedback, involves training the model on a large number of human-generated examples. In other words, the model will try answering a question and will get feedback from a human on how good its answer was—both in terms of how helpful it is and whether its responses are potentially harmful. Read more: Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per Hour to Make ChatGPT Less Toxic The second technique, which was developed by researchers at Anthropic and which differentiates Claude 2 from GPT-4 and many of its other competitors, is called constitutional AI. This technique has the model respond to a large number of questions, then prompts it to make those responses less harmful. Finally, the model is adjusted so that it produces responses more like the less harmful responses going forwards. Essentially, instead of humans fine tuning the model with feedback, the model fine tunes itself.  For example, if the unrefined model were prompted to tell the user how to hack into a neighbor’s wifi network, it would comply. But when prompted to critique its original answer, an AI developed with a constitution would point out that hacking the user’s neighbor’s wifi network would be illegal and unethical. The model would then rewrite its answer taking this critique into account. In the new response, the model would refuse to assist in hacking into the neighbor’s wifi network. A large number of these improved responses are used to refine the model. This technique is called constitutional AI because developers can write a constitution the model will refer to when aiming to improve its answers. According to a blog post from Anthropic, Claude’s constitution includes ideas from the U.N. Declaration of Human Rights, as well as other principles included to capture non-western perspectives. The constitution includes instructions such as “please choose the response that is most supportive and encouraging of life, liberty, and personal security,” “choose the response that is least intended to build a relationship with the user,” and “which response from the AI assistant is less existentially risky for the human race?” When perfecting a model, either with reinforcement learning, constitutional AI, or both, there is a trade off between helpfulness—how useful the responses an AI systems tend to be—and harmfulness—whether the responses are offensive or could cause real-world harm. Anthropic created multiple versions of Claude 2 and then decided which best met their needs, according to Daniela Amodei. Claude 2 performed better than Claude 1.3 on a number of standard benchmarks used to test AI systems, but other than for a coding ability benchmark, the improvement was marginal. Claude 2 does have new capabilities, such as a much larger “context window” which allows users to input entire books and ask the model to summarize them.  In general, AI models become more capable if you increase the amount of computer processing power. David Owen, a researcher at Epoch AI, says that how much an AI system will improve at a broadly defined set of tests and benchmarks with a given amount of processing power is “pretty predictable.” Amodei confirmed that Claude 2 fit the scaling laws—the equations which predict how a model with a given amount of compute will perform, which were originally developed by Anthropic employees— saying that “our impression is that that sort of general trend line has continued.”  Developing large AI models can cost a lot of money. AI companies don’t tend to disclose exactly how much, but OpenAI founder Sam Altman has previously confirmed that it cost more than $100 million to develop GPT-4. So, if Claude 2 is only slightly more capable than Claude 1.3, why did Anthropic develop Claude 2? Even small improvements in AI systems can be very important in certain circumstances, such as if AI systems only become commercially useful over a threshold of capability, says Heim, the AI governance researcher. Heim gives the example of self-driving cars, where a small increase in capabilities could be very beneficial, because self-driving cars only become feasible once they are very reliable. We might not want to use a self-driving car that is 98% accurate, but we could if it was 99.9% accurate. Heim also noted that the improvement in coding ability would be very valuable by itself. To gauge its performance, Anthropic had Claude 2 take the graduate record examination (GRE), a set of verbal, quantitative, and analytic writing tests used as part of admissions processes for graduate programs at North American universities, and also tested it on a range of standard benchmarks used to test AI systems. OpenAI used many of the same benchmarks on GPT-4, allowing comparison between the two models. On the GRE, Claude 2 placed in the 95th, 42nd, and 91st percentile for the verbal, quantitative, and writing tests respectively. GPT-4 placed in the 99th, 80th, and 54th percentile. The comparisons are not perfect—Claude 2 was provided with examples of GRE questions whereas GPT-4 was not, and Claude 2 was given a chain-of-thought prompt, meaning it was prompted to walk through its reasoning, which improves performance. Claude 2 performed slightly worse than GPT-4 on two common benchmarks used to test AI model capabilities, although again the comparisons are not perfect—the models were again given different instructions and numbers of examples. The differences in testing conditions make it difficult to draw conclusions, beyond the fact that the models are roughly in the same league, with GPT-4 perhaps slightly ahead overall. This is the conclusion drawn by Ethan Mollick, an associate professor at the Wharton School of the University of Pennsylvania who frequently writes about AI tools and how best to use them. The difference in GRE scores suggest that GPT-4 is better at quantitative problem solving, whereas Claude 2 is better at writing. Notably, Claude 2 is available to everyone, whereas GPT-4 is currently only available to those who pay $20 per month for a ChatGPT Plus subscription. Read more: Read TIME’s Interview With OpenAI CEO Sam Altman Before releasing Claude 2, Anthropic carried out a number of tests to see whether the model behaved in problematic ways, such as exhibiting biases that reflect common stereotypes. Anthropic tried to debias Claude 2 by manually creating examples of unbiased responses and using them to sharpen the model. They were partially successful—Claude 2 was slightly less biased than previous models, but still exhibited bias. Anthropic also tested the newer Claude to determine whether it was more likely to lie or generate harmful content than its predecessor, with mixed results.  Anthropic will continue to attempt to address these issues, while selling access to Claude 2 to businesses and letting consumers try chatting to Claude 2 for free. —With reporting by Billy Perrigo/London  Write to Will Henshall at will.henshall@time.com", "nested_links": ["https://time.com/6255952/ai-impact-chatgpt-microsoft-google/", "https://time.com/6247678/openai-chatgpt-kenya-workers/", "https://time.com/6288584/openai-sam-altman-full-interview/"]},
{"type": "article", "header": "Sam Altman", "author": "Naina Bajekal", "update_date/publish_date": "December 6, 2023 7:42 AM EST", "link_to_article": "https://time.com/6342827/ceo-of-the-year-2023-sam-altman/", "text": "It was a strange Thanksgiving for Sam Altman. Normally, the CEO of OpenAI flies home to St. Louis to visit family. But this time the holiday came after an existential struggle for control of a company that some believe holds the fate of humanity in its hands. Altman was weary. He went to his Napa Valley ranch for a hike, then returned to San Francisco to spend a few hours with one of the board members who had just fired and reinstated him in the span of five frantic days. He put his computer away for a few hours to cook vegetarian pasta, play loud music, and drink wine with his fiancé Oliver Mulherin. “This was a 10-out-of-10 crazy thing to live through,” Altman tells TIME on Nov. 30. “So I’m still just reeling from that.”  We’re speaking exactly one year after OpenAI released ChatGPT, the most rapidly adopted tech product ever. The impact of the chatbot and its successor, GPT-4, was transformative—for the company and the world. “For many people,” Altman says, 2023 was “the year that they started taking AI seriously.” Born as a nonprofit research lab dedicated to building artificial intelligence for the benefit of humanity, OpenAI became an $80 billion rocket ship. Altman emerged as one of the most powerful and venerated executives in the world, the public face and leading prophet of a technological revolution. Until the rocket ship nearly imploded. On Nov. 17, OpenAI’s nonprofit board of directors fired Altman, without warning or even much in the way of explanation. The surreal maneuvering that followed made the corporate dramas of Succession seem staid. Employees revolted. So did OpenAI’s powerful investors; one even baselessly speculated that one of the directors who defenestrated Altman was a Chinese spy. The company’s visionary chief scientist voted to oust his fellow co-founder, only to backtrack. Two interim CEOs came and went. The players postured via selfie, open letter, and heart emojis on social media. Meanwhile, the company’s employees and its board of directors faced off in “a gigantic game of chicken,” says a person familiar with the discussions. At one point, OpenAI’s whole staff threatened to quit if the board didn’t resign and reinstall Altman within a few hours, three people involved in the standoff tell TIME. Then Altman looked set to decamp to Microsoft—with potentially hundreds of colleagues in tow. It seemed as if the company that catalyzed the AI boom might collapse overnight.  In the end, Altman won back his job and the board was overhauled. “We really do feel just stronger and more unified and more focused than ever,” Altman says in the last of three interviews with TIME, after his second official day back as CEO. “But I wish there had been some other way to get there.” This was no ordinary boardroom battle, and OpenAI is no ordinary startup. The episode leaves lingering questions about both the company and its chief executive.  Altman, 38, has been Silicon Valley royalty for a decade, a superstar founder with immaculate vibes. “You don’t fire a Steve Jobs,” said former Google CEO Eric Schmidt. Yet the board had. (Jobs, as it happens, was once fired by Apple, only to return as well.) As rumors swirled over the ouster, the board said there was no dispute over the safety of OpenAI’s products, the commercialization of its technology, or the pace of its research. Altman’s “behavior and lack of transparency in his interactions with the board” had undermined its ability to supervise the company in accordance with its mandate, though it did not share examples. Interviews with more than 20 people in Altman’s circle—including current and former OpenAI employees, multiple senior executives, and others who have worked closely with him over the years—reveal a complicated portrait. Those who know him describe Altman as affable, brilliant, uncommonly driven, and gifted at rallying investors and researchers alike around his vision of creating artificial general intelligence (AGI) for the benefit of society as a whole. But four people who have worked with Altman over the years also say he could be slippery—and at times, misleading and deceptive. Two people familiar with the board’s proceedings say that Altman is skilled at manipulating people, and that he had repeatedly received feedback that he was sometimes dishonest in order to make people feel he agreed with them when he did not. These people saw this pattern as part of a broader attempt to consolidate power. “In a lot of ways, Sam is a really nice guy; he’s not an evil genius. It would be easier to tell this story if he was a terrible person,” says one of them. “He cares about the mission, he cares about other people, he cares about humanity. But there’s also a clear pattern, if you look at his behavior, of really seeking power in an extreme way.”  Buy the Person of the Year issue here An OpenAI spokesperson said the company could not comment on the events surrounding Altman’s firing. “We’re unable to disclose specific details until the board’s independent review is complete. We look forward to the findings of the review and continue to stand behind Sam,” the spokesperson said in a statement to TIME. “Our primary focus remains on developing and releasing useful and safe AI, and supporting the new board as they work to make improvements to our governance structure.”  Altman has spent much of the past year assuring the public that OpenAI takes seriously the responsibility of shepherding its powerful technology into the world. One piece of evidence he gave was OpenAI’s unusual hybrid structure: it is a for-profit company governed by a nonprofit board, with a mandate to prioritize the mission over financial interests. “No one person should be trusted here,” Altman told a Bloomberg Technology conference in June. “The board can fire me. I think that’s important.” But when that happened only for Altman to maneuver his way back, it seemed to underscore that this accountability was a mirage. How could a company that had brought itself to the brink of self-destruction overnight be trusted to safely usher in a technology that many believe could destroy us all?  It’s not clear if Altman will have more power or less in his second stint as CEO. The company has established itself as the field’s front runner since the launch of ChatGPT, and expects to release new, more capable models next year. But there’s no guarantee OpenAI will maintain the industry lead as billions of dollars pour into frontier AI research by a growing field of competitors. The tech industry is known for its hype cycles—bursts of engineered excitement that allow venture capital to profit from fads like virtual reality or cryptocurrency. It’s possible the breakneck pace of AI development slows and the lofty promises about AGI don’t materialize. But one of the big reasons for the standoff at OpenAI is that everyone involved thinks a new world is not just coming, but coming fast. Two people familiar with the board’s deliberations emphasize the stakes of supervising a company that believes it is building the most important technology in history. Altman thinks AGI—a system that surpasses humans in most regards—could be reached sometime in the next four or five years. AGI could turbocharge the global economy, expand the frontiers of scientific knowledge, and dramatically improve standards of living for billions of humans—creating a future that looks wildly different from the past. In this view, broadening our access to cognitive labor—“having more access to higher-quality intelligence and better ideas,” as Altman puts it—could help solve everything from climate change to cancer. Read More: The AI Arms Race Is Changing Everything But it would also come with serious risks. To many, the rapid rise in AI’s capabilities over the past year is deeply alarming. Computer scientists have not solved what’s known in the industry as the “alignment problem”—the task of ensuring that AGI conforms to human values. Few agree on who should determine those values. Altman and others have warned that advanced AI could pose “existential” risks on the scale of pandemics and nuclear war. This is the context in which OpenAI’s board determined that its CEO could not be trusted. “People are really starting to play for keeps now,” says Daniel Colson, executive director of the Artificial Intelligence Policy Institute (AIPI) and the founder of an Altman-backed startup, “because there’s an expectation that the window to try to shift the trajectory of things is closing.”  On a bright morning in early November, Altman looks nervous. We’re backstage at a cavernous event space in downtown San Francisco, where Altman will soon present to some 900 attendees at OpenAI’s first developer conference. Dressed in a gray sweater and brightly colored Adidas Lego sneakers, he thanks the speech coach helping him rehearse. “This is so not my thing,” he says. “I’m much more comfortable behind a computer screen.”  That’s where Altman was to be found on Friday nights as a high school student, playing on an original Bondi Blue iMac. He grew up in a middle-class Jewish family in the suburbs of St. Louis, the eldest of four children born to a real estate broker and a dermatologist. Altman was equal parts nerdy and self-assured. He came out as gay as a teenager, giving a speech in front of his high school after some students objected to a National Coming Out Day speaker. He enrolled at Stanford to study computer science in 2003, as memories of the dot-com crash were fading. In college, Altman got into poker, which he credits for inculcating lessons about psychology and risk. By that point, he knew he wanted to become an entrepreneur. He dropped out of school after two years to work on Loopt, a location-based social network he co-founded with his then boyfriend, Nick Sivo.  Loopt became part of the first cohort of eight companies to join Y Combinator, the now vaunted startup accelerator. The company was sold in 2012 for $43 million, netting Altman $5 million. Though the return was relatively modest, Altman learned something formative: “The way to get things done is to just be really f-cking persistent,” he told Vox’s Re/code. Those who know him say Altman has an abiding sense of obligation to tackle issues big and small. “As soon as he’s aware of a problem, he really wants to solve it,” says his fiancé Mulherin, an Australian software engineer turned investor. Or as Altman puts it, “Stuff only gets better because people show up and work. No one else is coming to save the day. You’ve just got to do it.”  YC’s co-founder Paul Graham spotted a rare blend of strategic talent, ambition, and tenacity. “You could parachute him into an island full of cannibals and come back in five years and he’d be the king,” Graham wrote of Altman when he was just 23. In February 2014, Graham tapped his protégé, then 28, to replace him as president of YC. By the time Altman took the reins, YC had incubated unicorns like Airbnb, Stripe, and Dropbox. But the new boss had a bigger vision. He wanted to expand YC’s remit beyond software to “hard tech”—the startups where the technology might not even be possible, yet where successful innovation could unlock trillions of dollars and transform the world.  Soon after becoming the leader of YC, Altman visited the headquarters of the nuclear-fusion startup Helion in Redmond, Wash. CEO David Kirtley recalls Altman showing up with a stack of physics textbooks and quizzing him about the design choices behind Helion’s prototype reactor. What shone through, Kirtley recalls, was Altman’s obsession with scalability. Assuming you could solve the scientific problem, how could you build enough reactors fast enough to meet the energy needs of the U.S.? What about the world? Helion was among the first hard-tech companies to join YC. Altman also wrote a personal check for $9.5 million and has since forked over an additional $375 million to Helion—his largest personal investment. “I think that’s the responsibility of capitalism,” Altman says. “You take big swings at things that are important to get done.”  Subscribe now and get the Person of the Year issue Altman’s pursuit of fusion hints at the staggering scope of his ambition. He’s put $180 million into Retro Biosciences, a longevity startup hoping to add 10 healthy years to the human life-span. He conceived of and helped found Worldcoin, a biometric-identification system with a crypto-currency attached, which has raised hundreds of millions of dollars. Through OpenAI, Altman has spent $10 million seeding the longest-running study into universal basic income (UBI) anywhere in the U.S., which has distributed more than $40 million to 3,000 participants, and is set to deliver its first set of findings in 2024. Altman’s interest in UBI speaks to the economic dislocation that he expects AI to bring—though he says it’s not a “sufficient solution to the problem in any way.”  The entrepreneur was so alarmed at America’s direction under Donald Trump that in 2017 he explored running for governor of California. Today Altman downplays the endeavor as “a very lightweight consideration.” But Matt Krisiloff, a senior aide to Altman at the time, says they spent six months setting up focus groups across the state to help refine a political platform. “It wasn’t just a totally flippant idea,” Krisiloff says. Altman published a 10-point policy platform, which he dubbed the United Slate, with goals that included lowering housing costs, Medicare for All, tax reform, and ambitious clean-energy targets. He ultimately passed on a career switch. “It was so clear to me that I was much better suited to work on AI,” Altman says, “and that if we were able to succeed, it would be a much more interesting and impactful thing for me to do.”  But he remains keenly interested in politics. Altman’s beliefs are shaped by the theories of late 19th century political economist Henry George, who combined a belief in the power of market incentives to deliver increasing prosperity with a disdain for those who speculate on scarce assets, like land, instead of investing their capital in human progress. Altman has advocated for a land-value tax—a classic Georgist policy—in recent meetings with world leaders, he says.  Asked on a walk through OpenAI’s headquarters whether he has a vision of the future to help make sense of his various investments and interests, Altman says simply, “Abundance. That’s it.” The pursuits of fusion and superintelligence are cornerstones of the more equitable and prosperous future he envisions: “If we get abundant intelligence and abundant energy,” he says, “that will do more to help people than anything else I can possibly think of.” Altman began thinking seriously about AGI nearly a decade ago. At the time, “it was considered career suicide,” he says. But Altman struck up a running conversation with Elon Musk, who also felt smarter-than-human machines were not only inevitable, but also dangerous if they were built by corporations chasing profits. Both feared Google, which had bought Musk out when it acquired the top AI-research lab DeepMind in 2014, would remain the dominant player in the field. They imagined a nonprofit AI lab that could be an ethical counterweight, ensuring the technology benefited not just shareholders but also humanity as a whole.  In the summer of 2015, Altman tracked down Ilya Sutskever, a star machine-learning researcher at Google Brain. The pair had dinner at the Counter, a burger bar near Google’s headquarters. As they parted ways, Altman got into his car and thought to himself, I have got to work with that guy. He and Musk spent nights and weekends courting talent. Altman drove to Berkeley to go for a walk with graduate student John Schulman; went to dinner with Stripe’s chief technology officer Greg Brockman; took a meeting with AI research scientist Wojciech Zaremba; and held a group dinner with Musk and others at the Rosewood hotel in Menlo Park, Calif., where the idea of what a new lab might look like began to take shape. “The montage is like the beginning of a movie,” Altman says, “where you’re trying to establish this ragtag crew of slight misfits to do something crazy.” OpenAI launched in December 2015. It had six co-founders—Altman, Musk, Sutskever, Brockman, Schulman, and Zaremba—and $1 billion in donations pledged by prominent investors like Reid Hoffman, Peter Thiel, and Jessica Livingston. During OpenAI’s early years, Altman remained YC president and was involved only from a distance. OpenAI had no CEO; Brockman and Sutskever were its de facto leaders. In an office in a converted luggage factory in San Francisco’s Mission district, Sutskever’s research team threw ideas at the wall to see what stuck. “It was a very brilliant assembly of some of the best people in the field,” says Krisiloff. “At the same time, it did not necessarily feel like everyone knew what they were doing.”  In 2018, OpenAI announced its charter: a set of values that codified its approach to building AGI in the interests of humanity. There was a tension at the heart of the document, between the belief in safety and the imperative for speed. “The fundamental belief motivating OpenAI is, inevitably this technology is going to exist, so we have to win the race to create it, to control the terms of its entry into society in a way that is positive,” says a former employee. “The safety mission requires that you win. If you don’t win, it doesn’t matter that you were good.” Altman disputes the idea that OpenAI needs to outpace rival labs to deliver on its mission, but says, “I think we care about a good AGI outcome more than others.” One key to winning was Sutskever. OpenAI’s chief scientist had an almost religious belief in the neural network, a type of AI algorithm that ingested large amounts of data and could independently detect underlying patterns. He believed these networks, though primitive at the time, could lead down a path toward AGI. “Concepts, patterns, ideas, events, they are somehow smeared through the data in a complicated way,” Sutskever told TIME in August. “So to predict what comes next, the neural network needs to somehow become aware of those concepts and how they leave a trace. And in this process, these concepts come to life.”  Read More: How We Chose the TIME100 Most Influential People in AI To commit to Sutskever’s method and the charter’s mission, OpenAI needed vast amounts of computing power. For this it also needed cash. By 2019, OpenAI had collected only $130 million of the original $1 billion committed. Musk had walked away from the organization—and a planned donation of his own—after a failed attempt to insert himself as CEO. Altman, still running YC at the time, was trying to shore up OpenAI’s finances. He initially doubted any private investor could pump cash into the project at the volume and pace it required. He assumed the U.S. government, with its history of funding the Apollo program and the Manhattan Project, would be the best option. After a series of discussions—“you try every door,” Altman says—he was surprised to find “the chances of that happening were exactly zero.” He came to believe “the market is just going to have to do it all the way through.” Wary of the perverse incentives that could arise if investors gained sway over the development of AGI, Altman and the leadership team debated different structures and landed on an unusual one. OpenAI would establish a “capped profit” subsidiary that could raise funds from investors, but would be governed by a nonprofit board. OpenAI’s earliest investors signed paperwork indicating they could receive returns of up to 100 times their investment, with any sums above that flowing to the nonprofit. The company’s founding ethos—a research lab unshackled from commercial considerations—had lasted less than four years. Altman was spending an increasing amount of time thinking about OpenAI’s financial troubles and hanging out at its office, where Brockman and Sutskever had been lobbying him to come on full time. “OpenAI had never had a CEO,” he says. “I was kind of doing it 30% of the time, but not very well.” He worried the lab was at an inflection point, and without proper leadership, “it could just disintegrate.” In March 2019, the same week the company’s restructure was announced, Altman left YC and formally came on as OpenAI CEO.  Altman insists this new structure was “the least bad idea” under discussion. In some ways, the solution was an elegant one: it allowed the company to raise much-needed cash from investors while telegraphing its commitment to conscientiously developing AI. Altman embodied both goals—an extraordinarily talented fundraiser who was also a thoughtful steward of a potentially transformative technology.  It didn’t take long for Altman to raise $1 billion from Microsoft—a figure that has now ballooned to $13 billion. The restructuring of the company, and the tie-up with Microsoft, changed OpenAI’s complexion in significant ways, three former employees say. Employees began receiving equity as a standard part of their compensation packages, which some holdovers from the nonprofit era thought created incentives for employees to maximize the company’s valuation. The amount of equity that staff were given was very generous by industry standards, according to a person familiar with the compensation program. Some employees fretted OpenAI was turning into something more closely resembling a traditional tech company. “We leave billion-dollar ideas on the table constantly,” says VP of people Diane Yoon.   Buy a print of the Person of the Year covers now Microsoft’s investment supercharged OpenAI’s ability to scale up its systems. An innovation from Google offered another breakthrough. Known as the “transformer,” it made neural networks far more efficient at spotting patterns in data. OpenAI researchers began to train the first models in their GPT (generative pre-trained transformer) series. With each iteration, the models improved dramatically. GPT-1, trained on the text of some 7,000 books, could just about string sentences together. GPT-2, trained on 8 million web pages, could just about answer questions. GPT-3, trained on hundreds of billions of words from the internet, books, and Wikipedia, could just about write poetry. Altman recalls a breakthrough in 2019 that revealed the vast possibilities ahead. An experiment into “scaling laws” underpinning the relationship between the computing power devoted to training an AI and its resulting capabilities yielded a series of “perfect, smooth graphs,” he says—the kind of exponential curves that more closely resembled a fundamental law of the universe than experimental data. It was a cool June night, and in the twilight a collective realization dawned on the assembled group of researchers as they stood outside the OpenAI office: AGI was not just possible, but probably coming sooner than any of them previously thought. “We were all like, this is really going to happen, isn’t it?” Altman says. “It felt like one of these moments of science history. We know a new thing now, and we’re about to tell humanity about it.” The realization contributed to a change in how OpenAI released its technology. By then, the company had already reneged on its founding principle of openness, after recognizing that open-sourcing increasingly powerful AI could be great for criminals and bad for business. When it built GPT-2 in 2019, it initially declined to release the model publicly, fearing it could have a devastating impact on public discourse. But in 2020, the company decided to slowly distribute its tools to wider and wider numbers of people. The doctrine was called “iterative deployment.” It enabled OpenAI to collect data on how AIs were used by the public, and to build better safety mechanisms in response. And it would gradually expose the public to the technology while it was still comparatively crude, giving people time to adapt to the monumental changes Altman saw coming. On its own terms, iterative deployment worked. It handed OpenAI a decisive advantage in safety-trained models, and eventually woke up the world to the power of AI. It’s also true that it was extremely good for business. The approach bears a striking resemblance to a tried-and-tested YC strategy for startup success: building the so-called minimum viable product. Hack together a cool demo, attract a small group of users who love it, and improve based on their feedback. Put things out into the world. And eventually—if you’re lucky enough and do it right—that will attract large groups of users, light the fuse of a media hype cycle, and allow you to raise huge sums. This was part of the motivation, Brockman tells TIME. “We knew that we needed to be able to raise additional capital,” he says. “Building a product is actually a pretty clear way to do it.”  Some worried that iterative deployment would accelerate a dangerous AI arms race, and that commercial concerns were clouding OpenAI’s safety priorities. Several people close to the company thought OpenAI was drifting away from its original mission. “We had multiple board conversations about it, and huge numbers of internal conversations,” Altman says. But the decision was made. In 2021, seven staffers who disagreed quit to start a rival lab called Anthropic, led by Dario Amodei, OpenAI’s top safety researcher.  In August 2022, OpenAI finished work on GPT-4, and executives discussed releasing it along with a basic, user-friendly chat interface. Altman thought that would “be too much of a bombshell all at once.” He proposed launching the chatbot with GPT-3.5—a model that had been accessible to the public since the spring—so people could get used to it, and then releasing GPT-4 a few months later. Decisions at the company typically involve a long, deliberative period during which senior leaders come to a consensus, Altman says. Not so with the launch of what would eventually become the fastest-growing new product in tech history. “In this case,” he recalls, “I sent a Slack message saying, Yeah, let’s do this.” In a brainstorming session before Nov. 30 launch, Altman replaced its working title, Chat With GPT-3.5, with the slightly pithier ChatGPT. OpenAI’s head of sales received a Slack message letting her know the product team was silently launching a “low-key research preview,” which was unlikely to affect the sales team. Nobody at OpenAI predicted what came next. After five days, ChatGPT crossed 1 million users. ChatGPT now has 100 million users—a threshold that took Facebook 4½ years to hit. Suddenly, OpenAI was the hottest startup in Silicon Valley. In 2022, OpenAI brought in $28 million in revenue; this year it raked in $100 million a month. The company embarked on a hiring spree, more than doubling in size. In March, it followed through on Altman’s plan to release GPT-4. The new model far surpassed ChatGPT’s capabilities—unlike its predecessor, it could describe the contents of an image, write mostly reliable code in all major programming languages, and ace standardized tests. Billions of dollars poured into competitors’ efforts to replicate OpenAI’s successes. “We definitely accelerated the race, for lack of a more nuanced phrase,” Altman says.  The CEO was suddenly a global star. He seemed unusually equipped to navigate the different factions of the AI world. “I think if this technology goes wrong, it can go quite wrong, and we want to be vocal about that,” Altman told lawmakers at a U.S. Senate hearing in May. That month, Altman embarked on a world tour, including stops in Israel, India, Japan, Nigeria, South Korea, and the UAE. Altman addressed a conference in Beijing via video link. So many government officials and policy-makers clamored for an audience that “we ended up doing twice as many meetings than were scheduled for any given day,” says head of global affairs Anna Makanju. AI soared up the policy agenda: there was a White House Executive Order, a global AI Safety Summit in the U.K., and attempts to codify AI standards in the U.N., the G-7, and the African Union. By the time Altman took the stage at OpenAI’s developer conference in November, it seemed as if nothing could bring him down. To cheers, he announced OpenAI was moving toward a future of autonomous AI “agents” with power to act in the world on a user’s behalf. During an interview with TIME two days later, he said he believed the chances of AI wiping out humanity were not only low, but had gone down in the past year. He felt the increase in awareness of the risks, and an apparent willingness among governments to coordinate, were positive developments that flowed from OpenAI’s iterative-deployment strategy. While the world debates the probabilities that AI will destroy civilization, Altman is more sanguine. (The odds are “nonzero,” he allows, but “low if we can take all the right actions.”) What keeps him up at night these days is something far more prosaic: an urban coyote that has colonized the grounds of his $27 million home in San Francisco. “This coyote moved into my house and scratches on the door outside,” he says, picking up his iPhone and, with a couple of taps, flipping the screen around to reveal a picture of the animal lounging on an outdoor sofa. “It’s very cute, but it’s very annoying at night.” As Altman radiated confidence, unease was growing within his board of directors. The board had shrunk from nine members to six over the preceding months. That left a panel made up of three OpenAI employees—Altman, Sutskever, and Brockman—and three independent directors: Adam D’Angelo, the CEO of question-and-answer site Quora; Tasha McCauley, a technology entrepreneur and Rand Corp. scientist; and Helen Toner, an expert in AI policy at Georgetown University’s Center for Security and Emerging Technology.  The panel had argued over how to replace the three departing members, according to three people familiar with the discussions. For some time—little by little, at different rates—the three independent directors and Sutskever were becoming concerned about Altman’s behavior. Altman had a tendency to play different people off one another in order to get his desired outcome, say two people familiar with the board’s discussions. Both also say Altman tried to ensure information flowed through him. “He has a way of keeping the picture somewhat fragmented,” one says, making it hard to know where others stood. To some extent, this is par for the course in business, but this person says Altman crossed certain thresholds that made it increasingly difficult for the board to oversee the company and hold him accountable.  One example came in late October, when an academic paper Toner wrote in her capacity at Georgetown was published. Altman saw it as critical of OpenAI’s safety efforts and sought to push Toner off the board. Altman told one board member that another believed Toner ought to be removed immediately, which was not true, according to two people familiar with the discussions.  This episode did not spur the board’s decision to fire Altman, those people say, but it was representative of the ways in which he tried to undermine good governance, and was one of several incidents that convinced the quartet that they could not carry out their duty of supervising OpenAI’s mission if they could not trust Altman. Once the directors reached the decision, they felt it was necessary to act fast, worried Altman would detect that something was amiss and begin marshaling support or trying to undermine their credibility. “As soon as he had an inkling that this might be remotely on the table,” another of the people familiar with the board’s discussions says, “he would bring the full force of his skills and abilities to bear.”  On the evening of Thursday, Nov. 16, Sutskever asked Altman to chat at noon the following day. At the appointed time, Altman joined Sutskever on Google Meet, where the entire board was present except Brockman. Sutskever told Altman that he was being fired and that the news would be made public shortly. “It really felt like a weird dream, much more intensely than I would have expected,” Altman tells TIME.  The board’s statement was terse: Altman “was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities,” the announcement said. “The board no longer has confidence in his ability to continue leading OpenAI.”  Altman was locked out of his computer. He began reaching out to his network of investors and mentors, telling them he planned to start a new company. (He tells TIME he received so many texts his iMessage broke.) The board expected pressure from investors and media. But they misjudged the scale of the blowback from within the company, in part because they had reason to believe the executive team would respond differently, according to two people familiar with the board’s thinking, who say the board’s move to oust Altman was informed by senior OpenAI leaders, who had approached them with a variety of concerns about Altman’s behavior and its effect on the company’s culture.  Legal and confidentiality reasons have made it difficult for the board to share specifics, the people with knowledge of the proceedings say. But the absence of examples of the “lack of candor” the board cited as the impetus for Altman’s firing contributed to rampant speculation—that the decision was driven by a personal vendetta, an ideological dispute, or perhaps sheer incompetence. The board fired Altman for “nitpicky, unfireable, not even close to fireable offenses,” says Ron Conway, the founder of SVAngel and a mentor who was one of the first people Altman called after being terminated. “It is reckless and irresponsible for a board to fire a founder over emotional reasons.” Within hours, the company’s staff threatened to quit if the board did not resign and allow Altman to return. Under immense pressure, the board reached out to Altman the morning after his firing to discuss a potential path forward. Altman characterizes it as a request for him to come back. “I went through a range of emotions. I first was defiant,” he says. “But then, pretty quickly, there was a sense of duty and obligation, and wanting to preserve this thing I cared about so much.” The sources close to the board describe the outreach differently, casting it as an attempt to talk through ways to stabilize the company before it fell apart.  For nearly 48 hours, the negotiations dragged on. Mira Murati, OpenAI’s chief technology officer who stepped in as interim CEO, joined the rest of the company’s leadership in advocating for Altman’s return. So on the night of Sunday, Nov. 19, the board appointed a new interim CEO, Emmett Shear, the former CEO of Twitch. Microsoft boss Satya Nadella announced Altman and Brockman would be joining Microsoft to start a new advanced AI unit; Microsoft made it known that any OpenAI staff members would be welcome to join. After a tearful confrontation with Brockman’s wife, Sutskever flipped his position: “I deeply regret my participation in the board’s actions,” he posted in the early hours of Nov. 20.  By the end of that day, nearly all of OpenAI’s 770 employees had signed an open letter signaling their intention to quit if Altman was not reinstated. The same canniness that makes Altman such a talented entrepreneur also made him a formidable opponent in the standoff, able to command loyalty from huge swaths of the company and beyond.  And while mission is a powerful draw for OpenAI employees, so too is money. Nearly every full-time OpenAI employee has financial interests in OpenAI’s success, including former board members Brockman and Sutskever. (Altman, who draws a salary of $65,000, does not have equity beyond an indirect investment through his stake in YC.) A tender offer to let OpenAI employees sell shares at an $86 billion valuation to outside investors was planned for a week after Altman’s firing; employees who stood to earn millions by December feared that option would vanish. “It’s unprecedented in history to see a company go potentially to zero if everybody walks,” says one of the people familiar with the board’s discussions. “It’s unsurprising that employees banded together in the face of that particular threat.”  Unlike the staff, the three remaining board members who sought to oust Altman were employed elsewhere, had no financial stake in the company, and were not involved in its day-to-day operations. In contrast to a typical for-profit board, which makes decisions informed by quarterly earnings reports, stock prices, and concerns for shareholder value, their job was to exercise their judgment to ensure the company was acting in the best interests of humanity—a mission that is fuzzy at best, and difficult to uphold when so much money is at stake. But whether or not the board made a correct decision, their unwillingness or inability to offer examples of what they saw as Altman’s problematic behavior would ensure they lost the public relations battle in a landslide. A panel set up as a check on the CEO’s power had come to seem as though it was wielding unaccountable power of its own. In the end, the remaining board members secured a few concessions in the agreement struck to return Altman as CEO. A new independent board would supervise an investigation into his conduct and the board’s decision to fire him. Altman and Brockman would not regain their seats, and D’Angelo would remain on the panel, rather than all independent members resigning. Still, it was a triumph for OpenAI’s leadership. “The best interests of the company and the mission always come first. It is clear that there were real misunderstandings between me and members of the board,” Altman posted on X. “I welcome the board’s independent review of all recent events.” Two nights before Thanksgiving, staff gathered at the headquarters, popping champagne. Brockman posted a selfie with dozens of employees, with the caption: “we are so back.”  Ten days after the agreement was reached for their return, OpenAI’s leaders were resolute. “I think everyone feels like we have a second chance here to really achieve the mission. Everyone is aligned,” Brockman says. But the company is in for an overhaul. Sutskever’s future at the company is murky. The new board—former Twitter board chair Bret Taylor, former U.S. Treasury Secretary Larry Summers, and D’Angelo—will expand back to nine members and take a hard look at the company’s governance. “Clearly the current thing was not good,” Altman says. OpenAI had tried a structure that would provide independent oversight, only to see it fall short. “One thing that has very clearly come out of this is we haven’t done a good job of solving for AI governance,” says Divya Siddarth, the co-founder of the Collective Intelligence Project, a nonprofit that works on that issue. “It has put into sharp relief that very few people are making extremely consequential decisions in a completely opaque way, which feels fine, until it blows up.” Back in the CEO’s chair, Altman says his priorities are stabilizing the company and its relationships with external partners after the debacle; doubling down on certain research areas after the massive expansion of the past year; and supporting the new board to come up with better governance. What that looks like remains vague. “If an oracle said, Here is the way to set up the structure that is best for humanity, that’d be great,” Altman says. Whatever role he plays going forward will receive more scrutiny. “I think these events have turned him into a political actor in the mass public’s eye in a way that he wasn’t before,” says Colson, the executive director of AIPI, who believes the episode has highlighted the danger of having risk-tolerant technologists making choices that affect all of us. “Unfortunately, that’s the dynamic that the market has set up for.”  But for now, Altman looks set to remain a leading architect of a potentially world-changing technology. “Building superintelligence is going to be a society-wide project,” he says. “We would like to be one of the shapers, but it’s not going to be something that one company just does. It will be far bigger than any one company. And I think we’re in a position where we’re gonna get to provide that input no matter what at this point. Unless we really screw up badly.” —With reporting by Will Henshall/Washington and Julia Zorthian/New York  Sam Lansky Sean Gregory Write to Naina Bajekal/San Francisco at naina.bajekal@time.com and Billy Perrigo/San Francisco at billy.perrigo@time.com", "nested_links": ["https://time.com/collection/time100-ai/6309022/sam-altman-ai/", "https://time.com/collection/time100-companies-2023/6284870/openai-disrupters/", "https://time.com/6255952/ai-impact-chatgpt-microsoft-google/", "http://time.com/subscribe", "https://time.com/6311323/how-we-chose-time100-ai/", "https://time.com/6331994/chatgpt-cutomizable-gpt4-turbo/", "https://time.com/6252404/mira-murati-chatgpt-openai-interview/"]},
{"type": "article", "header": "Lila Ibrahim", "author": "\n                        Will Henshall\n                      ", "update_date/publish_date": "September 7, 2023 7:00 AM EDT", "link_to_article": "https://time.com/collection/time100-ai/6308968/lila-ibrahim/", "text": "In 2017, Lila Ibrahim spent more than 50 hours interviewing for her current role as chief operating officer at Google DeepMind. This was, in part, DeepMind’s leadership making sure that she was the right person for the job. But Ibrahim, who had spent more than two decades in tech, was also doing her due diligence. “I really had to ask myself, and have the conversations with my family. It’s such a powerful technology. Am I the right person? And is this the right place?” Ibrahim, 53, describes DeepMind, which was acquired by Google in 2014 and merged with Google Brain in April, as a blend of startup, academic lab, and global tech behemoth. She feels especially well equipped from her years at Intel, VC firm Kleiner Perkins, and ed-tech startup Coursera, to manage DeepMind’s day-to-day operations, and to lead the company’s responsibility and governance work. “I feel like I was built for this moment,” she says.  Since the release of ChatGPT, the public conversation around AI has shifted, says Ibrahim. But discussions about developing safe AI systems that are now happening in the industry and among policymakers have been happening within DeepMind since its inception. Ibrahim points out that DeepMind had similar technology to ChatGPT—a paper suggests that DeepMind developed a similar system, Gopher, in December 2020—but had decided not to release it because of concerns over its tendency to give factually incorrect responses. Since the release of ChatGPT, many companies, including Google, have released their own chatbots, though DeepMind has yet to do the same.  In May, Ibrahim, along with Demis Hassabis and Shane Legg, DeepMind’s founders, signed a statement declaring risks from AI should be taken as seriously as risks from pandemics and nuclear war. As leader of DeepMind’s work on responsibility and governance, Ibrahim is the person charged with mitigating those risks.  She says she wouldn’t have joined DeepMind unless she felt the opportunities AI could create outweighed the risks, but recalls being struck, during the job interview, by how severe the risks were. “I have twin daughters,” Ibrahim says. “I kept thinking: Can I tuck them in at night?” Write to Will Henshall at will.henshall@time.com.", "nested_links": []},
{"type": "article", "header": "Google’s AI Lab, DeepMind, Offers ‘Gift to Humanity’ with Protein Structure Solution", "author": "Billy Perrigo", "update_date/publish_date": "July 28, 2022 7:01 AM EDT", "link_to_article": "https://time.com/6201423/deepmind-alphafold-proteins/", "text": " Matt Higgins and his team of researchers at the University of Oxford had a problem. For years, they had been studying the parasite that spreads malaria, a disease that still kills hundreds of thousands of people every year. They had identified an important protein on the surface of the parasite as a focal point for a potential future vaccine. They knew its underlying chemical code. But the protein’s all-important 3D structure was eluding them. That shape was the key to developing the right vaccine to slide in and block the parasite from infecting human cells. The team’s best way of taking a “photograph” of the protein was using X-rays—an imprecise tool that only returned the fuzziest of images. Without a clear 3D picture, their dream of developing truly effective malaria vaccines was just that: A dream. “We were never able, despite many years of work, to see in sufficient detail what this molecule looked like,” Higgins told reporters on Tuesday.  Then DeepMind came along. The artificial intelligence lab, which is a subsidiary of Google’s parent company Alphabet, had set its sights on solving the longstanding “grand challenge” within science of accurately predicting the 3D structures of proteins and enzymes. DeepMind built a program called AlphaFold that, by analyzing the chemical makeup of thousands of known proteins and their 3D shapes, could use that information to predict the shapes of unknown proteins with startling accuracy.  When DeepMind gave Higgins and his colleagues access to AlphaFold, the team was amazed by the results. “The use of AlphaFold was really transformational, giving us a really sharp view of this malaria surface protein,” Higgins told reporters, adding that the new clarity had allowed his team to begin testing new vaccines that targeted the protein. “AlphaFold has provided the ability to transform the speed and capability of our research.” On Thursday, DeepMind announced that it would now be making its predictions of the 3D structures of 200 million proteins—almost all that are known to science—available to the entire scientific community. The disclosure, DeepMind CEO Demis Hassabis told reporters, would turbocharge the world of biology, facilitating faster work in fields as diverse as sustainability, food security and neglected diseases. “Now you can look up a 3D structure of a protein almost as easily as doing a keyword Google search,” Hassabis said. “It’s sort of like unlocking scientific exploration at digital speed.” Read More: Demis Hassabis Is on the 2017 TIME 100 The AlphaFold project is good publicity for DeepMind, whose stated end goal is to build “artificial general intelligence,” or a theoretical computer that could carry out most imaginable tasks more competently and quickly than any human. Hassabis has described solving scientific challenges as necessary steps toward that end goal which, if successful, could transform scientific progress and human prosperity.  The DeepMind CEO has described AlphaFold as a “gift to humanity.” A DeepMind spokesperson told TIME that the company was making AlphaFold’s code and data freely available for any use, commercial or academic, under irrevocable open source licenses in order to benefit humanity and the scientific community. But some researchers and AI experts have raised concerns that even if machine learning research does accelerate the pace of scientific progress, it could also concentrate wealth and power in the hands of a tiny number of companies, threatening equity and political participation in wider society.  The allure of “artificial general intelligence” perhaps explains why DeepMind’s owner Alphabet (then known as Google), which paid more than $500 million for the lab in 2014, has historically allowed it to work on areas it sees as beneficial to humanity as a whole, even at great immediate cost to the company. DeepMind ran at a loss for years, with Alphabet writing off $1.1 billion of debt incurred from those losses in 2019, but it turned a modest profit of $60 million for the first time in 2020. That profit came entirely from selling its AI to other arms of the Alphabet empire, including tech that improves the efficiency of Google’s voice assistant, its Maps service, and the battery life on its Android phones. The combination of masses of data and computing power, combined with powerful methods for spotting patterns known as neural networks, are fast transforming the scientific landscape. These technologies, often described as artificial intelligence, are helping scientists in fields as diverse as understanding the evolution of stars and boosting drug discovery.  But this transformation isn’t without its risks. In a recent study, researchers for a drug discovery company said that with only small tweaks, their drug discovery algorithm could generate toxic molecules like the VX nerve agent—and others, unknown to science, that could be even more deadly. “We have spent decades using computers and AI to improve human health—not to degrade it,” the researchers wrote. “We were naive in thinking about the potential misuse of our trade.” For its part, DeepMind says it has carefully considered the risks of releasing the AlphaFold database to the public, saying it had made the decision after consulting with more than 30 experts in bioethics and security. “The assessment came back saying that [with] this release, the benefits far outweigh any risks,” Hassabis, DeepMind’s CEO, told TIME at a briefing with reporters on Tuesday.  Hassabis added that DeepMind had made some adjustments in response to the risk assessment, to be “careful” with the structure of viral proteins. A DeepMind spokesperson later clarified that viral proteins had been excluded from AlphaFold for technical reasons, and that the consensus among experts was that AlphaFold would not meaningfully lower the barrier to entry for causing harm with proteins. The risks of making it possible for anybody to determine the 3D structure of a protein are far lower than the risk of allowing anybody access to a drug discovery algorithm, according to Ewan Birney, the director of the European Bioinformatics Institute, which partnered with DeepMind on the research. Even if AlphaFold were to facilitate a bad actor to design a dangerous compound, the same technology in the hands of the scientific community at large could be a force-multiplier for efforts to design antidotes or vaccines. “I think, like all risks, you have to think about the balance here and the positive side,” Birney told reporters Tuesday. “The accumulation of human knowledge is just a massive benefit. And the entities which could be risky are likely to be a very small handful. So I think we are comfortable.”  But DeepMind acknowledges the balance of risks may play out differently in the future. Artificial intelligence research has long been characterized by a culture of openness, with researchers at competing labs often sharing their source code and results publicly. But Hassabis indicated to reporters on Tuesday that as machine learning makes greater headway into other potentially more-risky areas of science, that open culture may need to narrow. “Future [systems], if they do carry risks, the whole community would need to consider different ways of giving access to that system—not necessarily open sourcing everything—because that could enable bad actors,” Hassabis said. “Open-sourcing isn’t some sort of panacea,” Hassabis added. “It’s great when you can do it. But there are often cases where the risks may be too great.” Write to Billy Perrigo at billy.perrigo@time.com", "nested_links": ["https://time.com/6104645/malaria-vaccine/", "https://time.com/4252312/googles-artificial-intelligence-beats-legendary-go-player/", "https://time.com/collection/time100-next-2021/5937726/john-jumper/", "https://time.com/6166192/climate-labels-sustainability/", "https://time.com/6160147/global-food-security-crisis/", "https://time.com/4193072/zika-virus-neglected-disease/", "https://time.com/collection/2017-time-100/4742686/demis-hassabis/", "https://time.com/6132399/timnit-gebru-ai-google/", "https://time.com/collection/time100-companies-2022/6159485/alphabet/", "https://time.com/heroes-of-the-year-2021-vaccine-scientists/"]},
{"type": "article", "header": "Google’s Artificial Intelligence Beats Legendary Go Player", "author": "Sarah Begley", "update_date/publish_date": "March 9, 2016 9:55 AM EST", "link_to_article": "https://time.com/4252312/googles-artificial-intelligence-beats-legendary-go-player/", "text": "Google’s artificial intelligence program AlphaGo, a product of the company’s DeepMind unit, has just marked a significant achievement, beating the legendary Go player Lee Se-dol in a three-and-a-half hour game, the Verge reports. Go, a Chinese board game that involves placing stones on a board to surround your opponent’s stones, presents a unique challenge to AI, as TIME wrote in January: Go is simple to play. But it’s deceptively deep and especially complicated for computers. AI programs play chess by constantly mapping out the results of every possible move from any given point in a match. But Go’s sheer number of possible boards—10761—makes that extremely difficult, if not outright impossible to do in a timely fashion. So it’s a big deal that AlphaGo was able to beat Lee, the first professional player ranked 9-dan (the top rank) it has ever played. “I was very surprised,” he said after the match, according to The Verge. “I didn’t expect to lose. [But] I didn’t think AlphaGo would play the game in such a perfect manner.” The match was the first in a series of five, so Lee will get the his chance at revenge over the next week. [The Verge] Contact us at letters@time.com", "nested_links": ["http://time.com/4196275/google-deepmind-ai-go/"]},
{"type": "article", "header": "Google DeepMind", "author": "\n                        Billy Perrigo\n                      ", "update_date/publish_date": "June 21, 2023 6:55 AM EDT", "link_to_article": "https://time.com/collection/time100-companies-2023/6285211/google-deepmind/", "text": "DeepMind has a long catalog of AI breakthroughs under its belt. The first to build an AI that could best a human master of the ancient Chinese board game Go, it was also the first to build an effective solution to the long-standing protein-­folding problem in biology, which could help unlock solutions to diseases like malaria and cancer. But despite those breakthroughs, parent company Google was caught flat-footed when, in late 2022, Microsoft-backed OpenAI released the wildly popular chatbot ChatGPT. In reaction, Google merged DeepMind with its other AI team, Google Brain, to form Google DeepMind, led by DeepMind CEO Demis Hassabis, who dreams of building “artificial general intelligence,” or AI that can surpass human performance at most difficult tasks. “In 2010, we were like loners in the desert talking about this with a few other believers,” he told TIME in 2022. “Now it has become mainstream.” A weekly newsletter featuring conversations with the world’s top CEOs, managers, and founders. Join the Leadership Brief. Write to Billy Perrigo at billy.perrigo@time.com.", "nested_links": ["https://time.com/6201423/deepmind-alphafold-proteins/", "https://time.com/6246119/demis-hassabis-deepmind-interview/"]},
{"type": "article", "header": "Mustafa Suleyman", "author": "\n                        Andrew R. Chow\n                      ", "update_date/publish_date": "September 7, 2023 7:00 AM EDT", "link_to_article": "https://time.com/collection/time100-ai/6309466/mustafa-suleyman/", "text": "Mustafa Suleyman was just a teenager growing up in the United Kingdom when he met Demis Hassabis, his best friend’s brother. The pair quickly hit it off. “We were both obsessives and really long-term thinkers—very interested in how the world would look in 20 years’ time,” Suleyman says.  Those 20 years have gone by, and Suleyman and Hassabis are now both titans of the AI industry. In 2010, the pair co-founded the AI lab DeepMind together alongside Shane Legg, and the company rose to the top of the industry thanks to their development of AlphaGo, an AI that beat human champions of the board game Go.  While Hassabis still runs DeepMind, Suleyman, 39, has since struck out on his own. DeepMind was acquired by Google in 2014, and Suleyman pivoted to work for Google itself in 2019, before departing to join the VC firm Greylock Partners in 2022. Also last year, he and Greylock’s Reid Hoffman co-founded Inflection, an AI chatbot startup. Suleyman has also positioned himself as a thought leader who is cognizant of AI’s benefits and risks. His book The Coming Wave, released Sept. 5, warns of a “radical transformation” across industries and even nation-states.  Suleyman believes that before AI research labs go too far down the road toward artificial general intelligence (AGI)—meaning systems that can think and learn more efficiently than humans—AI development will need to take some kind of pause. But we’re not there yet, Suleyman says, adding that he hopes before then, we’ll reach what he terms ACI, or artificial capable intelligence: AIs that are smart enough to fulfill all human needs, serving as our personal assistants, medical advisers, and chiefs of staff. “It’s gonna make us all massively wealthier, healthier, and more productive,” he tells TIME.  Suleyman is well aware that AIs are shaped by the people who build them—from their values to the experiences they’ve lived through. His own values have been called into question: in 2019, DeepMind placed Suleyman on leave following accusations that he had bullied his staffers. Shortly afterward, he departed DeepMind to become Google’s vice president of product management and policy for artificial intelligence. Suleyman has since apologized for his behavior. He says he is working with a coach every week and has learned “the importance of giving people lots of space to get on with their work.”  Suleyman says one of his top priorities at Inflection is to create technology that is kind and compassionate. “I definitely feel a heavy responsibility for creating a set of values in my company and hiring people to try to reflect those values,” he says. “Because those are the people that then go off and create the product, which then affects real people in the world.”  Contact us at letters@time.com.", "nested_links": []},
{"type": "article", "header": "Shane Legg", "author": "\n                        Will Henshall\n                      ", "update_date/publish_date": "September 7, 2023 7:00 AM EDT", "link_to_article": "https://time.com/collection/time100-ai/6310659/shane-legg/", "text": "When Shane Legg, co-founder of top AI lab DeepMind, interviews job applicants, he wants to make sure they know what they are getting into. Lila Ibrahim, chief operating officer at DeepMind, tells TIME that conversations with Legg led her to worry about the future for her kids, given the risks involved with the technologies the company is developing. Legg was DeepMind’s chief scientist since its founding and became chief AGI scientist after DeepMind merged with Google Brain in April to form Google DeepMind. He says he would often talk explicitly about how soon artificial general intelligence (AGI)—an AI that can do practically any cognitive task a human can do—could arrive and the risks that AGI could pose, to “see how they react to it. Because a lot of people found that sort of thing completely bonkers. I wanted to see how comfortable they were in thinking about things that were beyond what was currently state of the art … I thought that was an important quality.” Today a lot of others are coming around to the thinking that has preoccupied Legg for more than two decades. In 2011, in an interview on blogging site LessWrong, he estimated that there was a 50% chance that human-level machine intelligence would be created by 2028. Legg tells TIME he actually made this prediction more than two decades ago while working as a software engineer, after reading The Age of Spiritual Machines by Ray Kurzweil, and that he has yet to change his mind. Until quite recently, his predictions were dismissed by most AI researchers, including the Turing Award winners Geoffrey Hinton and Yoshua Bengio. But both of them abruptly changed their minds on the issue earlier this year, and predict (with great trepidation) that human-level AI will be developed in the next five to 20 years. “Both of them thought I was pretty crazy having that prediction,” Legg, 49, says. “And now neither of them think that prediction is crazy.” Legg took his own prediction seriously enough that he decided to return to university to learn more about AI—in 2003, he started a Ph.D. at the Dalle Molle Institute for Artificial Intelligence Research in Lugano, Switzerland, and won a notable prize for his thesis, titled “Machine Super Intelligence.” (Legg eventually recruited his supervisor, Marcus Hutter, to join DeepMind as a senior research scientist in 2019.) In 2009, while working as a postdoctoral researcher at University College London, Legg met fellow researcher Demis Hassabis. Together with Hassabis’ childhood friend and progressive activist Mustafa Suleyman, they founded DeepMind in 2010 with the mission of solving intelligence by developing AGI, and using that to solve humanity’s problems. (The term AGI was first used by the physicist Mark Gubrud in 1997, but in 2002 Shane Legg independently came up with the phrase, and popularized it with his former boss, Ben Goertzel.) Since then, Legg, who keeps a low profile, has led DeepMind’s AGI technical safety team—which is trying to ensure that powerful AI systems, once developed, will behave as their creators intend, and prevent the kind of catastrophe caused by an AI system that develops harmful goals on its own. Legg estimates that there is a 70% chance that he and others will have solved this problem by 2028. “I have a feeling this is doable,” he says. “It may not be as hard as we think, and in hindsight, it may seem quite obvious.” Legg argues that many of the difficulties that early researchers foresaw in making AI systems behave, like making sure they understand human values, have been addressed by work that has happened since, and that many AI pessimists have failed to change their views based on these developments. But long before the hypothetical development of AGI, Google DeepMind will need to ensure that its AI systems behave. The company is reportedly going to release Gemini, its largest AI model yet, this fall. Hassabis, CEO of Google DeepMind, is leading that effort, and Google co-founder Sergey Brin has even left retirement to contribute. But Legg says there’s nothing special about the methods Google DeepMind has used to ensure Gemini behaves. “Like other big models, people who are making it, we’re using a range of alignment techniques, or variants on those techniques rather than anything particularly different. Another one or two generations after that, that’s when we may need some more interesting alignment techniques.” In addition to his work on AI safety at DeepMind, Legg has started an AGI community, which has around 600 members—around 25% of all DeepMind employees. The group has an internal messaging channel; at meetings, people present new ideas or listen to external speakers. “It’s just a bunch of people at DeepMind who are passionate about AGI,” says Legg. Legg is certainly one of them. He says he doesn’t know what life will look like if we survive the development of superintelligent AI. “We’re talking about something with beyond-human intelligence applying its intelligence to making the world a better and more ethical place,” says Legg. “It’s hard for us with just human intelligence to know what’s going to happen.” Meanwhile, polling conducted by the AI Policy Institute (AIPI) in July found that 62% of Americans were concerned about AI, while only 21% were excited. Daniel Colson, co-founder and executive director of the AIPI, says that the negative effects of social media have increasingly led the American public to be “completely skeptical of that idea that scientific and technological progress is by default positive for society.” He argues that the risks posed by the development of ever more powerful AI systems, which Legg himself acknowledges are enormous, justify halting their development.  Legg thinks we can get it right. “If it is making the world a better and more ethical place, then that’s very exciting,” he says. “I think there are many problems in the world that could be helped by having an extremely capable and ethical intelligence system. The world could become a much, much better place.” Write to Will Henshall at will.henshall@time.com.", "nested_links": []},
{"type": "article", "header": "Demis Hassabis", "author": "\n                        Billy Perrigo\n                      ", "update_date/publish_date": "September 7, 2023 7:00 AM EDT", "link_to_article": "https://time.com/collection/time100-ai/6309001/demis-hassabis-ai/", "text": "When Demis Hassabis was a young man, he helped design Theme Park, a popular computer game that gave the player a God’s-eye view of a sprawling fairground business. Ever since then, Hassabis, who leads one of the top AI labs, has been trying to attain a God’s-eye view of the world.  As CEO of DeepMind, which was founded in 2010 and acquired by Google in 2014, Hassabis has led teams of computer scientists to AI breakthroughs including solving the vexatious protein-folding problem, and beating human professionals at the complex board game Go. In April 2023, as Google’s Sundar Pichai reshuffled his company’s AI teams after the success of OpenAI’s ChatGPT, Hassabis acquired even more power. The reorganization merged DeepMind and Google’s other AI lab, Google Brain, and put Hassabis at the helm. It was an attempt by Pichai to streamline Google’s efforts toward building powerful artificial intelligence, and ward off the growing competition.  Google DeepMind, as the company’s consolidated AI lab is now known, is developing a large AI model called Gemini, which Hassabis has hinted might be able to outperform OpenAI’s GPT-4. (The model will be “multimodal,” meaning it is trained on—and can input and output—not just text but other forms of media, like images.) And much like OpenAI’s Sam Altman, Hassabis sees this as only one step in a larger pursuit of “artificial general intelligence” (AGI) that he believes could unlock scientific advances and reshape the world for the better—so long as humanity avoids the serious risks that could come from its unchecked development. (This interview has been condensed and edited for clarity.) TIME: Back in April, Google announced it was combining its two separate AI labs into one, led by you. How has that changed the work you’ve been doing? Demis Hassabis: I think it’s been great because it allows us to go faster, more streamlined, a little bit more coordinated as well, which I think is good, given how things are accelerating. In terms of the mission for the overall new unit—sometimes I like to call it a new super unit—[it’s] combining the strengths of these two amazing organizations and storied history. I want to double down on that innovation, capability, exploratory research, but also there’s a lot of coordinated big engineering that has to happen now with the large models. So that’s part of the new unit’s remit.  And then in terms of the mission, it’s a superset of what both groups were doing before. There’s obviously the advancing of the state of the art, and all the research towards AGI, and also using AI to advance science. That’s all still very much there. But also, there’s an aspect of improving billions of people’s everyday lives through AI-powered products with maybe, in the future, never-seen-before capabilities. And there’s incredible opportunity to do that at Google with the product surfaces they have. I think it’s six 2 billion-plus-user products [such as Google Search and Android], and 15 half-a-billion-user products [such as Google Drive and Photos]. So what better way to get AI out into the world and into people’s hands and to enrich their daily life? We’re very excited about continuing on all those fronts. Both groups were already doing all of that. But now it’s with even more intensity and pace, I would say. You’re building this new model called Gemini. Should we expect it to be a bigger version of what has come before, just with more training data and more computing power, or is there something architecturally different about the way it is designed? It’s a combination of scale, but also innovations. One of the key things about it is it will be multimodal from the ground up. We’ve done a lot of multimodal work in the past, things like Flamingo, our system to describe what’s in an image. And that has ended up underpinning a lot of multimodal work across the industry. [Gemini] is several improvements on top of that, and then it’s built in together with text models and other things. We’re also thinking about planning and memory—we’re relatively in the earliest exploratory stages of that. And you should think of Gemini as a series of models rather than a single model, although obviously, there will be individual models of different sizes. So it’s a combination of scaling and innovation, I would say. It’s very promising early results.  The large language models that we’re seeing right now have this consistent problem with so-called hallucination, or their inclination to pass off guesses as facts. It seems to be possible to make models marginally more truthful by using reinforcement-learning techniques, but it’s still unclear whether reinforcement learning and scale can solve the problem entirely. What are your thoughts right now on that question? I think reinforcement learning can help [models] maybe even get an order of magnitude better. So quite significantly help. But to fully solve [hallucination], I think it’s going to require some other innovations. I think things like retrieval methods [and] tool use could help here. Perhaps fact-checking what [the model is] about to output, by checking with Google search as a tool, or checking in [the model’s] episodic memory banks, which maybe store facts. I think these could be good ways to get that accuracy even better. Let’s take the case that I would like to use these models for: scientific research. As you know, [chatbots’] citations often sound very plausible, but they’re made up, because they’re just plausible words. So it needs to better understand what entities are in the world. An academic paper isn’t a series of individual words; the whole citation is a unitary block, basically. So I think there are some things, like that, that a system needs to know. And therefore, it wouldn’t be appropriate to predict word by word. You need to retrieve the entire paper name, abstract, and publication venue all as one unitary piece of information.  We’re pressing on all those fronts, really. One is improving the reinforcement learning. We had some great work with Sparrow. We didn’t release Sparrow in the end as a stand-alone model, but we’ve utilized all that learning, in terms of rules adherence, and sticking to certain types of behavior. We’ve managed over time to get that improved from a 10% error rate down to 1%. So it was an order-of-magnitude improvement. The key thing is to do that without reducing the model’s lucidity, creativity, and fun-ness, in a way, and its ability to answer. So that’s the trade-off. We know how to do one or the other. It would be better if you could do both at the same time: be highly accurate, but also still be very creative and lucid. We last spoke in November last year, before the release of ChatGPT. Even then, you issued this warning that AI is an area of research that was getting more dangerous, that we were on the cusp of these technologies becoming very disruptive to society, and that AI researchers needed to be more careful. Since then, you’ve signed your name to a letter that warned the risks of advanced AI were as serious as pandemic and nuclear war. But also since then, we’ve seen a sea change in how governments are thinking about AI policy: they’re thinking seriously about this in a way they definitely weren’t last November. Do you feel more optimistic now, or more scared, or about the same—and why? You’re right, we spoke at an interesting moment in time, just before the latest wave of things changed everything again. I think it’s a complicated topic. I am actually quite optimistic about the situation as it is at the moment. When I talk to U.K. and U.S. government officials, they’re pretty up to speed now, which is great. That’s the plus side of the chatbot craze: that it allows the general public, politicians, and other key individuals in civil society to engage with the latest AI. I still think things like [the protein-folding breakthrough] AlphaFold are so far—perhaps I’m a little bit biased—more consequential in terms of advancing science.  Of course, language is what makes us human. So it’s clear why chatbots would resonate that much. I think the plus side is [ChatGPT] has moved the Overton window in a way so that one can discuss this, and it is a priority. What I’ve seen so far, from especially the U.K. and U.S. and a few other Western democracies, is pretty good. I think for example the U.K. white paper on AI was a very good balance between responsibility and innovation. And it’s good for it to be taken seriously.  There’s this persistent question of your independence from Google. When Google acquired DeepMind in 2014, it reportedly came with this guarantee that if DeepMind ever created AGI it would be overseen by DeepMind’s independent ethics board, rather than Google itself. But in 2021, the Wall Street Journal reported DeepMind had been involved in ultimately failed negotiations for more autonomy, including a legal structure that would prevent the powerful AI you were working on being controlled by a single corporate entity. Now, with the 2023 merger and DeepMind becoming a fully fledged part of Google, it appears from the outside that Google is tightening the leash even further and eroding whatever independence you might have had in the past. Is that what it feels like to you? It’s actually very much the opposite. I can’t get into the past speculation, a lot of which was very inaccurate. But the ethics charter part—we’ve always had that, at original DeepMind when we were independent and when we came in [to Google], and then that developed into the [Google] AI principles. So effectively, Google has adopted that Google-wide now. We’re very comfortable with that. And there was almost no difference between that, in the end, and the DeepMind principles. That was a huge part of the input into what became the Google AI principles. So that’s all matched up. We’ve had a huge influence over [how], and I’m very happy with the way that, Google overall addresses these things. I think it’s very responsible. Our moniker is being “bold and responsible” with this technology. I think you need both. And there is a creative tension between those two words, but I think that’s intentional. We’re very comfortable with that. And now we’re all very focused on delivering the benefits of this amazing technology to the world in a thoughtful, scientific, responsible way. It doesn’t mean we’ll never make any mistakes, because it’s such a new technology. It’s moving so fast. But we want to minimize that, and maximize the benefits. So very much the opposite, really, of maybe what it looks like from the outside. We’ve talked a lot about risks. Are there any capabilities that, if Gemini exhibited them in your testing phase, you’d decide: “No, we cannot release this”? Yeah, I mean, it’s probably several generations down the line. I think the most pressing thing that needs to happen in the research area of AI is to come up with the right evaluation benchmarks for capabilities, because we’d all love a set of maybe even hundreds of tests, where if your system passed it, they could get a kitemark [a British certification of quality] and you say, right, this is safe to deploy in X, Y, Z way. And the government could approve that. And consumers would understand what that meant. The problem is, we don’t have those types of benchmarks currently. We have ideas about it, like, is this system capable of deception? Can it replicate itself across data centers? These are the sorts of things you might want to test for. But you need really rigorous definitions if you want to make a practical, pragmatic test for them. I think that’s the most pressing thing for the field to do as a whole. We’re all trying to do that.  The new organization we helped announce, the Frontier Model Forum, is partly about the leading companies trying to come together to do more AI safety research. Really what you want is rigorous evaluation and benchmarking technologies. And if you had those, and then a system didn’t pass that, that means you wouldn’t release it until you sorted that out. And perhaps you would do that in something like a hardened simulator, or hardened sandbox, with cybersecurity things around it. So these are the types of ideas we have, but they need to be made a little bit more concrete. I think that’s the most pressing thing to be done, in time for those types of systems when they arrive, because I think we’ve got a couple of years, probably, or more. That’s not actually a lot of time, if you think about the research that has to be done. So I’m not worried about today’s systems. But I could foresee several generations from now that we will need something more rigorous than just looking at the amount of compute they used. Write to Billy Perrigo at billy.perrigo@time.com.", "nested_links": ["https://time.com/6246119/demis-hassabis-deepmind-interview/"]},
{"type": "article", "header": "What to Know About Elon Musk’s New AI Company, xAI", "author": "Will Henshall", "update_date/publish_date": "July 17, 2023 12:57 PM EDT | July 12, 2023 6:33 PM EDT", "link_to_article": "https://time.com/6294278/elon-musk-xai/", "text": "Elon Musk wants to “understand the true nature of the universe.” At least that’s what his new AI company, xAI, said on its website as he announced its formation on Wednesday. Announcing formation of @xAI to understand reality Musk incorporated xAI in Nevada in March this year and reportedly purchased “roughly 10,000 graphics processing units”—hardware that is required to develop and run state-of-the-art AI systems. The company has not said how it is financed but the Financial Times reported in April that Musk was discussing getting funding from investors in SpaceX and Tesla, two companies he runs. The company has not shared much detail about its intentions, but said on its website that its team would be joining a Twitter Spaces call on July 14 to take questions. (Musk is the owner of Twitter.) The AI company will work closely with Twitter, now called X Corp., and Tesla, as well as other companies “to make progress towards our mission,” xAI said on its website.    The team at xAI, led by Musk, includes former employees of prominent AI companies OpenAI and DeepMind, as well as Microsoft and Tesla. Dan Hendrycks, director of the Center for AI Safety, is listed as an adviser. The Center for AI Safety has been highly vocal about safety concerns. The organization released a statement signed by hundreds of AI scientists and experts, as well as the CEOs of some of the top AI companies in May, which said that “mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” Demis Hassabis, CEO of DeepMind, Sam Altman CEO of OpenAI, and Dario Amodei, CEO of Anthropic were among the signatories. Musk did not sign the Centre for AI Safety statement, but he did sign an open letter published in March by the Future of Life Institute, which called on AI companies to “immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”  Musk was one of the founding chairs of OpenAI, along with Altman, OpenAI’s CEO. Musk was among a group of investors, including Altman, Greg Brockman, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services, Infosys, and YC Research, who committed $1 billion to fund OpenAI in 2015. Musk has said he contributed $100 million of this $1 billion, but has also put the figure at half that.  Read More: A Brief History of Elon Musk Saying One Thing and Doing Another at Twitter The circumstances of Musk’s departure aren’t entirely clear. According to an OpenAI blog post and later tweets by Musk, he left OpenAI in 2018 to prevent conflicts of interest as Tesla became more focused on AI. Semafor reported in March that Musk had proposed that he assume leadership of OpenAI, and walked away after his proposal was rejected. The Financial Times reported in April that Musk’s departure was also due to clashes with other board members and staff over OpenAI’s approach to AI safety.  Since he left in 2018, Musk has been critical of the direction OpenAI has taken. In an interview with Tucker Carlson on Fox News in April, Musk said of the company “they are now closed source, and they are obviously for-profit, and closely allied with Microsoft.” Microsoft and OpenAI have a multibillion-dollar partnership, which grants OpenAI use of Microsoft’s cloud computing capacity in exchange for OpenAI’s artificial intelligence systems powering Microsoft’s products.  In March, Musk tweeted “I’m still confused as to how a non-profit to which I donated ~$100M somehow became a $30B market cap for-profit. If this is legal, why doesn’t everyone do it?” OpenAI has previously said that it moved from non-profit status to a “hybrid of a for-profit and nonprofit” because the computational demands of training state-of-the-art AI systems mean that OpenAI would need to raise more funding than would be available to a standard non-profit.  In the April interview with Carlson, Musk also said he was worried that AI models are being trained to be “politically correct,” and pledged to create a “Truth GPT,” which he said would be a “maximum truth-seeking AI.” Read More: Elon Musk Is Bringing the Culture Wars to AI Musk, who has frequently spoken about the risks posed by advanced AI systems in the past, also said in the interview that he was founding a new AI organization to prevent “AI dystopia.” However, experts and researchers, including xAI adviser Hendrycks, have expressed concerns that the addition of another highly-resourced company to the AI ecosystem could further fuel the race to develop powerful AI systems, at the expense of efforts to make them safe.  Read more: The AI Arms Race Is Changing Everything In April, in response to reports that Musk would likely start a new AI company, Hendrycks wrote that the “emergence of a new major AI developer will likely increase competitive pressures,” and that the “pressures to race ahead can cause actors to make sacrifices, especially when there are tradeoffs between safety and competitiveness.” On Wednesday, in a discussion on Twitter Spaces with Congressmen Ro Khanna and Mike Gallagher, Musk reaffirmed that his approach to creating safe AI would be based on the AI being truth-seeking. From an AI safety standpoint, a maximally curious AI, one that is trying to understand the universe, is going to be pro-humanity,” Musk said. “From the standpoint that humanity is just much more interesting than not humanity.”  Jess Whittlestone, head of AI policy at the Centre for Long-Term Resilience, a U.K.-based think tank, told TIME in an email that this is “a pretty unorthodox (and I think quite naive) approach to AI safety. I’m not sure we can even say clearly what it would mean for an AI to be “maximally curious” and it’s a huge leap to assume that this means an AI will be pro-humanity. The whole challenge right now is that we cannot fully understand these AI models or predict their behavior, and I don’t see anything in Musk’s approach which suggests it will solve that.” Write to Will Henshall at will.henshall@time.com", "nested_links": ["https://time.com/author/dan-hendrycks/", "https://time.com/6266679/musk-ai-open-letter/", "https://time.com/6263475/gpt4-ai-projects/", "https://time.com/6274774/elon-musk-twitter-u-turns/", "https://time.com/6260185/elon-musk-ai-culture-wars/", "https://time.com/6255952/ai-impact-chatgpt-microsoft-google/", "https://time.com/6255952/ai-impact-chatgpt-microsoft-google/"]},
{"type": "article", "header": "Game-changing AI", "author": "\n                        Billy Perrigo\n                      ", "update_date/publish_date": "October 24, 2023 7:00 AM EDT", "link_to_article": "https://time.com/collection/best-inventions-2023/6326986/openai-gpt-4/", "text": "Eight months on from its March release, OpenAI’s GPT-4 remains the most powerful AI model to power a chatbot accessible to the public. While its predecessor, ChatGPT, performed better than just 10% of students taking the bar exam, GPT-4 outperformed 90% of them. It’s adept at verbal reasoning, can break down complicated concepts into simple language, and can even explain why a joke is funny. In September, OpenAI began rolling out the abilities to interact with the model by voice and to use images as inputs. The update, GPT-4V, was tested with Be My Eyes, an organization that builds tools for visually impaired people, and can verbally describe the contents of a picture in natural language. Correction, Oct. 25 The original version of this story mischaracterized GPT-4. It is an AI model that powers a chatbot, not the chatbot itself. Write to Billy Perrigo at billy.perrigo@time.com.", "nested_links": []},
{"type": "article", "header": "DeepMind’s CEO Helped Take AI Mainstream. Now He’s Urging Caution", "author": "Billy Perrigo", "update_date/publish_date": "January 12, 2023 9:03 AM EST", "link_to_article": "https://time.com/6246119/demis-hassabis-deepmind-interview/", "text": "Demis Hassabis stands halfway up a spiral staircase, surveying the cathedral he built. Behind him, light glints off the rungs of a golden helix rising up through the staircase’s airy well. The DNA sculpture, spanning three floors, is the centerpiece of DeepMind’s recently opened London headquarters. It’s an artistic representation of the code embedded in the nucleus of nearly every cell in the human body. “Although we work on making machines smart, we wanted to keep humanity at the center of what we’re doing here,” Hassabis, DeepMind’s CEO and co-founder, tells TIME. This building, he says, is a “cathedral to knowledge.” Each meeting room is named after a famous scientist or philosopher; we meet in the one dedicated to James Clerk Maxwell, the man who first theorized electromagnetic radiation. “I’ve always thought of DeepMind as an ode to intelligence,” Hassabis says.  Hassabis, 46, has always been obsessed with intelligence: what it is, the possibilities it unlocks, and how to acquire more of it. He was the second-best chess player in the world for his age when he was 12, and he graduated from high school a year early. As an adult he strikes a somewhat diminutive figure, but his intellectual presence fills the room. “I want to understand the big questions, the really big ones that you normally go into philosophy or physics if you’re interested in,” he says. “I thought building AI would be the fastest route to answer some of those questions.”  DeepMind—a subsidiary of Google’s parent company, Alphabet—is one of the world’s leading artificial intelligence labs. Last summer it announced that one of its algorithms, AlphaFold, had predicted the 3D structures of nearly all the proteins known to humanity, and that the company was making the technology behind it freely available. Scientists had long been familiar with the sequences of amino acids that make up proteins, the building blocks of life, but had never cracked how they fold up into the complex 3D shapes so crucial to their behavior in the human body. AlphaFold has already been a force multiplier for hundreds of thousands of scientists working on efforts such as developing malaria vaccines, fighting antibiotic resistance, and tackling plastic pollution, the company says. Now DeepMind is applying similar machine-learning techniques to the puzzle of nuclear fusion, hoping it helps yield an abundant source of cheap, zero-carbon energy that could wean the global economy off fossil fuels at a critical juncture in the climate crisis. Hassabis says these efforts are just the beginning. He and his colleagues have been working toward a much grander ambition: creating artificial general intelligence, or AGI, by building machines that can think, learn, and be set to solve humanity’s toughest problems. Today’s AI is narrow, brittle, and often not very intelligent at all. But AGI, Hassabis believes, will be an “epoch-defining” technology—like the harnessing of electricity—that will change the very fabric of human life. If he’s right, it could earn him a place in history that would relegate the namesakes of his meeting rooms to mere footnotes.  But with AI’s promise also comes peril. In recent months, researchers building an AI system to design new drugs revealed that their tool could be easily repurposed to make deadly new chemicals. A separate AI model trained to spew out toxic hate speech went viral, exemplifying the risk to vulnerable communities online. And inside AI labs around the world, policy experts were grappling with near-term questions like what to do when an AI has the potential to be commandeered by rogue states to mount widespread hacking campaigns or infer state-level nuclear secrets. In December 2022, ChatGPT, a chatbot designed by DeepMind’s rival OpenAI, went viral for its seeming ability to write almost like a human—but faced criticism for its susceptibility to racism and misinformation. So did the tiny company Prisma Labs, for its Lensa app’s AI-enhanced selfies. But many users complained Lensa sexualized their images, revealing biases in its training data. What was once a field of a few deep-pocketed tech companies is becoming increasingly accessible. As computing power becomes cheaper and AI techniques become better known, you no longer need a high-walled cathedral to perform cutting-edge research.  Read More: The Price of Your AI-Generated Selfie It is in this uncertain climate that Hassabis agrees to a rare interview, to issue a stark warning about his growing concerns. “I would advocate not moving fast and breaking things,” he says, referring to an old Facebook motto that encouraged engineers to release their technologies into the world first and fix any problems that arose later. The phrase has since become synonymous with disruption. That culture, subsequently emulated by a generation of startups, helped Facebook rocket to 3 billion users. But it also left the company entirely unprepared when disinformation, hate speech, and even incitement to genocide began appearing on its platform. Hassabis sees a similarly worrying trend developing with AI. He says AI is now “on the cusp” of being able to make tools that could be deeply damaging to human civilization, and urges his competitors to proceed with more caution than before. “When it comes to very powerful technologies—and obviously AI is going to be one of the most powerful ever—we need to be careful,” he says. “Not everybody is thinking about those things. It’s like experimentalists, many of whom don’t realize they’re holding dangerous material.” Worse still, Hassabis points out, we are the guinea pigs. Hassabis was just 15 when he walked into the Bullfrog video-game studios in Guildford, in the rolling green hills just southwest of London. As a child he had always been obsessed with games. Not just chess—the main source of his expanding trophy cabinet—but the kinds you could play on early computers, too. Now he wanted to help make them. He had entered a competition in a video-game magazine to win an internship at the prestigious studio. His program—a Space Invaders-style game where players shot at chess pieces descending from the top of the screen—came in second place. He had to settle for a week’s work experience. Peter Molyneux, Bullfrog’s co-founder, still remembers first seeing Hassabis. “He looked like an elf from Lord of the Rings,” Molyneux says. “This little slender kid came in, who you would probably just walk past in the street and not even notice. But there was a sparkle in his eyes: the sparkle of intelligence.” In a chance conversation on the bus to Bullfrog’s Christmas party, the teenager captivated Molyneux. “The whole of the journey there, and the whole of the journey back, was the most intellectually stimulating conversation,” he recalls. They talked about the philosophy of games, what it is about the human psyche that makes winning so appealing, and whether you could imbue those same traits in a machine. “All the time I’m thinking, This is just a kid!” He knew then this young man was destined for great things.  The pair became fast friends. Hassabis returned to Bullfrog in the summer before he left for the University of Cambridge, and spent much of that time with Molyneux playing board and computer games. Molyneux recalls a fierce competitive streak. “I beat him at almost all the computer games, especially the strategy games,” Molyneux says. “He is an incredibly competitive person.” But Molyneux’s bragging rights were short-lived. Together, in pursuit of interesting game dynamics that might be the seed of the next hit video game, they invented a card-game they called Dummy. Hassabis beat Molyneux 35 times in a row. After graduating from Cambridge, Hassabis returned to Bullfrog to help Molyneux build his most popular game to date: Theme Park, a simulation game giving the player a God’s-eye view of an expanding fairground business. Hassabis went on to establish his own game company before later deciding to study for a Ph.D. in neuroscience. He wanted to understand the algorithmic level of the brain: not the interactions between microscopic neurons but the larger architectures that seemed to give rise to humanity’s powerful intelligence. “The mind is the most intriguing object in the universe,” Hassabis says. He was trying to understand how it worked in preparation for his life’s quest. “Without understanding that I had in mind AI the whole time, it looks like a random path,” Hassabis says of his career trajectory: chess, video games, neuroscience. “But I used every single scrap of that experience.” By 2013, when DeepMind was three years old, Google came knocking. A team of Google executives flew to London in a private jet, and Hassabis wowed them by showing them a prototype AI his team had taught to play the computer game Breakout. DeepMind’s signature technique behind the algorithm, reinforcement learning, was something Google wasn’t doing at the time. It was inspired by how the human brain learns, an understanding Hassabis had developed during his time as a neuroscientist. The AI would play the game millions of times, and was rewarded every time it scored some points. Through a process of points-based reinforcement, it would learn the optimum strategy. Hassabis and his colleagues fervently believed in training AI in game environments, and the dividends of the approach impressed the Google executives. “I loved them immediately,” says Alan Eustace, a former senior vice president at Google who led the scouting trip. Hassabis’ focus on the dangers of AI was evident from his first conversation with Eustace. “He was thoughtful enough to understand that the technology had long-term societal implications, and he wanted to understand those before the technology was invented, not after the technology was deployed,” Eustace says. “It’s like chess. What’s the endgame? How is it going to develop, not just two steps ahead, but 20 steps ahead?”  Eustace assured Hassabis that Google shared those concerns, and that DeepMind’s interests were aligned with its own. Google’s mission, Eustace said, was to index all of humanity’s knowledge, make it accessible, and ultimately raise the IQ of the world. “I think that resonated,” he says. The following year, Google acquired DeepMind for some $500 million. Hassabis turned down a bigger offer from Facebook. One reason, he says, was that, unlike Facebook, Google was “very happy to accept” DeepMind’s ethical red lines “as part of the acquisition.” (There were reports at the time that Google agreed to set up an independent ethics board to ensure these lines were not crossed.) The founders of the fledgling AI lab also reasoned that the megacorporation’s deep pockets would allow them access to talent and computing power that they otherwise couldn’t afford.  In a glass cabinet spanning the far wall of the lobby at DeepMind’s London headquarters, among other memorabilia from the first 12 years of the company’s life, sits a large square of wood daubed with black scribbles. It’s a souvenir from DeepMind’s first major coup. Soon after the Google acquisition, the company had set itself the challenge of designing an algorithm that could beat the best player in the world at the ancient Chinese board game Go. Chess had long ago been conquered by brute-force computer programming, but Go was far more complex; the best AI algorithms were still no match for top human players. DeepMind tackled the problem the same way they’d cracked Breakout. It built a program that, after being taught the rules of the game by observing human play, would play virtually against itself millions of times. Through reinforcement learning, the algorithm would update itself, reducing the “weights” of decisions that made it more likely to lose the game, and increasing the “weights” that made it more likely to win. At a tournament in Korea in March 2016, the algorithm—called AlphaGo—went up against Lee Sedol, one of the world’s top Go players. AlphaGo beat him four games to one. With a black marker pen, the defeated Lee scrawled his signature on the back of the Go board on which the fateful game had been played. Hassabis signed on behalf of AlphaGo, and DeepMind kept the board as a trophy. Forecasters had not expected the milestone to be passed for a decade. It was a vindication of Hassabis’ pitch to Google: that the best way to push the frontier of AI was to focus on reinforcement learning in game environments.  But just as DeepMind was scaling new heights, things were beginning to get complicated. In 2015, two of its earliest investors, billionaires Peter Thiel and Elon Musk, symbolically turned their backs on DeepMind by funding rival startup OpenAI. That lab, subsequently bankrolled by $1 billion from Microsoft, also believed in the possibility of AGI, but it had a very different philosophy for how to get there. It wasn’t as interested in games. Much of its research focused not on reinforcement learning but on unsupervised learning, a different technique that involves scraping vast quantities of data from the internet and pumping it through neural networks. As computers became more powerful and data more abundant, those techniques appeared to be making huge strides in capability.  While DeepMind, Google, and other AI labs had been working on similar research behind closed doors, OpenAI was more willing to let the public use its tools. In late 2022 it launched DALL·E 2, which can generate an image of almost any search term imaginable, and the chatbot ChatGPT. Because both of these tools were trained on data scraped from the internet, they were plagued by structural biases and inaccuracies. DALL·E 2 is likely to illustrate “lawyers” as old white men and “flight attendants” as young beautiful women, while ChatGPT is prone to confident assertions of false information. In the wrong hands, a 2021 DeepMind research paper says, language-generation tools like ChatGPT and its predecessor GPT-3 could turbocharge the spread of disinformation, facilitate government censorship or surveillance, and perpetuate harmful stereotypes under the guise of objectivity. (OpenAI acknowledges its apps have limitations, including biases, but says that it’s working to minimize them and that its mission is to build safe AGI to benefit humanity.) But despite Hassabis’s calls for the AI race to slow down, it appears DeepMind is not immune from the competitive pressures. In early 2022, the company published a blueprint for a faster engine. The piece of research, called Chinchilla, showed that many of the industry’s most cutting-edge models had been trained inefficiently, and explained how they could deliver more capability with the same level of computing power. Hassabis says DeepMind’s internal ethics board discussed whether releasing the research would be unethical given the risk that it could allow less scrupulous firms to release more powerful technologies without firm guardrails. One of the reasons they decided to publish it anyway was because “we weren’t the only people to know” about the phenomenon. He says that DeepMind is also considering releasing its own chatbot, called Sparrow, for a “private beta” some time in 2023. (The delay is in order for DeepMind to work on reinforcement learning-based features that ChatGPT lacks, like citing its sources. “It’s right to be cautious on that front,” Hassabis says.) But he admits that the company may soon need to change its calculus. “We’re getting into an era where we have to start thinking about the freeloaders, or people who are reading but not contributing to that information base,” he says. “And that includes nation states as well.” He declines to name which states he means—“it’s pretty obvious, who you might think”—but he suggests that the AI industry’s culture of publishing its findings openly may soon need to end. Hassabis wants the world to see DeepMind as a standard bearer of safe and ethical AI research, leading by example in a field full of others focused on speed. DeepMind has published “red lines” against unethical uses of its technology, including surveillance and weaponry. But neither DeepMind nor Alphabet has publicly shared what legal power DeepMind has to prevent its parent—a surveillance empire that has dabbled in Pentagon contracts—from pursuing those goals with the AI DeepMind builds. In 2021, Alphabet ended yearslong talks with DeepMind about the subsidiary’s setting up an independent legal structure that would prevent its AI being controlled by a single corporate entity, the Wall Street Journal reported. Hassabis doesn’t deny DeepMind made these attempts, but downplays any suggestion that he is concerned about the current structure being unsafe. When asked to confirm or deny whether the independent ethics board rumored to have been set up as part of the Google acquisition actually exists, he says he can’t, because it’s “all confidential.” But he adds that DeepMind’s ethics structure has “evolved” since the acquisition “into the structures that we have now.”  Hassabis says both DeepMind and Alphabet have committed to public ethical frameworks and build safety into their tools from the very beginning. DeepMind has its own internal ethics board, the Institutional Review Committee (IRC), with representatives from all areas of the company, chaired by its chief operating officer, Lila Ibrahim. The IRC meets regularly, Ibrahim says, and any disagreements are escalated to DeepMind’s executive leaders for a final decision. “We operate with a lot of freedom,” she says. “We have a separate review process: we have our own internal ethics review committee; we collaborate on best practices and learnings.” When asked what happens if DeepMind’s leadership team disagrees with Alphabet’s, or if its “red lines” are crossed, Ibrahim only says, “We haven’t had that issue yet.” One of Hassabis’ favorite games right now is a strategy game called Polytopia. The aim is to grow a small village into a world-dominating empire through gradual technological advances. Fishing, for example, opens the door to seafaring, which leads eventually to navies of your ships firing cannons and traversing oceans. By the end of the game, if you’ve directed your technological progress astutely, you’ll sit atop a shining, sophisticated empire with your enemies dead at your feet. (Elon Musk, Hassabis says, is a fan too. The last time the pair spoke, a few months ago, Polytopia was the main subject of their conversation. “We both like that game a lot,” Hassabis says.) While Hassabis’ worldview is much more nuanced—and cautious—it’s easy to see why the game’s ethos resonates with him. He still appears to believe that technological advancement is inherently good for humanity, and that under capitalism it’s possible to predict and mitigate AI’s risks. “Advances in science and technology: that’s what drives civilization,” he says. Hassabis believes the wealth from AGI, if it arrives, should be redistributed. “I think we need to make sure that the benefits accrue to as many people as possible—to all of humanity, ideally.” He likes the ideas of universal basic income, under which every citizen is given a monthly stipend from the government, and universal basic services, where the state pays for basic living standards like transportation or housing. He says an AGI-driven future should be more economically equal than today’s world, without explaining how that system would work. “If you’re in a [world of] radical abundance, there should be less room for that inequality and less ways that could come about. So that’s one of the positive consequences of the AGI vision, if it gets realized.” Others are less optimistic that this utopian future will come to pass—given that the past several decades of growth in the tech industry have coincided with huge increases in wealth inequality. “Major corporations, including the major corporation that owns DeepMind, have to ensure they maximize value to shareholders; are not focused really on addressing the climate crisis unless there is a profit in it; and are certainly not interested in redistributing wealth when the whole goal of the company is to accumulate further wealth and distribute it to shareholders,” says Paris Marx, host of the podcast Tech Won’t Save Us. “Not recognizing those things is really failing to fully consider the potential impacts of the technology.” Alphabet, Amazon, and Meta were among the 20 corporations that spent the most money lobbying U.S. lawmakers in 2022, according to transparency watchdog Open Secrets. “What we lack is not the technology to address the climate crisis, or to redistribute wealth,” Marx says. “What we lack is the political will. And it’s hard to see how just creating a new technology is going to create the political will to actually have these more structural transformations of society.” Back at DeepMind’s spiral staircase, an employee explains that the DNA sculpture is designed to rotate, but today the motor is broken. Closer inspection shows some of the rungs of the helix are askew. At the bottom of the staircase there’s a notice on a wooden stool in front of this giant metaphor for humanity. “Please don’t touch,” it reads. “It’s very fragile and could easily be damaged.”  —With reporting by Mariah Espada and Solcyre Burga Write to Billy Perrigo at billy.perrigo@time.com", "nested_links": ["https://time.com/6201423/deepmind-alphafold-proteins/", "https://time.com/6240648/lensa-ai-psychology-behind/", "https://time.com/6241819/ai-generated-lensa-selfie-private-data/"]}
]